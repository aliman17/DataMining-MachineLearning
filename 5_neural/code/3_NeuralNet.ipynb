{
 "metadata": {
  "name": "",
  "signature": "sha256:a71751962f4bb6f09ab257dd115de0e62b6da3206002b9855696d0939a66c305"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from scipy.optimize import fmin_l_bfgs_b\n",
      "from sklearn import cross_validation\n",
      "from sklearn import preprocessing\n",
      "from sklearn import decomposition\n",
      "import numpy as np\n",
      "import scipy as sp"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Load Data"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "norm = preprocessing.Normalizer()\n",
      "X_train_in = np.loadtxt(open(\"../data/train.csv\",\"rb\"),delimiter=\",\",skiprows=1, usecols=range(1,94))\n",
      "X_train_in = norm.fit_transform(X_train_in)\n",
      "\n",
      "y_train_in = np.loadtxt(open(\"../data/train.csv\",\"rb\"),dtype=str,delimiter=\",\",skiprows=1, usecols=[94])\n",
      "y_train_in = np.array([int(c[-2]) for c in y_train_in])  # Parse classes from Class_1 into 1\n",
      "\n",
      "X_test_in = np.loadtxt(open(\"../data/test.csv\",\"rb\"),delimiter=\",\",skiprows=1, usecols=range(1,94))\n",
      "X_test_in = norm.transform(X_test_in)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Preprocesse Data"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Shuffle\n",
      "XY = np.hstack((X_train_in, np.matrix(y_train_in).T))\n",
      "np.random.shuffle(XY)\n",
      "X_train = XY[:, :X_train_in.shape[1]]\n",
      "y_train = XY[:, -1].T.tolist()\n",
      "y_train = np.array(y_train[0])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "y_train.shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 4,
       "text": [
        "(50000,)"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Pca\n",
      "\n",
      "pca = decomposition.PCA(n_components=20)\n",
      "X_train = pca.fit_transform(X_train)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Normalize \n",
      "\n",
      "# with l1. We want to have linear corelation because we have multiple classes. \n",
      "X_train = preprocessing.normalize(X_train, norm='l1', axis=0, copy=True)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Class number to vector: 4 = [0 0 0 1 0 0 0 0 0]\n",
      "\n",
      "NUM_OF_CLASSES = len( np.unique(y_train))  # Warning: number of classes must go from 1 to n\n",
      "I = np.identity(NUM_OF_CLASSES)\n",
      "y_train_extended = np.array( [ I[y-1] for y in y_train])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "y_train_extended.shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 8,
       "text": [
        "(50000, 9)"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X_train.shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 9,
       "text": [
        "(50000, 20)"
       ]
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "heading",
     "level": 6,
     "metadata": {},
     "source": [
      "Testing box"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Testing normalization\n",
      "m = np.matrix([[0.5,2.0,3.0], [0.5,8.0,3.0]])\n",
      "normalized_m = preprocessing.normalize(m, axis=0)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "m"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 11,
       "text": [
        "matrix([[ 0.70710678,  0.24253563,  0.70710678],\n",
        "        [ 0.70710678,  0.9701425 ,  0.70710678]])"
       ]
      }
     ],
     "prompt_number": 11
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "np.hstack((m, m))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 12,
       "text": [
        "matrix([[ 0.70710678,  0.24253563,  0.70710678,  0.70710678,  0.24253563,\n",
        "          0.70710678],\n",
        "        [ 0.70710678,  0.9701425 ,  0.70710678,  0.70710678,  0.9701425 ,\n",
        "          0.70710678]])"
       ]
      }
     ],
     "prompt_number": 12
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Helper Functions"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def add_ones(X):\n",
      "    return np.column_stack(X, (np.ones(len(X))))\n",
      "\n",
      "def add_one(X):\n",
      "    return np.append(X, 1)\n",
      "\n",
      "def del_one(X):\n",
      "    return X[:-1]\n",
      "\n",
      "def sigmoid(z):\n",
      "    return 1/(1+np.exp(-z))\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 13
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Classifier"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class NeuralNetClassifier():\n",
      "    \"\"\"Neural network classifier based on a set of binary classifiers.\"\"\"\n",
      "    def __init__(self, h, thetas):\n",
      "        self.thetas = thetas  # model parameters\n",
      "        self.h = h\n",
      "        \n",
      "    def predict(self, X):\n",
      "        return np.array( [ self.h(x_, self.thetas) for x_ in X])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 14
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Learner"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class NeuralNetLearner():\n",
      "    def __init__(self, arch, g=sigmoid, lambda_=1e-5):\n",
      "        self.arch = arch\n",
      "        self.n_levels = len(arch)\n",
      "        self.g = g\n",
      "        self.lambda_ = lambda_\n",
      "        self.theta_shape = np.array([ ( arch[i] + 1, arch[i + 1]) for i in range( len( arch) - 1)])\n",
      "        ind = np.array( [ sh1 * sh2 for sh1, sh2 in self.theta_shape])\n",
      "        self.theta_ind = np.cumsum( ind[:-1])  # last index falls out of the boundary\n",
      "        self.theta_len = sum(ind)\n",
      "    \n",
      "    def init_thetas(self, epsilon=1e-5):\n",
      "        \"\"\"Return thetas with random values\"\"\"\n",
      "        np.random.seed(42)\n",
      "        return np.random.rand(self.theta_len) * 2 * epsilon - epsilon\n",
      "    \n",
      "    def shape_thetas(self, thetas):\n",
      "        \"\"\"Return list of thetas at particular level\"\"\"\n",
      "        t = np.split(thetas, self.theta_ind)\n",
      "        return np.array( [t[i].reshape(shape) for i, \n",
      "                          shape in enumerate(self.theta_shape)])\n",
      "    \n",
      "    def feedforward(self, a_, thetas):\n",
      "        \"\"\"Feed forward, prediction, add ones to the end for bias\"\"\"\n",
      "        thetas = self.shape_thetas(thetas)\n",
      "        activationsByLevel = [a_]\n",
      "        for theta in thetas: \n",
      "            a1_ = add_one(a_)  # add bias\n",
      "            a_ = self.g(a1_.dot(theta))\n",
      "            activationsByLevel.append(a_)\n",
      "        return np.array(activationsByLevel)\n",
      "    \n",
      "    def backprop(self, thetas):\n",
      "        \"\"\"Add bias to the end\"\"\"\n",
      "\n",
      "        activations = self.feedforward( self.x_, thetas)\n",
      "        thetas = self.shape_thetas( thetas)\n",
      "        # output error\n",
      "        err = - np.multiply( self.y_ - activations[-1],  \n",
      "                             np.multiply( activations[-1], 1-activations[-1])) \n",
      "               \n",
      "        errors = [0] * self.n_levels\n",
      "        errors[self.n_levels - 1] = err \n",
      "        \n",
      "        levels = list(range(0, self.n_levels-1))\n",
      "        for l in levels[::-1]:  # reverse\n",
      "            th_ = thetas[l] * np.matrix(err).T\n",
      "            th_ = th_.T\n",
      "            ac_ = add_one( activations[l] ) # add bias\n",
      "            ac_ = np.multiply( ac_, (1 - ac_))\n",
      "            errors[l] = np.array( np.multiply( th_, ac_) )[0]\n",
      "            err = del_one(errors[l])\n",
      "\n",
      "        return np.array(errors), activations\n",
      "    \n",
      "    def J(self, thetas):\n",
      "        return 1/2 * sum(((self.feedforward( self.x_, thetas)[-1] - self.y_)**2)) \n",
      "\n",
      "    def Jgrad(self, thetas):\n",
      "        errors, activations = self.backprop( thetas)\n",
      "        grad = np.array([])\n",
      "        for l in range(self.n_levels-1):\n",
      "            ac_ = add_one(activations[l])\n",
      "            err = del_one(errors[l+1]) if l != self.n_levels-2 else errors[l+1]\n",
      "            cur_grad = np.matrix(err).T.dot(np.matrix(ac_)).T\n",
      "            grad = np.append(grad , cur_grad)\n",
      "        return np.array(grad)\n",
      "        \n",
      "    def grad_approx(self, thetas, e=1e-1):\n",
      "        return np.array([(self.J(thetas+eps) - self.J(thetas-eps))/(2*e)\n",
      "                         for eps in np.identity(len(thetas)) * e])\n",
      "    \n",
      "    def set_data( self, x_, y_, thetas=None):\n",
      "        self.x_ = x_\n",
      "        self.y_ = y_\n",
      "        self.thetas = thetas if thetas != None else self.init_thetas()\n",
      "    \n",
      "    def fit(self, x_, y_, thetas=None):\n",
      "        \"\"\" x_ is a vector, y is a class between 1 and sth\"\"\"\n",
      "        self.set_data(x_, y_, thetas)\n",
      "        \n",
      "        thetas, _, _ = fmin_l_bfgs_b(func = self.J,\n",
      "                                           x0 = self.thetas,\n",
      "                                           fprime = self.Jgrad,\n",
      "                                           factr=1000)\n",
      "        self.thetas = thetas\n",
      "        return thetas\n",
      "    \n",
      "    def predict(self, x_, thetas):\n",
      "        return self.feedforward( x_, thetas)[-1]\n",
      "        \n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 19
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Testing"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "x_ = X_train[0, :5]\n",
      "y = y_train_extended[0]\n",
      "lr = NeuralNetLearner([len(x_), 2,  9])\n",
      "r = lr.fit(x_, y)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 20
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "# Test atributes and some minor functions of a learner\n",
      "\n",
      "print( \"Testing shape\\n\", lr.theta_shape)\n",
      "print( \"Testing indices\\n\", lr.theta_ind)\n",
      "print( \"Testing theta lenght\\n\", lr.theta_len)\n",
      "print( \"Testing theta shape\\n\", lr.shape_thetas( thetas))"
     ]
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "# Test feed forward\n",
      "\n",
      "print( \"Test feed forward:\")\n",
      "print( lr.feedforward(lr.x_, lr.thetas))"
     ]
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "# Test back propagation\n",
      "\n",
      "print( \"Test errors and activations\")\n",
      "errors, acts = lr.backprop(lr.thetas)\n",
      "print( errors.shape, acts.shape)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Test cost function\n",
      "\n",
      "print( \"Cost function:\", lr.J(lr.thetas))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Cost function: 3.23917625694e-05\n"
       ]
      }
     ],
     "prompt_number": 21
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Test gradient function \n",
      "\n",
      "print( \"Grad. function:\", lr.Jgrad(lr.thetas))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Grad. function: [  7.62745384e-11   7.62736474e-11   2.12154555e-10   2.12152077e-10\n",
        "   7.50196054e-11   7.50187291e-11  -1.98823883e-11  -1.98821561e-11\n",
        "   1.61407928e-11   1.61406042e-11  -5.37321332e-06  -5.37315056e-06\n",
        "   6.83906340e-06   6.83914232e-06   6.83915527e-06   6.83902895e-06\n",
        "   6.83900908e-06   6.83904515e-06   6.83910648e-06  -6.83901682e-06\n",
        "   6.83906497e-06   6.83906919e-06   6.83914812e-06   6.83916106e-06\n",
        "   6.83903474e-06   6.83901488e-06   6.83905095e-06   6.83911227e-06\n",
        "  -6.83902262e-06   6.83907077e-06   7.17884999e-06   7.17893283e-06\n",
        "   7.17894642e-06   7.17881382e-06   7.17879297e-06   7.17883083e-06\n",
        "   7.17889521e-06  -7.17880110e-06   7.17885164e-06]\n"
       ]
      }
     ],
     "prompt_number": 22
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Test gradient approximation\n",
      "\n",
      "print( \"Grad. approx:\", lr.grad_approx(lr.thetas, e=1e-5))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Grad. approx: [  7.62732840e-11   7.62729452e-11   2.12174299e-10   2.12170911e-10\n",
        "   7.50091721e-11   7.50084944e-11  -1.98927382e-11  -1.98927382e-11\n",
        "   1.61647768e-11   1.61647768e-11  -5.37321333e-06  -5.37315059e-06\n",
        "   6.83906340e-06   6.83914232e-06   6.83915527e-06   6.83902895e-06\n",
        "   6.83900908e-06   6.83904515e-06   6.83910648e-06  -6.83901679e-06\n",
        "   6.83906497e-06   6.83906919e-06   6.83914812e-06   6.83916106e-06\n",
        "   6.83903474e-06   6.83901488e-06   6.83905095e-06   6.83911227e-06\n",
        "  -6.83902260e-06   6.83907077e-06   7.17884999e-06   7.17893283e-06\n",
        "   7.17894642e-06   7.17881382e-06   7.17879297e-06   7.17883083e-06\n",
        "   7.17889521e-06  -7.17880109e-06   7.17885164e-06]\n"
       ]
      }
     ],
     "prompt_number": 23
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Test difference\n",
      "print( \"grad - grad.aprox =\", sum( lr.Jgrad(lr.thetas) - lr.grad_approx(lr.thetas, e=1e-5) ))  # = -2.310293595541139e-12"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "grad - grad.aprox = -7.01685216763e-14\n"
       ]
      }
     ],
     "prompt_number": 24
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Batch Sequential"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# stohastic training:   size = 1\n",
      "# batch training:       size = num of samples .... TU MORAS PAZITI. SESTEVATI MORAS GRADIENTE, NE THERE\n",
      "# batch sequential:     otherwise\n",
      "class BatchSeq():\n",
      "    def __init__(self, learner, size=100, alpha=0.1):\n",
      "        self.learner = learner\n",
      "        self.size = size\n",
      "        self.alpha = alpha \n",
      "        \n",
      "    def update_thetas(self, X, y_, thetas=None):\n",
      "        gradsum = []\n",
      "        for x, y in zip(X, y_):\n",
      "            self.learner.set_data( x, y, thetas)\n",
      "            gradsum = self.learner.Jgrad(self.learner.thetas) if len(gradsum) == 0 else gradsum + self.learner.Jgrad(self.learner.thetas)\n",
      "        new_thetas = thetas - self.alpha * 1/float(X.shape[0]) * gradsum\n",
      "        return new_thetas\n",
      "        \n",
      "    def fit_thetas(self, X, y_, thetas=None):\n",
      "        # devide into subset of 100 elements\n",
      "        # we have 50 000 / 100 = 500 subsets\n",
      "        size = self.size\n",
      "        # num of samples = 1130 = 0, 100, ..., 900, 1000, 1130 (last block takes all remainded)\n",
      "        split = [ [lo, lo+size] \n",
      "                          for lo in range( 0, X.shape[0], size)\n",
      "                        ] if X.shape[0] >= size else [[0, 0]]\n",
      "        split[-1][-1] = X.shape[0]\n",
      "        for lo, hi in split:\n",
      "            thetas = self.update_thetas( X[lo:hi, :], y_[lo:hi], thetas)\n",
      "        return thetas\n",
      "    \n",
      "    def fit(self, X, y_):\n",
      "        eps = 1e-4\n",
      "        thetas = self.learner.init_thetas()\n",
      "        # run until the error is very small\n",
      "        for i in range(500):\n",
      "            prev = thetas\n",
      "            thetas = self.fit_thetas(X, y_, thetas)\n",
      "            #if (sum( abs(thetas - prev) > eps ) == 0):\n",
      "            #    break\n",
      "        model = NeuralNetClassifier(self.learner.predict, thetas)\n",
      "        return model           \n",
      "            "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 59
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "size = 100\n",
      "split_indices = [ [lo, lo+size] for lo in range( 0, 1130-size, size)]\n",
      "split_indices[-1][-1] = 1130\n",
      "split_indices"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "threshold = 100\n",
      "X = X_train[:threshold, :]\n",
      "y_ = y_train_extended[:threshold]\n",
      "lr = NeuralNetLearner([X.shape[1], 1,  9])\n",
      "batchseq = BatchSeq(lr, 1)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 58
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "model = batchseq.fit(X, y_)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 38
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "model.thetas"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 39,
       "text": [
        "array([ -3.86623465e-06,  -1.86341965e-06,   9.84910105e-06,\n",
        "        -1.97288860e-05,   2.66797810e-06,  -1.27088519e-05,\n",
        "        -1.71731589e-05,   1.27360923e-05,  -9.02486863e-06,\n",
        "         9.96128313e-08,  -1.69677019e-05,   2.04863840e-05,\n",
        "         5.23618253e-06,  -5.22394338e-06,  -7.94576421e-06,\n",
        "        -5.85551281e-06,  -1.10720917e-05,   1.66935815e-06,\n",
        "         1.50138133e-06,  -4.20219935e-06,   1.38983870e+00,\n",
        "        -1.41743477e+00,  -5.57173756e-01,  -5.90991542e-01,\n",
        "        -1.31234502e+00,  -1.31573586e+00,  -5.26595148e-01,\n",
        "        -1.52051334e+00,  -6.56967886e-01,  -8.06972077e-01,\n",
        "        -2.23175609e+00,  -9.54570243e-01,  -1.03474424e+00,\n",
        "        -2.09135625e+00,  -2.09338638e+00,  -9.38084451e-01,\n",
        "        -2.37200997e+00,  -1.13822956e+00,  -1.36608813e+00])"
       ]
      }
     ],
     "prompt_number": 39
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Cross Validation"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn import metrics"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 40
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def evaluate(pred, real):\n",
      "    return metrics.log_loss(real, pred)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 41
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def my_cross_validation(X, Y, arch):\n",
      "    k = 5\n",
      "    S = []\n",
      "    kf = cross_validation.KFold(len(y), n_folds=k, shuffle=True)\n",
      "    for train_index, test_index in kf:\n",
      "        # Split\n",
      "        X_train, X_test = X[train_index], X[test_index]\n",
      "        Y_train, Y_test = Y[train_index], Y[test_index]\n",
      "        # Predict\n",
      "        \n",
      "        learner = NeuralNetLearner(arch)\n",
      "        bs = BatchSeq( learner)\n",
      "        model = bs.fit(X_train, Y_train)\n",
      "        predicted = model.predict(X_test)\n",
      "        # Evaluate\n",
      "        score = evaluate(predicted, Y_test)\n",
      "        S.append( score )\n",
      "    S = np.array(S)\n",
      "    return(np.mean(S))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 60
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "threshold = 50000\n",
      "X = X_train[:threshold, :]\n",
      "Y = y_train_extended[:threshold]\n",
      "\n",
      "level_size = 40\n",
      "n_of_hidden = 20\n",
      "hidden = [level_size for i in range(n_of_hidden)]\n",
      "arch = [X.shape[1]] + hidden + [NUM_OF_CLASSES]\n",
      "print( i, my_cross_validation(X, Y, arch))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "30 1.85691967935\n"
       ]
      }
     ],
     "prompt_number": 61
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}