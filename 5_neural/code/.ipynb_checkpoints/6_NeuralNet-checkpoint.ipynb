{
 "metadata": {
  "name": "",
  "signature": "sha256:bb32e979673cfd13e34ad4ece99287b7f2112a1903cfb684bb9b338998959860"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from scipy.optimize import fmin_l_bfgs_b\n",
      "from sklearn import cross_validation\n",
      "from sklearn import preprocessing\n",
      "from sklearn import decomposition\n",
      "from sklearn import datasets\n",
      "import numpy as np\n",
      "import scipy as sp"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Load Data"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "NUM_OF_CLASSES = 0\n",
      "def load_data1():\n",
      "    global NUM_OF_CLASSES\n",
      "    X_train = np.loadtxt(open(\"../data/train.csv\",\"rb\"),delimiter=\",\",skiprows=1, usecols=range(1,94))\n",
      "\n",
      "    y_train = np.loadtxt(open(\"../data/train.csv\",\"rb\"),dtype=str,delimiter=\",\",skiprows=1, usecols=[94])\n",
      "    y_train = np.array([int(c[-2])-1 for c in y_train])  # Parse classes from Class_1 into 1\n",
      "\n",
      "      # Warning: number of classes must go from 1 to n\n",
      "\n",
      "    X_test = np.loadtxt(open(\"../data/test.csv\",\"rb\"),delimiter=\",\",skiprows=1, usecols=range(1,94))\n",
      "    \n",
      "    return X_train, y_train, X_test\n",
      "\n",
      "def load_data2():\n",
      "    iris = datasets.load_iris()\n",
      "    X_train = iris.data\n",
      "    y_train = iris.target.T\n",
      "    return X_train, y_train, None"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 21
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X_train, Y_train, X_test = load_data1()\n",
      "NUM_OF_CLASSES = len( np.unique(Y_train))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 151
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Preprocesse Data"
     ]
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "# Shuffle\n",
      "XY = np.hstack((X_train, np.matrix(Y_train).T))\n",
      "np.random.shuffle(XY)\n",
      "X_train = XY[:, :X_train.shape[1]]\n",
      "Y_train = XY[:, X_train.shape[1]:]"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Class 1 write as 1,0,0,0,0,0,0,0,0, class 2 write as 0,1,0,0,0,0,0,0,0, ...\n",
      "I = np.identity(NUM_OF_CLASSES)\n",
      "Y_train = np.array( [ I[y] for y in Y_train])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 152
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Normalize\n",
      "norm = preprocessing.Normalizer()\n",
      "X_train = norm.fit_transform(X_train)\n",
      "if X_test != None:\n",
      "    X_test = norm.transform(X_test)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 133
    },
    {
     "cell_type": "heading",
     "level": 6,
     "metadata": {},
     "source": [
      "Test"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "Y_train.shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 134,
       "text": [
        "(50000, 9)"
       ]
      }
     ],
     "prompt_number": 134
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "Y_train[:10]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 135,
       "text": [
        "array([[ 1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
        "       [ 1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
        "       [ 1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
        "       [ 1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
        "       [ 1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
        "       [ 1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
        "       [ 1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
        "       [ 1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
        "       [ 1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
        "       [ 1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]])"
       ]
      }
     ],
     "prompt_number": 135
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X_train.shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 136,
       "text": [
        "(50000, 93)"
       ]
      }
     ],
     "prompt_number": 136
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X_train[:10,:4]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 137,
       "text": [
        "array([[ 0.06388766,  0.        ,  0.        ,  0.        ],\n",
        "       [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
        "       [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
        "       [ 0.01546166,  0.        ,  0.        ,  0.01546166],\n",
        "       [ 0.11527808,  0.05763904,  0.        ,  0.        ],\n",
        "       [ 0.25819889,  0.        ,  0.        ,  0.        ],\n",
        "       [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
        "       [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
        "       [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
        "       [ 0.        ,  0.05661385,  0.05661385,  0.1132277 ]])"
       ]
      }
     ],
     "prompt_number": 137
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Helper Functions"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def add_ones(X):\n",
      "    return np.column_stack((X, np.ones(X.shape[0])))\n",
      "\n",
      "def add_one(X):\n",
      "    return np.append(X, 1)\n",
      "\n",
      "def add_zero(X):\n",
      "    return np.append(X, 0)\n",
      "\n",
      "def del_one(X):\n",
      "    return X[:-1]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 138
    },
    {
     "cell_type": "heading",
     "level": 6,
     "metadata": {},
     "source": [
      "Sigmoid"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def sigmoid(z):\n",
      "    return 1/(1+np.exp(-z))\n",
      "\n",
      "def sigmoid_derivative(z):\n",
      "    return np.multiply(z , 1 - z)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 139
    },
    {
     "cell_type": "heading",
     "level": 6,
     "metadata": {},
     "source": [
      "Tansigmoid"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def tansigmoid(z):\n",
      "    return ( np.exp(z) - np.exp(-z) ) / (np.exp(z) + np.exp(-z))\n",
      "\n",
      "def tansigmoid_derivative(z):\n",
      "    return 1 - tansigmoid(z)**2"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 140
    },
    {
     "cell_type": "heading",
     "level": 6,
     "metadata": {},
     "source": [
      "Arctan (my try)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def arctan(z):\n",
      "    return np.arctan(2*z) / np.pi + 1/2\n",
      "\n",
      "def arctan_derivative(z):\n",
      "    return 2 / (((2*z)**2 + 1) * np.pi)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 141
    },
    {
     "cell_type": "heading",
     "level": 6,
     "metadata": {},
     "source": [
      "Test"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "m = np.array([1,2,3])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 142
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def grad_approx(J, thetas, e=1e-5):\n",
      "        return np.array([(J(thetas+eps) - J(thetas-eps))/(2*e)\n",
      "                         for eps in np.identity(len(thetas)) * e])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 143
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print(tansigmoid_derivative(m))\n",
      "grad_approx(tansigmoid, m).diagonal()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[ 0.41997434  0.07065082  0.00986604]\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 144,
       "text": [
        "array([ 0.41997434,  0.07065082,  0.00986604])"
       ]
      }
     ],
     "prompt_number": 144
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print(arctan_derivative(m))\n",
      "grad_approx(arctan, m).diagonal()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[ 0.12732395  0.03744822  0.01720594]\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 145,
       "text": [
        "array([ 0.12732395,  0.03744822,  0.01720594])"
       ]
      }
     ],
     "prompt_number": 145
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Classifier"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class NeuralNetClassifier():\n",
      "    \"\"\"Neural network classifier based on a set of binary classifiers.\"\"\"\n",
      "    def __init__(self, h, thetas):\n",
      "        self.thetas = thetas  # model parameters\n",
      "        self.h = h\n",
      "        \n",
      "    def predict(self, X):\n",
      "        return np.array( [ self.h(x_, self.thetas) for x_ in X])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 146
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Learner"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 230
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class NeuralNetLearner():\n",
      "    def __init__(self, arch, g=sigmoid, g_derivative=sigmoid_derivative, lambda_=1e-5):\n",
      "        self.arch = arch\n",
      "        self.n_levels = len(arch)\n",
      "        \n",
      "        self.g = g\n",
      "        self.g_derivative = g_derivative\n",
      "        \n",
      "        self.lambda_ = lambda_\n",
      "        self.theta_shape = np.array([ ( arch[i] + 1, arch[i + 1]) for i in range( len( arch) - 1)])\n",
      "        ind = np.array( [ sh1 * sh2 for sh1, sh2 in self.theta_shape])\n",
      "        self.theta_ind = np.cumsum( ind[:-1])  # last index falls out of the boundary\n",
      "        self.theta_len = sum(ind)\n",
      "        self.seed = False\n",
      "    \n",
      "    def init_thetas(self, epsilon=1e-5):\n",
      "        \"\"\"Return thetas with random values\"\"\"\n",
      "        #if self.seed: \n",
      "        np.random.seed(42)\n",
      "        return np.random.rand(self.theta_len) * 2 * epsilon - epsilon\n",
      "    \n",
      "    def shape_thetas(self, thetas):\n",
      "        \"\"\"Return list of thetas at particular level\"\"\"\n",
      "        t = np.split(thetas, self.theta_ind)\n",
      "        return np.array( [t[i].reshape(shape) for i, \n",
      "                          shape in enumerate(self.theta_shape)])\n",
      "    \n",
      "    def feedforward(self, a_, thetas):\n",
      "        \"\"\"Feed forward, prediction, add ones to the end for bias\"\"\"\n",
      "        thetas = self.shape_thetas(thetas)\n",
      "        activations = [0] * self.n_levels\n",
      "        activations[0] = a_\n",
      "        for i in range(self.n_levels - 1): \n",
      "            activations[i+1] = self.g(np.dot(add_one(activations[i]), thetas[i]))\n",
      "        return np.array(activations)\n",
      "    \n",
      "    def backprop(self, x_, y_, thetas):\n",
      "        \"\"\"Add bias to the end\"\"\"\n",
      "        # Compute activations for each level. It includes first level too\n",
      "        activations = self.feedforward( x_, thetas)\n",
      "        # Compute error at the last level\n",
      "        err = - np.multiply( y_ - activations[-1],  \n",
      "                             self.g_derivative( activations[-1])) \n",
      "        # Prepare space for computing errors on all layers and insert output level\n",
      "        errors = [0] * self.n_levels\n",
      "        errors[self.n_levels - 1] = err \n",
      "        # Reshape thetas\n",
      "        thetas = self.shape_thetas( thetas)\n",
      "        # Compute errors on hidden layers and input layer. Start from behind\n",
      "        for l in range(self.n_levels - 2, -1, -1):  \n",
      "            th_ = np.matrix(err) * thetas[l].T \n",
      "            ac_ = add_one( activations[l] ) # add bias\n",
      "            ac_ = np.multiply( ac_, (1 - ac_))\n",
      "            errors[l] = np.array( np.multiply( th_, ac_) )[0]\n",
      "            err = del_one(errors[l])\n",
      "\n",
      "        return np.array(errors), activations\n",
      "    \n",
      "    def l2_reg( self, thetas):\n",
      "        thetas2 = thetas * thetas\n",
      "        thetas2 = self.shape_thetas( thetas2)\n",
      "        sum_ = 0\n",
      "        for levelthetas in thetas2:\n",
      "            for i in range( levelthetas.shape[0] - 1):\n",
      "                sum_ += np.sum( levelthetas[i])\n",
      "        return sum_ * self.lambda_ / 2\n",
      "    \n",
      "    def l2_reg_grad( self, thetas):\n",
      "        thetasModified = self.shape_thetas( thetas)\n",
      "        for i in range( len( thetasModified)):\n",
      "            thetasModified[i][-1] *= 0\n",
      "        return self.lambda_ * thetasModified\n",
      "    \n",
      "    # TODO self.m, self.X, self.Y, self.lambda_\n",
      "    def J(self, thetas):\n",
      "        sum_ = 0\n",
      "        for i in range( self.X.shape[0]):\n",
      "            sum_ += np.sum( np.square( self.feedforward( self.X[i], thetas)[-1] - self.Y[i])) \n",
      "        sum_ = sum_ / (2 * self.X.shape[0]) + self.l2_reg( thetas)\n",
      "        return sum_\n",
      "\n",
      "    def Jgrad(self, thetas):\n",
      "        errorsANDactivations = [ self.backprop( self.X[i], self.Y[i], thetas) for i in range(self.X.shape[0])]\n",
      "            \n",
      "        #errorsANDactivations = map( lambda z: self.backprop( z[0], z[1], thetas), zip( self.X, self.Y))\n",
      "        grad = [0] * (self.n_levels-1)\n",
      "        for errors, activations in errorsANDactivations:\n",
      "            for l in range(self.n_levels-1):\n",
      "                err = del_one(errors[l+1]) if l != self.n_levels-2 else errors[l+1]\n",
      "                cur_grad = np.dot(np.matrix( add_one(activations[l]) ).T, np.matrix(err))\n",
      "                grad[l] = grad[l] + cur_grad\n",
      "        \n",
      "        grad = np.array(grad) / self.X.shape[0] + self.l2_reg_grad( thetas)\n",
      "        flatten = np.array([])\n",
      "        for matrix in grad:\n",
      "            flatten = np.append(flatten, matrix.ravel())\n",
      "        return flatten\n",
      "        \n",
      "    def grad_approx(self, thetas, e=1e-1):\n",
      "        return np.array([(self.J(thetas+eps) - self.J(thetas-eps))/(2*e)\n",
      "                         for eps in np.identity(len(thetas)) * e]).ravel()\n",
      "    \n",
      "    def test_grad(self, thetas):\n",
      "        return np.sum( self.Jgrad(lr.thetas) - self.grad_approx(lr.thetas, e=1e-5) )\n",
      "    \n",
      "    def set_data( self, X, Y, thetas=None):\n",
      "        self.X = X\n",
      "        self.Y = Y\n",
      "        self.thetas = thetas if thetas != None else self.init_thetas()\n",
      "    \n",
      "    def fit(self, X, Y, thetas=None):\n",
      "        \"\"\" x_ is a vector, y is a class between 1 and sth\"\"\"\n",
      "        self.set_data(X, Y, thetas)\n",
      "        self.seed = True\n",
      "        \n",
      "        #self.thetas, _, _ = fmin_l_bfgs_b(func = self.J,\n",
      "        #                                   x0 = self.thetas,\n",
      "        #                                   fprime = self.Jgrad)   \n",
      "        return self.thetas\n",
      "    \n",
      "    def predict(self, X):\n",
      "        return np.array( [ self.feedforward(x_, self.thetas) for x_ in X])\n",
      "        \n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 328
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Testing"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "threshold = 10\n",
      "X = X_train[0:threshold, :10]\n",
      "Y = Y_train[0:threshold]\n",
      "#lr = NeuralNetLearner([X.shape[1], 2,  9], tansigmoid, tansigmoid_derivative)  # use this for tangens\n",
      "#lr = NeuralNetLearner([X.shape[1], 2,  9], arctan, arctan_derivative)\n",
      "lr = NeuralNetLearner([X.shape[1], 2,  NUM_OF_CLASSES])  # use this for logistic funciton\n",
      "r = lr.fit(X, Y)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 329
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "lr.thetas"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 330,
       "text": [
        "array([ -2.50919762e-06,   9.01428613e-06,   4.63987884e-06,\n",
        "         1.97316968e-06,  -6.87962719e-06,  -6.88010959e-06,\n",
        "        -8.83832776e-06,   7.32352292e-06,   2.02230023e-06,\n",
        "         4.16145156e-06,  -9.58831011e-06,   9.39819704e-06,\n",
        "         6.64885282e-06,  -5.75321779e-06,  -6.36350066e-06,\n",
        "        -6.33190980e-06,  -3.91515514e-06,   4.95128633e-07,\n",
        "        -1.36109963e-06,  -4.17541720e-06,   2.23705789e-06,\n",
        "        -7.21012279e-06,  -4.15710703e-06,  -2.67276313e-06,\n",
        "        -8.78600316e-07,   5.70351923e-06,  -6.00652436e-06,\n",
        "         2.84688768e-07,   1.84829138e-06,  -9.07099175e-06,\n",
        "         2.15089704e-06,  -6.58951753e-06,  -8.69896814e-06,\n",
        "         8.97771075e-06,   9.31264066e-06,   6.16794696e-06,\n",
        "        -3.90772462e-06,  -8.04655772e-06,   3.68466053e-06,\n",
        "        -1.19695013e-06,  -7.55923530e-06,  -9.64617978e-08,\n",
        "        -9.31222958e-06,   8.18640804e-06,  -4.82440037e-06,\n",
        "         3.25044569e-06,  -3.76577848e-06,   4.01360424e-07,\n",
        "         9.34205587e-07])"
       ]
      }
     ],
     "prompt_number": 330
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Test cost function\n",
      "print( \"Cost function:\", lr.J(lr.thetas))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Cost function: 1.12500081648\n"
       ]
      }
     ],
     "prompt_number": 331
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "lr.thetas"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 332,
       "text": [
        "array([ -2.50919762e-06,   9.01428613e-06,   4.63987884e-06,\n",
        "         1.97316968e-06,  -6.87962719e-06,  -6.88010959e-06,\n",
        "        -8.83832776e-06,   7.32352292e-06,   2.02230023e-06,\n",
        "         4.16145156e-06,  -9.58831011e-06,   9.39819704e-06,\n",
        "         6.64885282e-06,  -5.75321779e-06,  -6.36350066e-06,\n",
        "        -6.33190980e-06,  -3.91515514e-06,   4.95128633e-07,\n",
        "        -1.36109963e-06,  -4.17541720e-06,   2.23705789e-06,\n",
        "        -7.21012279e-06,  -4.15710703e-06,  -2.67276313e-06,\n",
        "        -8.78600316e-07,   5.70351923e-06,  -6.00652436e-06,\n",
        "         2.84688768e-07,   1.84829138e-06,  -9.07099175e-06,\n",
        "         2.15089704e-06,  -6.58951753e-06,  -8.69896814e-06,\n",
        "         8.97771075e-06,   9.31264066e-06,   6.16794696e-06,\n",
        "        -3.90772462e-06,  -8.04655772e-06,   3.68466053e-06,\n",
        "        -1.19695013e-06,  -7.55923530e-06,  -9.64617978e-08,\n",
        "        -9.31222958e-06,   8.18640804e-06,  -4.82440037e-06,\n",
        "         3.25044569e-06,  -3.76577848e-06,   4.01360424e-07,\n",
        "         9.34205587e-07])"
       ]
      }
     ],
     "prompt_number": 332
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "# Test atributes and some minor functions of a learner\n",
      "\n",
      "print( \"Testing shape\\n\", lr.theta_shape)\n",
      "print( \"Testing indices\\n\", lr.theta_ind)\n",
      "print( \"Testing theta lenght\\n\", lr.theta_len)\n",
      "print( \"Testing theta shape\\n\", lr.shape_thetas( thetas))"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Test feed forward\n",
      "\n",
      "print( \"Test feed forward:\")\n",
      "print( lr.feedforward(lr.X[0], lr.thetas))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Test feed forward:\n",
        "[array([ 1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.])\n",
        " array([ 0.49999993,  0.50000045])\n",
        " array([ 0.49999677,  0.49999855,  0.49999868,  0.50000392,  0.49999881,\n",
        "        0.50000036,  0.49999828,  0.49999943,  0.50000035])]\n"
       ]
      }
     ],
     "prompt_number": 333
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print( lr.predict(lr.X[:1]))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[[array([ 1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.])\n",
        "  array([ 0.49999993,  0.50000045])\n",
        "  array([ 0.49999677,  0.49999855,  0.49999868,  0.50000392,  0.49999881,\n",
        "        0.50000036,  0.49999828,  0.49999943,  0.50000035])]]\n"
       ]
      }
     ],
     "prompt_number": 334
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "# Test back propagation\n",
      "\n",
      "print( \"Test errors and activations\")\n",
      "errors, acts = lr.backprop(lr.thetas)\n",
      "print( errors.shape, acts.shape)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Test gradient function \n",
      "\n",
      "print( \"Grad. function:\", lr.Jgrad(lr.thetas))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Grad. function: [-0.01324149 -0.01324148 -0.00412771 -0.0041277  -0.00324629 -0.00324631\n",
        " -0.00790778 -0.00790779 -0.00489834 -0.00489836 -0.00458035 -0.00458031\n",
        " -0.01294343 -0.01294366 -0.01859355 -0.01859373 -0.00268979 -0.0026898\n",
        " -0.01099457 -0.01099466 -0.05701376 -0.05701388 -0.03141946  0.03141895\n",
        "  0.03141894  0.03141953  0.03141896  0.03141914  0.03141891  0.03141904\n",
        "  0.03141914 -0.03141943  0.03141892  0.03141891  0.0314195   0.03141893\n",
        "  0.03141912  0.03141888  0.03141901  0.03141911 -0.04507564  0.04507497\n",
        "  0.04507495  0.04507574  0.04507499  0.04507523  0.04507492  0.04507509\n",
        "  0.04507522]\n"
       ]
      }
     ],
     "prompt_number": 298
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Test gradient approximation\n",
      "\n",
      "print( \"Grad. approx:\", lr.grad_approx(lr.thetas, e=1e-5))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Grad. approx: [ -4.76618744e-08   1.36901601e-07  -1.54209978e-08   4.42756942e-08\n",
        "  -1.40110146e-08   4.02566869e-08  -2.80331314e-08   8.05244760e-08\n",
        "  -4.76285678e-08   1.36879397e-07  -2.24376073e-08   6.44040377e-08\n",
        "  -7.42517159e-08   2.13373763e-07  -1.63957736e-07   4.71023220e-07\n",
        "  -1.54098956e-08   4.42867965e-08  -5.60551605e-08   1.61037850e-07\n",
        "  -1.40176759e-07   4.02555766e-07  -6.24999009e-02   6.24995554e-02\n",
        "   6.24998596e-02   6.24999677e-02   6.24997356e-02   6.24996764e-02\n",
        "   6.24996362e-02   6.24996489e-02   6.24997480e-02  -6.25000086e-02\n",
        "   6.24996631e-02   6.24999673e-02   6.25000753e-02   6.24998433e-02\n",
        "   6.24997841e-02   6.24997439e-02   6.24997566e-02   6.24998557e-02\n",
        "  -1.25000336e-01   1.24999645e-01   1.25000253e-01   1.25000469e-01\n",
        "   1.25000005e-01   1.24999887e-01   1.24999806e-01   1.24999832e-01\n",
        "   1.25000030e-01]\n"
       ]
      }
     ],
     "prompt_number": 285
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Test difference\n",
      "print( \"grad - grad.aprox =\", lr.test_grad(lr.thetas))  # = -2.310293595541139e-12"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "grad - grad.aprox = -1.93631363448e-10\n"
       ]
      }
     ],
     "prompt_number": 286
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Cross Validation"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn import metrics\n",
      "\n",
      "def evaluate(pred, real):\n",
      "    return metrics.log_loss(real, pred)\n",
      "\n",
      "def my_cross_validation(X, Y, model, k=3):\n",
      "    S = []\n",
      "    kf = cross_validation.KFold(len(Y), n_folds=k, shuffle=True)\n",
      "    for train_index, test_index in kf:\n",
      "        print(\"New circle\", arch)\n",
      "        # Split\n",
      "        X_train, X_test = X[train_index], X[test_index]\n",
      "        Y_train, Y_test = Y[train_index], Y[test_index]\n",
      "        \n",
      "        model.fit(X_train, Y_train)\n",
      "        predicted = model.predict(X_test)\n",
      "        print(predicted.shape)\n",
      "        # Evaluate\n",
      "        score = evaluate(predicted, Y_test)\n",
      "        print(\"Vmesni rezultat: \", score)\n",
      "        S.append( score )\n",
      "    S = np.array(S)\n",
      "    return(np.mean(S))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 112
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Batch Sequential"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# stohastic training:   size = 1\n",
      "# batch training:       size = num of samples \n",
      "# batch sequential:     otherwise\n",
      "class BatchSeq():\n",
      "    def __init__(self, learner, size=None, alpha=0.1):\n",
      "        self.learner = learner\n",
      "        self.alpha = alpha \n",
      "        self.size = size\n",
      "        \n",
      "    def fit_thetas(self, X, y_, thetas=None):\n",
      "        # num of samples = 1130 = 0, 100, ..., 900, 1000, 1130 (last block takes all remainded)\n",
      "        split = [ [lo, lo + self.size] \n",
      "                          for lo in range( 0, X.shape[0], self.size)\n",
      "                        ] if X.shape[0] >= self.size else [[0, 0]]\n",
      "        split[-1][-1] = X.shape[0]\n",
      "        counter = 0\n",
      "        for lo, hi in split:\n",
      "            print(lo)\n",
      "            counter += 1\n",
      "            thetas = self.learner.fit( X[lo:hi], y_[lo:hi], thetas)\n",
      "        return thetas\n",
      "    \n",
      "    def fit(self, X, y_):\n",
      "        if self.size == None:\n",
      "            self.size = X.shape[0]\n",
      "        eps = 1e-4\n",
      "        thetas = self.learner.init_thetas()\n",
      "        # run until the error is very small ... because lbfgs, we don't need this anymore\n",
      "        for i in range(1):\n",
      "            #prev = thetas\n",
      "            thetas = self.fit_thetas(X, y_, thetas)\n",
      "            #if (sum( abs(thetas - prev) > eps ) == 0):\n",
      "            #    break\n",
      "        #model = NeuralNetClassifier(self.learner.predict, thetas)\n",
      "        #return model         \n",
      "    \n",
      "    def predict(self, X):\n",
      "        return self.learner.predict(X)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 84
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "RUN"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Pca\n",
      "if False:\n",
      "    pca = decomposition.PCA(n_components=20)\n",
      "    X_train = pca.fit_transform(X_train)\n",
      "    X_test = pca.transform(X_test)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 49
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print(X_train.shape, y_train.shape)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(50000, 93) (50000, 9)\n"
       ]
      }
     ],
     "prompt_number": 50
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "threshold = 500\n",
      "X = X_train[:threshold, :]\n",
      "Y = y_train[:threshold]\n",
      "hidden = [ 20, 20 ]\n",
      "arch = [X.shape[1]] + hidden + [NUM_OF_CLASSES]\n",
      "\n",
      "learner = NeuralNetLearner(arch)\n",
      "bs = BatchSeq( learner)\n",
      "score = my_cross_validation(X, Y, bs)\n",
      "print(score)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "New circle [93, 20, 20, 9]\n",
        "0\n",
        "(167, 4)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      },
      {
       "ename": "ValueError",
       "evalue": "The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
        "\u001b[1;32m<ipython-input-89-1b731ee8bb42>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mlearner\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mNeuralNetLearner\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0march\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mbs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBatchSeq\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0mlearner\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0mscore\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmy_cross_validation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m<ipython-input-88-fecc37f76b5b>\u001b[0m in \u001b[0;36mmy_cross_validation\u001b[1;34m(X, Y, model, k)\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredicted\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[1;31m# Evaluate\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m         \u001b[0mscore\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredicted\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Vmesni rezultat: \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscore\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[0mS\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0mscore\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m<ipython-input-88-fecc37f76b5b>\u001b[0m in \u001b[0;36mevaluate\u001b[1;34m(pred, real)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreal\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreal\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mmy_cross_validation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32mC:\\Python34\\lib\\site-packages\\sklearn\\metrics\\metrics.py\u001b[0m in \u001b[0;36mlog_loss\u001b[1;34m(y_true, y_pred, eps, normalize)\u001b[0m\n\u001b[0;32m   1104\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1105\u001b[0m     \u001b[1;31m# Clipping\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1106\u001b[1;33m     \u001b[0mY\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0meps\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1107\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1108\u001b[0m     \u001b[1;31m# This happens in cases when elements in y_pred have type \"str\".\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32mC:\\Python34\\lib\\site-packages\\numpy\\core\\fromnumeric.py\u001b[0m in \u001b[0;36mclip\u001b[1;34m(a, a_min, a_max, out)\u001b[0m\n\u001b[0;32m   1626\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1627\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0m_wrapit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'clip'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ma_min\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ma_max\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1628\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mclip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma_min\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ma_max\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1629\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1630\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;31mValueError\u001b[0m: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()"
       ]
      }
     ],
     "prompt_number": 89
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Find best parameters with Cross Validation"
     ]
    },
    {
     "cell_type": "heading",
     "level": 5,
     "metadata": {},
     "source": [
      "Parameters to test"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "- lambda for regularization\n",
      "- levels\n",
      "- batchsequential size"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "threshold = 5000\n",
      "X = X_train[:threshold, :]\n",
      "Y = y_train_extended[:threshold]\n",
      "\n",
      "\n",
      "# Because the input is much larger than output. Then it is good idea\n",
      "# to build hidden levels in a way that number of hidden states falls \n",
      "# from the length of the input till the length of the output. We need\n",
      "# to specify the length and then compute number off hidden neurons at\n",
      "# each level.\n",
      "\n",
      "lambdas = [0.1, 0.5, 1, 5]\n",
      "lengths = [2, 10, 20]\n",
      "\n",
      "for length in lengths:\n",
      "    \n",
      "    k = - (X_train.shape[1] - NUM_OF_CLASSES) / (length+1)\n",
      "    hidden = [ int( k * j + X_train.shape[1]) for j in range(1, length+1)]\n",
      "    arch = [X.shape[1]] + hidden + [NUM_OF_CLASSES]\n",
      "    \n",
      "    for lam in lambdas:\n",
      "        #score = my_cross_validation(X, Y, arch, lam)\n",
      "        score = 10\n",
      "        with open(\"crossval.txt\", \"a\") as myfile:\n",
      "            myfile.write(str(score)+\"\\t lambda=\"+str(lam)+\"\\t length=\"+str(length)+\"\\t arch=\"+str(arch)+\"\\n\")\n",
      "            myfile.close()\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 252
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Final Fit-Predict"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "hidden = [30, 15]\n",
      "arch = [X_train.shape[1]] + hidden + [NUM_OF_CLASSES]\n",
      "learner = NeuralNetLearner(arch, lambda_=1e-5)\n",
      "bs = BatchSeq( learner, 50000)\n",
      "model = bs.fit(X_train, y_train)\n",
      "predicted = model.predict(X_test)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "predicted = model.predict(X_test)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def write_to_file(predicted, name):\n",
      "    answer = np.matrix(predicted.astype(str))\n",
      "    ids = np.matrix(range(1, answer.shape[0]+1))\n",
      "    answer = np.hstack((ids.T.astype(str), answer))\n",
      "    answer = np.vstack((np.array([\"id\",\"Class_1\",\"Class_2\",\"Class_3\",\"Class_4\",\"Class_5\",\"Class_6\",\"Class_7\",\"Class_8\",\"Class_9\"]), answer))\n",
      "    answer = answer.tolist()\n",
      "    answer = \"\\n\".join( [\",\".join(line) for line in answer] )\n",
      "\n",
      "    fo = open(\"../results/\"+name+\".csv\", \"wt\")\n",
      "    fo.write(answer)\n",
      "    fo.close()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "write_to_file(predicted, \"batch1\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "adaptivni gradient, pri drugem stohasti\u010dnem primeru. \n",
      "Vsako iteracijo manj pristejemo. \n",
      "\n",
      "Lbfgs 1 iteracija za batch seq. \n",
      "\n",
      "\n",
      "A prave thete nastavis na nic? preveri"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}