{
 "metadata": {
  "name": "",
  "signature": "sha256:422ab4e7967e991e85eeaaefd9749ba24b535391b9558a9c451067d8a69016e3"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from scipy.optimize import fmin_l_bfgs_b\n",
      "from sklearn import cross_validation\n",
      "from sklearn import preprocessing\n",
      "from sklearn import decomposition\n",
      "from sklearn import datasets\n",
      "import numpy as np\n",
      "import scipy as sp\n",
      "import random\n",
      "import time"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Load Data"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "NUM_OF_CLASSES = 0\n",
      "def load_data1():\n",
      "    \"\"\"Load keggle\"\"\"\n",
      "    global NUM_OF_CLASSES\n",
      "    X_train = np.loadtxt(open(\"../data/train.csv\",\"rb\"),delimiter=\",\",skiprows=1, usecols=range(1,94))\n",
      "    X_test = np.loadtxt(open(\"../data/test.csv\",\"rb\"),delimiter=\",\",skiprows=1, usecols=range(1,94))\n",
      "    \n",
      "    y_train = np.loadtxt(open(\"../data/train.csv\",\"rb\"),dtype=str,delimiter=\",\",skiprows=1, usecols=[94])\n",
      "    y_train = np.array([int(c[-2])-1 for c in y_train])  # Parse classes from Class_1 into 1\n",
      "    return X_train, y_train, X_test\n",
      "\n",
      "def load_data2():\n",
      "    \"\"\"Load iris\"\"\"\n",
      "    iris = datasets.load_iris()\n",
      "    X_train = iris.data\n",
      "    y_train = iris.target.T\n",
      "    return X_train, y_train, None"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "data = 1\n",
      "if data == 1:\n",
      "    X_train, Y_train, X_test = load_data1()\n",
      "else:\n",
      "    X_train, Y_train, X_test = load_data2()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X_train[:5, :4]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 4,
       "text": [
        "array([[ 1.,  0.,  0.,  0.],\n",
        "       [ 0.,  0.,  0.,  0.],\n",
        "       [ 0.,  0.,  0.,  0.],\n",
        "       [ 1.,  0.,  0.,  1.],\n",
        "       [ 2.,  1.,  0.,  0.]])"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "NUM_OF_CLASSES = len( np.unique(Y_train))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Preprocesse Data"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def shuffle(X, Y):\n",
      "    ind = np.array(list(range(X.shape[0])))\n",
      "    np.random.shuffle(ind)\n",
      "    X = np.array( [ X[i] for i in ind] )\n",
      "    Y = np.array( [ Y[i] for i in ind] )\n",
      "    return X, Y"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X_train, Y_train = shuffle(X_train, Y_train)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def binaryClassRepresentation(Y):\n",
      "    \"\"\" Class 1 write as 1,0,0,0,0,0,0,0,0, class 2 write as 0,1,0,0,0,0,0,0,0, ... \"\"\"\n",
      "    I = np.identity(NUM_OF_CLASSES)\n",
      "    Y = np.array( [ I[y] for y in Y])\n",
      "    return Y"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "Y_train = binaryClassRepresentation(Y_train)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 9
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Helper functions"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def add_ones(X):\n",
      "    return np.column_stack((X, np.ones(X.shape[0])))\n",
      "\n",
      "def add_one(X):\n",
      "    return np.append(X, 1)\n",
      "\n",
      "def add_zero(X):\n",
      "    return np.append(X, 0)\n",
      "\n",
      "def del_one(X):\n",
      "    return X[:-1]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 40
    },
    {
     "cell_type": "heading",
     "level": 6,
     "metadata": {},
     "source": [
      "Sigmoid"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def sigmoid(z):\n",
      "    return 1/(1+np.exp(-z))\n",
      "\n",
      "def sigmoid_derivative(z):\n",
      "    return np.multiply(z , 1 - z)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 11
    },
    {
     "cell_type": "heading",
     "level": 6,
     "metadata": {},
     "source": [
      "Tansigmoid"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def tansigmoid(z):\n",
      "    return ( np.exp(z) - np.exp(-z) ) / (np.exp(z) + np.exp(-z))\n",
      "\n",
      "def tansigmoid_derivative(z):\n",
      "    return 1 - tansigmoid(z)**2"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 12
    },
    {
     "cell_type": "heading",
     "level": 6,
     "metadata": {},
     "source": [
      "Arctan"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Trying to make our own function\n",
      "def arctan(z):\n",
      "    return np.arctan(2*z) / np.pi + 1/2\n",
      "\n",
      "def arctan_derivative(z):\n",
      "    return 2 / (((2*z)**2 + 1) * np.pi)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 13
    },
    {
     "cell_type": "heading",
     "level": 6,
     "metadata": {},
     "source": [
      "Test"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def grad_approx(J, thetas, e=1e-5):\n",
      "        return np.array([(J(thetas+eps) - J(thetas-eps))/(2*e)\n",
      "                         for eps in np.identity(len(thetas)) * e])\n",
      "    \n",
      "m = np.array([1,2,3])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 14
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print(\"tansigmoid error\", tansigmoid_derivative(m) - grad_approx(tansigmoid, m).diagonal())\n",
      "print(\"arctan error\", arctan_derivative(m) - grad_approx(arctan, m).diagonal())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "tansigmoid error [ -1.15246146e-11  -9.51938528e-12   5.97277783e-12]\n",
        "arctan error [ -6.15821283e-12  -2.79085088e-12   4.11880252e-12]\n"
       ]
      }
     ],
     "prompt_number": 15
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Learner"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class NeuralNetLearner():\n",
      "    def __init__(self, arch, g=sigmoid, g_derivative=sigmoid_derivative, lambda_=1e-4, maxiter=1, learning_rate=1, dropout=0.1, lbfgs=True):\n",
      "        self.arch = arch\n",
      "        self.g = g\n",
      "        self.g_derivative = g_derivative\n",
      "        self.lambda_ = lambda_\n",
      "        self.maxiter = maxiter\n",
      "        self.learning_rate = learning_rate\n",
      "        self.dropout = dropout\n",
      "        \n",
      "        self.n_levels = len(arch)\n",
      "        \n",
      "        self.theta_shape = [ ( arch[i] + 1, arch[i + 1]) for i in range( len( arch) - 1)]\n",
      "        ind = [ sh1 * sh2 for sh1, sh2 in self.theta_shape]\n",
      "        self.theta_ind = np.cumsum( ind[:-1])  # last index falls out of the boundary\n",
      "        self.theta_len = sum(ind)\n",
      "        self.lbfgs = lbfgs\n",
      "        \n",
      "    \n",
      "    def init_thetas(self, epsilon=1e-1):\n",
      "        \"\"\"Return thetas with random values\"\"\"\n",
      "        #np.random.seed(42)\n",
      "        return np.random.rand(self.theta_len) * 2 * epsilon - epsilon\n",
      "    \n",
      "    \n",
      "    def shape_thetas(self, thetas):\n",
      "        \"\"\"Return list of thetas at particular level\"\"\"\n",
      "        t = np.split(thetas, self.theta_ind)\n",
      "        return [t[i].reshape(shape) for i, \n",
      "                          shape in enumerate(self.theta_shape)]\n",
      "    \n",
      "    \n",
      "    def add_ones(self, X):\n",
      "        return np.column_stack((X, np.ones(X.shape[0])))\n",
      "    \n",
      "    \n",
      "    def del_ones(self, X):\n",
      "        return X[:, :-1]\n",
      "    \n",
      "    \n",
      "    def feedforward(self, a_, thetas):\n",
      "        \"\"\"Feed forward, prediction, add ones to the end for bias\"\"\"\n",
      "        thetas = self.shape_thetas(thetas)\n",
      "        activations = [0] * self.n_levels\n",
      "        activations[0] = a_\n",
      "        for i in range(self.n_levels - 1): \n",
      "            activations[i+1] = self.g(np.dot(add_one(activations[i]), thetas[i]))\n",
      "        return activations\n",
      "    \n",
      "    \n",
      "    def feedforwardMatrix(self, A, thetas):\n",
      "        \"\"\"Feed forward, prediction, add ones to the end for bias\"\"\"\n",
      "        thetas = self.shape_thetas(thetas)\n",
      "        activations = [0] * self.n_levels\n",
      "        activations[0] = A\n",
      "        for i in range(self.n_levels - 1): \n",
      "            activations[i+1] = self.g(self.add_ones(activations[i]).dot(thetas[i]))\n",
      "        return activations\n",
      "    \n",
      "    \n",
      "    def backprop(self, x_, y_, thetas):\n",
      "        \"\"\"Add bias to the end\"\"\"\n",
      "        # Compute activations for each level. It includes first level too\n",
      "        activations = self.feedforward( x_, thetas)\n",
      "        # Compute error at the last level\n",
      "        err = - np.multiply( y_ - activations[-1],  \n",
      "                             self.g_derivative( activations[-1])) \n",
      "        # Prepare space for computing errors on all layers and insert output level\n",
      "        errors = [0] * self.n_levels\n",
      "        errors[self.n_levels - 1] = err \n",
      "        # Reshape thetas\n",
      "        thetas = self.shape_thetas( thetas)\n",
      "        # Compute errors on hidden layers and input layer. Start from behind\n",
      "        for l in range(self.n_levels - 2, -1, -1):  \n",
      "            th_ = err.dot(thetas[l].T) \n",
      "            ac_ = add_one( activations[l] ) # add bias\n",
      "            ac_ = np.multiply( ac_, (1 - ac_))\n",
      "            errors[l] = np.multiply( th_, ac_)\n",
      "            err = del_one(errors[l])\n",
      "\n",
      "        return errors, activations\n",
      "    \n",
      "    \n",
      "    def backpropMatrix(self, X, Y, thetas):\n",
      "        \"\"\"Add bias to the end\"\"\"\n",
      "        # Compute activations for each level. It includes first level too\n",
      "        activations = self.feedforwardMatrix( X, thetas)\n",
      "        # Compute error at the last level\n",
      "        err = - np.multiply( Y - activations[-1],  \n",
      "                             self.g_derivative( activations[-1])) \n",
      "        # Prepare space for computing errors on all layers and insert output level\n",
      "        errors = [0] * self.n_levels\n",
      "        errors[self.n_levels - 1] = err \n",
      "        # Reshape thetas\n",
      "        thetas = self.shape_thetas( thetas)\n",
      "        # Compute errors on hidden layers and input layer. Start from behind\n",
      "        for l in range(self.n_levels - 2, 0, -1):\n",
      "            new_err = err.dot(thetas[l].T) \n",
      "            act = self.add_ones( activations[l] ) # add bias\n",
      "            act = self.g_derivative( act )\n",
      "            errors[l] = np.multiply( new_err, act)\n",
      "            err = errors[l]\n",
      "            err = self.del_ones(errors[l])\n",
      "\n",
      "        return errors, activations\n",
      "    \n",
      "    \n",
      "    def l2_reg( self, thetas):\n",
      "        \"\"\"Regularization\"\"\"\n",
      "        thetas2 = thetas * thetas\n",
      "        thetas2 = self.shape_thetas( thetas2)\n",
      "        sum_ = 0\n",
      "        for levelthetas in thetas2:\n",
      "            for i in range( levelthetas.shape[0] - 1):\n",
      "                sum_ += np.sum( levelthetas[i])\n",
      "        return sum_ * self.lambda_ / 2\n",
      "    \n",
      "    \n",
      "    def l2_reg_grad( self, thetas):\n",
      "        \"\"\"Gradient of regularization\"\"\"\n",
      "        thetasModified = np.array(self.shape_thetas( thetas))\n",
      "        for i in range( len( thetasModified)):\n",
      "            thetasModified[i][-1] *= 0\n",
      "        return self.lambda_ * thetasModified\n",
      "    \n",
      "    \n",
      "    # TODO self.m, self.X, self.Y, self.lambda_\n",
      "    def J(self, thetas):\n",
      "        \"\"\"Cost funciton\"\"\"\n",
      "        sum_ = 0\n",
      "        m = self.X.shape[0]\n",
      "        for i in range( m):\n",
      "            sum_ += 1/(2*m) * np.sum( np.square( self.feedforward( self.X[i], thetas)[-1] - self.Y[i])) \n",
      "        sum_ = sum_ + self.l2_reg( thetas)\n",
      "        return sum_\n",
      "\n",
      "    \n",
      "    def Jgrad(self, thetas):\n",
      "        \"\"\"Gradient\"\"\"\n",
      "        m = self.X.shape[0]\n",
      "        errorsANDactivations = [ self.backprop( self.X[i], self.Y[i], thetas) for i in range(self.X.shape[0])]\n",
      "        grad = [0] * (self.n_levels-1)\n",
      "        for errors, activations in errorsANDactivations:\n",
      "            for l in range(self.n_levels-1):\n",
      "                err = del_one(errors[l+1]) if l != self.n_levels-2 else errors[l+1]\n",
      "                cur_grad = np.matrix(add_one(activations[l])).T.dot(np.matrix(err))\n",
      "                grad[l] = grad[l] + cur_grad\n",
      "        \n",
      "        grad = np.array(grad) / m + self.l2_reg_grad( thetas)\n",
      "        flatten = np.array([])\n",
      "        for matrix in grad:\n",
      "            flatten = np.append(flatten, matrix.ravel())\n",
      "        return flatten\n",
      "    \n",
      "    \n",
      "    def JgradMatrix(self, thetas):\n",
      "        \"\"\"Gradient\"\"\"\n",
      "        m = self.X.shape[0]\n",
      "        errors, activations = self.backpropMatrix( self.X, self.Y, thetas)     \n",
      "        grad = [0] * (self.n_levels-1)\n",
      "        for i in range(m):\n",
      "            for l in range(self.n_levels-1):\n",
      "                err = del_one(errors[l+1][i]) if l != self.n_levels-2 else errors[l+1][i]\n",
      "                cur_grad = np.matrix(add_one(activations[l][i])).T.dot(np.matrix(err))\n",
      "                grad[l] = grad[l] + cur_grad\n",
      "        \n",
      "        grad = np.array(grad) / m + self.l2_reg_grad( thetas)\n",
      "        flatten = np.array([])\n",
      "        for matrix in grad:\n",
      "            flatten = np.append(flatten, matrix.ravel())\n",
      "        return flatten\n",
      "    \n",
      "    \n",
      "    def fit(self, X, Y, thetas=None):\n",
      "        \n",
      "        self.norm = preprocessing.Normalizer()\n",
      "        self.X = self.norm.fit_transform(X)\n",
      "        self.Y = Y\n",
      "        self.thetas = thetas if thetas != None else self.init_thetas() \n",
      "            \n",
      "        if self.lbfgs:\n",
      "            self.thetas, fmin, info = fmin_l_bfgs_b(func = self.J,\n",
      "                                          x0 = self.thetas,\n",
      "                                           fprime = self.JgradMatrix,\n",
      "                                           #maxiter=self.maxiter,\n",
      "                                           #factr=10\n",
      "                                           )   \n",
      "            print(\"info:\", info)\n",
      "        else:\n",
      "            grad = self.Jgrad(self.thetas)\n",
      "            grad = np.array([g if random.random() > self.dropout else 0 for g in grad])\n",
      "            self.thetas = self.thetas - self.learning_rate * grad\n",
      "            \n",
      "        return self.thetas\n",
      "    \n",
      "    def predict(self, X):\n",
      "        X = self.norm.transform(X)\n",
      "        return np.array( [ self.feedforward(x_, self.thetas)[-1] for x_ in X])\n",
      "    \n",
      "    def get_params(self, *args, **argss):\n",
      "        return {'arch':self.arch, 'g':self.g,\n",
      "                'g_derivative': self.g_derivative,\n",
      "                'lambda_':self.lambda_,\n",
      "                'maxiter':self.maxiter,\n",
      "                'learning_rate':self.learning_rate,\n",
      "                'dropout':self.dropout,\n",
      "                'lbfgs':self.lbfgs}\n",
      "                    \n",
      "    def predict_proba(self, X):\n",
      "        return self.predict(X)\n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 41
    },
    {
     "cell_type": "heading",
     "level": 6,
     "metadata": {},
     "source": [
      "Test feed forward and cost function"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Test feed forward OK\n",
      "\n",
      "threshold = 1\n",
      "lenght = 4\n",
      "\n",
      "X = X_train[0:threshold, :lenght]\n",
      "Y = Y_train[0:threshold]\n",
      "lr = NeuralNetLearner([X.shape[1], 2,  NUM_OF_CLASSES]) \n",
      "thetas = np.array([1.5] * lr.theta_len)\n",
      "print( \"Test feedforward:\\n\", lr.feedforward(X, thetas))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Test feedforward:\n",
        " [array([[ 0.,  0.,  0.,  0.]]), array([ 0.81757448,  0.81757448]), array([ 0.98115945,  0.98115945,  0.98115945,  0.98115945,  0.98115945,\n",
        "        0.98115945,  0.98115945,  0.98115945,  0.98115945])]\n"
       ]
      }
     ],
     "prompt_number": 17
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Test feed forward matrix OK\n",
      "\n",
      "threshold = 1\n",
      "lenght = 4\n",
      "\n",
      "X = X_train[0:threshold, :lenght]\n",
      "Y = Y_train[0:threshold]\n",
      "lr = NeuralNetLearner([X.shape[1], 2,  NUM_OF_CLASSES]) \n",
      "thetas = np.array([1.5] * lr.theta_len)\n",
      "print( \"Test feedforward:\\n\", lr.feedforwardMatrix(X, thetas))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Test feedforward:\n",
        " [array([[ 0.,  0.,  0.,  0.]]), array([[ 0.81757448,  0.81757448]]), array([[ 0.98115945,  0.98115945,  0.98115945,  0.98115945,  0.98115945,\n",
        "         0.98115945,  0.98115945,  0.98115945,  0.98115945]])]\n"
       ]
      }
     ],
     "prompt_number": 18
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Test regularization OK\n",
      "\n",
      "threshold = 1\n",
      "lenght = 4\n",
      "\n",
      "X = X_train[0:threshold, :lenght]\n",
      "Y = Y_train[0:threshold]\n",
      "lr = NeuralNetLearner([X.shape[1], 2,  NUM_OF_CLASSES]) \n",
      "lr.lambda_ = 1e-5\n",
      "thetas = np.array([1.5] * lr.theta_len)\n",
      "lr.l2_reg(thetas)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 19,
       "text": [
        "0.00029250000000000001"
       ]
      }
     ],
     "prompt_number": 19
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Test cost function OK\n",
      "\n",
      "threshold = 1\n",
      "lenght = 4\n",
      "\n",
      "X = X_train[0:threshold, :lenght]\n",
      "Y = Y_train[0:threshold]\n",
      "lr = NeuralNetLearner([X.shape[1], 2,  NUM_OF_CLASSES]) \n",
      "thetas = np.array([1.5] * lr.theta_len)\n",
      "lr.X = X\n",
      "lr.Y = Y\n",
      "print( \"Cost function:\", lr.J(thetas))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Cost function: 3.85379793484\n"
       ]
      }
     ],
     "prompt_number": 20
    },
    {
     "cell_type": "heading",
     "level": 6,
     "metadata": {},
     "source": [
      "Test backprop and gradient"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "threshold = 1\n",
      "lenght = 4\n",
      "\n",
      "X = X_train[0:threshold, :lenght]\n",
      "Y = Y_train[0:threshold]\n",
      "lr = NeuralNetLearner([X.shape[1], 2,  NUM_OF_CLASSES]) \n",
      "thetas = np.array([1.5] * lr.theta_len)\n",
      "lr.X = X\n",
      "lr.Y = Y\n",
      "lr.backpropMatrix(X, Y, thetas)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 21,
       "text": [
        "([0,\n",
        "  array([[ 0.03238346,  0.03238346,  0.        ]]),\n",
        "  array([[ 0.01813731, -0.00034828,  0.01813731,  0.01813731,  0.01813731,\n",
        "           0.01813731,  0.01813731,  0.01813731,  0.01813731]])],\n",
        " [array([[ 0.,  0.,  0.,  0.]]),\n",
        "  array([[ 0.81757448,  0.81757448]]),\n",
        "  array([[ 0.98115945,  0.98115945,  0.98115945,  0.98115945,  0.98115945,\n",
        "           0.98115945,  0.98115945,  0.98115945,  0.98115945]])])"
       ]
      }
     ],
     "prompt_number": 21
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Test Jgrad OK\n",
      "\n",
      "threshold = 1\n",
      "lenght = 4\n",
      "\n",
      "X = X_train[0:threshold, :lenght]\n",
      "Y = Y_train[0:threshold]\n",
      "lr = NeuralNetLearner([X.shape[1], 2,  NUM_OF_CLASSES]) \n",
      "thetas = np.array([1.5] * lr.theta_len)\n",
      "lr.X = X\n",
      "lr.Y = Y\n",
      "print( \"grad =\", lr.Jgrad(thetas))  # = -2.310293595541139e-12"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "grad = [ 0.00015     0.00015     0.00015     0.00015     0.00015     0.00015\n",
        "  0.00015     0.00015     0.03238346  0.03238346  0.0149786  -0.00013474\n",
        "  0.0149786   0.0149786   0.0149786   0.0149786   0.0149786   0.0149786\n",
        "  0.0149786   0.0149786  -0.00013474  0.0149786   0.0149786   0.0149786\n",
        "  0.0149786   0.0149786   0.0149786   0.0149786   0.01813731 -0.00034828\n",
        "  0.01813731  0.01813731  0.01813731  0.01813731  0.01813731  0.01813731\n",
        "  0.01813731]\n"
       ]
      }
     ],
     "prompt_number": 22
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def grad_approx(lr, thetas, e=1e-4):\n",
      "    \"\"\"Aproximation\"\"\"\n",
      "    return np.array([(lr.J(thetas+eps) - lr.J(thetas-eps))/(2*e)\n",
      "                     for eps in np.identity(len(thetas)) * e]).ravel()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 23
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Test grad aprox OK\n",
      "\n",
      "threshold = 1\n",
      "lenght = 4\n",
      "\n",
      "X = X_train[0:threshold, :lenght]\n",
      "Y = Y_train[0:threshold]\n",
      "lr = NeuralNetLearner([X.shape[1], 2,  NUM_OF_CLASSES]) \n",
      "thetas = np.array([1.5] * lr.theta_len)\n",
      "lr.X = X\n",
      "lr.Y = Y\n",
      "print( \"grad =\\n\", grad_approx(lr, thetas))  # = -2.310293595541139e-12"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "grad =\n",
        " [ 0.00015     0.00015     0.00015     0.00015     0.00015     0.00015\n",
        "  0.00015     0.00015     0.03238346  0.03238346  0.0149786  -0.00013474\n",
        "  0.0149786   0.0149786   0.0149786   0.0149786   0.0149786   0.0149786\n",
        "  0.0149786   0.0149786  -0.00013474  0.0149786   0.0149786   0.0149786\n",
        "  0.0149786   0.0149786   0.0149786   0.0149786   0.01813731 -0.00034828\n",
        "  0.01813731  0.01813731  0.01813731  0.01813731  0.01813731  0.01813731\n",
        "  0.01813731]\n"
       ]
      }
     ],
     "prompt_number": 24
    },
    {
     "cell_type": "heading",
     "level": 6,
     "metadata": {},
     "source": [
      "JgradMatrix"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "lr = NeuralNetLearner([X.shape[1], 2,  NUM_OF_CLASSES]) \n",
      "thetas = np.array([1.5] * lr.theta_len)\n",
      "lr.X = X\n",
      "lr.Y = Y\n",
      "lr.JgradMatrix(thetas)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 25,
       "text": [
        "array([ 0.00015   ,  0.00015   ,  0.00015   ,  0.00015   ,  0.00015   ,\n",
        "        0.00015   ,  0.00015   ,  0.00015   ,  0.03238346,  0.03238346,\n",
        "        0.0149786 , -0.00013474,  0.0149786 ,  0.0149786 ,  0.0149786 ,\n",
        "        0.0149786 ,  0.0149786 ,  0.0149786 ,  0.0149786 ,  0.0149786 ,\n",
        "       -0.00013474,  0.0149786 ,  0.0149786 ,  0.0149786 ,  0.0149786 ,\n",
        "        0.0149786 ,  0.0149786 ,  0.0149786 ,  0.01813731, -0.00034828,\n",
        "        0.01813731,  0.01813731,  0.01813731,  0.01813731,  0.01813731,\n",
        "        0.01813731,  0.01813731])"
       ]
      }
     ],
     "prompt_number": 25
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "lr = NeuralNetLearner([X.shape[1], 7,  NUM_OF_CLASSES]) \n",
      "thetas = np.array([1.5] * lr.theta_len)\n",
      "lr.X = X\n",
      "lr.Y = Y\n",
      "lr.JgradMatrix(thetas)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 26,
       "text": [
        "array([  1.50000000e-04,   1.50000000e-04,   1.50000000e-04,\n",
        "         1.50000000e-04,   1.50000000e-04,   1.50000000e-04,\n",
        "         1.50000000e-04,   1.50000000e-04,   1.50000000e-04,\n",
        "         1.50000000e-04,   1.50000000e-04,   1.50000000e-04,\n",
        "         1.50000000e-04,   1.50000000e-04,   1.50000000e-04,\n",
        "         1.50000000e-04,   1.50000000e-04,   1.50000000e-04,\n",
        "         1.50000000e-04,   1.50000000e-04,   1.50000000e-04,\n",
        "         1.50000000e-04,   1.50000000e-04,   1.50000000e-04,\n",
        "         1.50000000e-04,   1.50000000e-04,   1.50000000e-04,\n",
        "         1.50000000e-04,   7.46587910e-05,   7.46587910e-05,\n",
        "         7.46587910e-05,   7.46587910e-05,   7.46587910e-05,\n",
        "         7.46587910e-05,   7.46587910e-05,   1.84104868e-04,\n",
        "         1.49998577e-04,   1.84104868e-04,   1.84104868e-04,\n",
        "         1.84104868e-04,   1.84104868e-04,   1.84104868e-04,\n",
        "         1.84104868e-04,   1.84104868e-04,   1.84104868e-04,\n",
        "         1.49998577e-04,   1.84104868e-04,   1.84104868e-04,\n",
        "         1.84104868e-04,   1.84104868e-04,   1.84104868e-04,\n",
        "         1.84104868e-04,   1.84104868e-04,   1.84104868e-04,\n",
        "         1.49998577e-04,   1.84104868e-04,   1.84104868e-04,\n",
        "         1.84104868e-04,   1.84104868e-04,   1.84104868e-04,\n",
        "         1.84104868e-04,   1.84104868e-04,   1.84104868e-04,\n",
        "         1.49998577e-04,   1.84104868e-04,   1.84104868e-04,\n",
        "         1.84104868e-04,   1.84104868e-04,   1.84104868e-04,\n",
        "         1.84104868e-04,   1.84104868e-04,   1.84104868e-04,\n",
        "         1.49998577e-04,   1.84104868e-04,   1.84104868e-04,\n",
        "         1.84104868e-04,   1.84104868e-04,   1.84104868e-04,\n",
        "         1.84104868e-04,   1.84104868e-04,   1.84104868e-04,\n",
        "         1.49998577e-04,   1.84104868e-04,   1.84104868e-04,\n",
        "         1.84104868e-04,   1.84104868e-04,   1.84104868e-04,\n",
        "         1.84104868e-04,   1.84104868e-04,   1.84104868e-04,\n",
        "         1.49998577e-04,   1.84104868e-04,   1.84104868e-04,\n",
        "         1.84104868e-04,   1.84104868e-04,   1.84104868e-04,\n",
        "         1.84104868e-04,   1.84104868e-04,   4.17146923e-05,\n",
        "        -1.74033336e-09,   4.17146923e-05,   4.17146923e-05,\n",
        "         4.17146923e-05,   4.17146923e-05,   4.17146923e-05,\n",
        "         4.17146923e-05,   4.17146923e-05])"
       ]
      }
     ],
     "prompt_number": 26
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Difference OK 9.16699325959e-11\n",
      "# This may not work after dropout application\n",
      "\n",
      "threshold = 1\n",
      "lenght = 4\n",
      "\n",
      "X = X_train[0:threshold, :lenght]\n",
      "Y = Y_train[0:threshold]\n",
      "lr = NeuralNetLearner([X.shape[1], 2,  NUM_OF_CLASSES]) \n",
      "thetas = np.array([1.5] * lr.theta_len)\n",
      "lr.X = X\n",
      "lr.Y = Y\n",
      "print( \"Grad diff =\", np.sum( grad_approx(lr, thetas) - lr.Jgrad(thetas) ) )"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Grad diff = 4.47578588612e-10\n"
       ]
      }
     ],
     "prompt_number": 27
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Cross Validation"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn import metrics\n",
      "\n",
      "def evaluate(pred, real):\n",
      "    return metrics.log_loss(real, pred)\n",
      "\n",
      "def my_cross_validation(X, Y, model, k=3):\n",
      "    S = []\n",
      "    kf = cross_validation.KFold(len(Y), n_folds=k, shuffle=False)\n",
      "    for train_index, test_index in kf:\n",
      "        print(\"New fold\")\n",
      "        # Split\n",
      "        X_train, X_test = X[train_index], X[test_index]\n",
      "        Y_train, Y_test = Y[train_index], Y[test_index]\n",
      "        \n",
      "        model.fit(X_train, Y_train)\n",
      "        predicted = model.predict(X_test)\n",
      "        #print(predicted.shape)\n",
      "        # Evaluate\n",
      "        score = evaluate(predicted, Y_test)\n",
      "        print(\"                                             ### Vmesni rezultat ###: \", score)\n",
      "        S.append( score )\n",
      "    S = np.array(S)\n",
      "    return(np.mean(S))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 28
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Batch Stohastic"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# stohastic training:   size = 1\n",
      "# batch training:       size = num of samples \n",
      "# batch sequential:     otherwise\n",
      "class BatchSeq():\n",
      "    def __init__(self, learner, size=0, iterations=500):\n",
      "        self.learner = learner\n",
      "        self.size = size\n",
      "        self.iterations = iterations\n",
      "\n",
      "        \n",
      "    def fit_thetas(self, X, y_, thetas=None):\n",
      "        # num of samples = 1130 = 0, 100, ..., 900, 1000, 1130 (last block takes all remainded)\n",
      "        split = [ [lo, lo + self.size] \n",
      "                          for lo in range( 0, X.shape[0], self.size)\n",
      "                        ] if X.shape[0] >= self.size else [[0, 0]]\n",
      "        split[-1][-1] = X.shape[0]\n",
      "        counter = 0\n",
      "        for lo, hi in split:\n",
      "            counter += 1\n",
      "            thetas = self.learner.fit( X[lo:hi], y_[lo:hi], thetas=thetas)\n",
      "        return thetas\n",
      "    \n",
      "    def fit(self, X, Y):\n",
      "        # batch vs stohastic\n",
      "        if self.size <= 0:\n",
      "            self.size = X.shape[0]\n",
      "        \n",
      "        # stop criteria\n",
      "        eps = 1e-7\n",
      "        \n",
      "        # init thetas and grad. first step\n",
      "        thetas = self.learner.init_thetas()\n",
      "        \n",
      "        #random.seed(42)\n",
      "        \n",
      "        for i in range(self.iterations):\n",
      "            start = time.time()\n",
      "            # decrease grad step\n",
      "            #self.learner.grad_step *= 0.95 \n",
      "            # shuffle a bit\n",
      "            \n",
      "            X, Y = shuffle(X, Y)\n",
      "            prev_thetas = thetas\n",
      "            thetas = self.fit_thetas(X, Y, thetas)   \n",
      "            diff = abs(thetas - prev_thetas)\n",
      "            \n",
      "            end = time.time()\n",
      "            print(\"Iter:\", i, \" \\n        Time:\", round( end - start, 2) , \"s    Theta diff:\",sum( diff ))\n",
      "            if (sum( diff > eps ) == 0):\n",
      "                print(\"OK Convergent\")\n",
      "                break        \n",
      "    \n",
      "    def predict(self, X):\n",
      "        return self.learner.predict(X)\n",
      "    \n",
      "    def get_params(self, *args, **argss):\n",
      "        return {'learner':self.learner, 'size':self.size,\n",
      "                'iterations': self.iterations,\n",
      "                }\n",
      "                \n",
      "        \n",
      "    def predict_proba(self, X):\n",
      "        return self.predict(X)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 29
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "threshold = 500\n",
      "hidden = [40]\n",
      "\n",
      "X = X_train[:threshold]\n",
      "Y = Y_train[:threshold]\n",
      "\n",
      "arch = [X.shape[1]] + hidden + [NUM_OF_CLASSES]\n",
      "lr = NeuralNetLearner(arch, lbfgs=False) \n",
      "scores = - cross_validation.cross_val_score(lr, X, Y, cv=3, scoring='log_loss')\n",
      "scores"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 32,
       "text": [
        "array([ 2.14089731,  2.11087294,  2.1172044 ])"
       ]
      }
     ],
     "prompt_number": 32
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Testing learner directly, sklearn CV"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def tester(myCV=False, \n",
      "           threshold=1000, \n",
      "           hidden=[20, 20], \n",
      "           g=sigmoid, \n",
      "           g_derivative=sigmoid_derivative, \n",
      "           lambda_=1e-4, \n",
      "           lbfgs_maxiter=1, \n",
      "           learning_rate=1, \n",
      "           dropout=0.1, \n",
      "           lbfgs=False,\n",
      "           \n",
      "           batch_size=500, \n",
      "           batch_iterations=500, \n",
      "           ):\n",
      "    \n",
      "    \n",
      "    \n",
      "    \n",
      "    X = X_train[:threshold]\n",
      "    Y = Y_train[:threshold]\n",
      "    arch = [X.shape[1]] + hidden + [NUM_OF_CLASSES]\n",
      "    \n",
      "    print(\"Architecture is: \", arch)\n",
      "\n",
      "    learner = NeuralNetLearner(arch, g=g, g_derivative=g_derivative, lambda_=lambda_, maxiter=lbfgs_maxiter, learning_rate=learning_rate, dropout=dropout, lbfgs=lbfgs)\n",
      "    bs = BatchSeq( learner, size=batch_size, iterations=batch_iterations)\n",
      "    if myCV:\n",
      "        score = my_cross_validation(X, Y, bs)\n",
      "    else:\n",
      "        score = - cross_validation.cross_val_score(bs, X, Y, cv=3, scoring='log_loss')\n",
      "    return score"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 50
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Tester CV"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "tester(myCV=False, \n",
      "       threshold=10000, \n",
      "       hidden=[20, 20], \n",
      "       g=sigmoid, \n",
      "       g_derivative=sigmoid_derivative, \n",
      "       lambda_=1e-5, \n",
      "       lbfgs_maxiter=1000000, \n",
      "       learning_rate=0.1, \n",
      "       dropout=0.1, \n",
      "       lbfgs=True,\n",
      "\n",
      "       batch_size=1000, \n",
      "       batch_iterations=1, \n",
      "       )"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Architecture is:  [93, 20, 20, 9]\n",
        "New fold\n",
        "info:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " {'funcalls': 34, 'grad': array([ -3.35484756e-06,  -4.59811571e-06,   1.35909371e-05, ...,\n",
        "        -6.18968131e-04,  -1.50455012e-03,   2.11489550e-03]), 'warnflag': 0, 'task': b'CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH', 'nit': 23}\n",
        "info:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " {'funcalls': 17, 'grad': array([ -8.91736458e-05,   3.40756042e-04,   8.16113870e-04, ...,\n",
        "         3.29252818e-03,  -1.60652215e-03,  -4.17076775e-04]), 'warnflag': 0, 'task': b'CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH', 'nit': 9}\n",
        "info:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " {'funcalls': 34, 'grad': array([  1.80003561e-05,  -7.80279800e-06,  -1.22300936e-04, ...,\n",
        "        -9.12214580e-05,   5.05684041e-03,  -2.40812205e-03]), 'warnflag': 0, 'task': b'CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH', 'nit': 25}\n",
        "info:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " {'funcalls': 77, 'grad': array([  4.51854508e-05,   5.65081999e-05,   3.27034510e-05, ...,\n",
        "         2.47357302e-04,   5.79022887e-04,   2.23575002e-03]), 'warnflag': 0, 'task': b'CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH', 'nit': 62}\n",
        "info:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " {'funcalls': 24, 'grad': array([ -4.62215293e-06,   4.79055756e-05,   1.84093421e-04, ...,\n",
        "        -4.54357670e-04,   9.04837039e-04,   2.99503254e-04]), 'warnflag': 0, 'task': b'CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH', 'nit': 14}\n",
        "info:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " {'funcalls': 39, 'grad': array([ -2.89158913e-05,  -5.42350266e-05,  -3.55786856e-05, ...,\n",
        "         4.68610861e-04,   3.41680696e-04,   8.66001543e-04]), 'warnflag': 0, 'task': b'CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH', 'nit': 30}\n",
        "info:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " {'funcalls': 78, 'grad': array([ -3.86635635e-06,   1.21340930e-04,   5.82503419e-05, ...,\n",
        "        -5.98944578e-04,  -2.12133618e-04,   1.06033064e-03]), 'warnflag': 0, 'task': b'CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH', 'nit': 56}\n",
        "Iter: 0  \n",
        "        Time: 78.56 s    Theta diff: 1585.87107423\n",
        "                                             ### Vmesni rezultat ###: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 1.60961573134\n",
        "New fold\n",
        "info:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " {'funcalls': 31, 'grad': array([ -1.17993860e-06,   9.11830801e-07,  -1.55499619e-06, ...,\n",
        "        -1.43138928e-04,   2.47059188e-05,   1.24006330e-04]), 'warnflag': 0, 'task': b'CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH', 'nit': 24}\n",
        "info:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " {'funcalls': 28, 'grad': array([  1.85356451e-05,  -1.55518918e-05,   1.76943436e-05, ...,\n",
        "         2.24580425e-02,   1.40329246e-04,  -1.46610908e-03]), 'warnflag': 0, 'task': b'CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH', 'nit': 17}\n",
        "info:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " {'funcalls': 61, 'grad': array([ -1.87309026e-04,   1.69940361e-04,  -2.45885550e-04, ...,\n",
        "        -8.34683282e-13,   2.94506029e-03,   2.61710923e-04]), 'warnflag': 0, 'task': b'CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH', 'nit': 48}\n",
        "info:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " {'funcalls': 24, 'grad': array([ -1.23921514e-04,   1.78619692e-04,  -5.76268627e-05, ...,\n",
        "        -1.17759877e-11,   1.00670461e-02,  -2.21681851e-03]), 'warnflag': 0, 'task': b'CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH', 'nit': 13}\n",
        "info:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " {'funcalls': 25, 'grad': array([ -5.74482191e-05,   1.43028172e-04,  -7.13181125e-05, ...,\n",
        "        -5.20507855e-12,   2.68204531e-03,  -4.94888134e-03]), 'warnflag': 0, 'task': b'CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH', 'nit': 13}\n",
        "info:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " {'funcalls': 32, 'grad': array([ -2.84208324e-05,   3.57844118e-05,   9.56178953e-07, ...,\n",
        "        -5.37573026e-11,   2.22117339e-03,   1.50723595e-05]), 'warnflag': 0, 'task': b'CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH', 'nit': 20}\n",
        "info:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " {'funcalls': 56, 'grad': array([  1.67297063e-05,  -4.50524343e-06,   7.39425112e-06, ...,\n",
        "        -1.02251726e-10,  -5.33400546e-04,   1.07897224e-03]), 'warnflag': 0, 'task': b'CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH', 'nit': 44}\n",
        "Iter: 0  \n",
        "        Time: 66.93 s    Theta diff: 1554.78776728\n",
        "                                             ### Vmesni rezultat ###: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 1.87927760142\n",
        "New fold\n",
        "info:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " {'funcalls': 35, 'grad': array([ -2.46345641e-04,   1.21120004e-05,  -1.99171146e-04, ...,\n",
        "         4.44061010e-03,  -2.83124541e-03,   1.86159656e-02]), 'warnflag': 0, 'task': b'CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH', 'nit': 23}\n",
        "info:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " {'funcalls': 24, 'grad': array([ -6.58355887e-04,  -5.34758762e-05,  -4.21954780e-04, ...,\n",
        "        -1.78772608e-04,  -9.82963914e-04,  -7.87646532e-04]), 'warnflag': 0, 'task': b'CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH', 'nit': 13}\n",
        "info:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " {'funcalls': 91, 'grad': array([ -2.78216704e-05,   3.92252007e-05,   1.84293294e-05, ...,\n",
        "        -7.03427669e-04,   1.75296935e-03,  -3.03146313e-05]), 'warnflag': 0, 'task': b'CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH', 'nit': 72}\n",
        "info:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " {'funcalls': 21, 'grad': array([ -1.64378781e-04,   3.34800551e-05,   1.34385711e-05, ...,\n",
        "         3.56758023e-04,   1.51886358e-03,   5.58476807e-04]), 'warnflag': 0, 'task': b'CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH', 'nit': 11}\n",
        "info:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " {'funcalls': 49, 'grad': array([ -1.86483490e-05,   6.80977180e-05,   3.71921953e-05, ...,\n",
        "        -1.42093185e-03,   2.03620883e-03,   4.82998610e-05]), 'warnflag': 0, 'task': b'CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH', 'nit': 36}\n",
        "info:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " {'funcalls': 50, 'grad': array([  9.12368077e-05,  -1.09501705e-06,   7.27320604e-05, ...,\n",
        "        -1.20436223e-03,   1.14561287e-05,   3.57518099e-04]), 'warnflag': 0, 'task': b'CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH', 'nit': 34}\n",
        "info:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " {'funcalls': 49, 'grad': array([ -2.07837112e-04,  -4.25887413e-05,   1.71577549e-05, ...,\n",
        "        -7.58659339e-04,   3.75232005e-03,   2.12176051e-03]), 'warnflag': 0, 'task': b'CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH', 'nit': 33}\n",
        "Iter: 0  \n",
        "        Time: 84.58 s    Theta diff: 1811.92780719\n",
        "                                             ### Vmesni rezultat ###: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.954448048731\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 53,
       "text": [
        "1.4811137938287633"
       ]
      }
     ],
     "prompt_number": 53
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Final Fit-Predict"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def predict(hidden=[20, 20], \n",
      "           g=sigmoid, \n",
      "           g_derivative=sigmoid_derivative, \n",
      "           lambda_=1e-4, \n",
      "           lbfgs_maxiter=1, \n",
      "           learning_rate=1, \n",
      "           dropout=0.1, \n",
      "           lbfgs=False,\n",
      "           \n",
      "           batch_size=500, \n",
      "           batch_iterations=500, \n",
      "           ):\n",
      "    \n",
      "    X = X_train\n",
      "    Y = Y_train\n",
      "    \n",
      "    arch = [X.shape[1]] + hidden + [NUM_OF_CLASSES]\n",
      "\n",
      "    learner = NeuralNetLearner(arch, g=g, g_derivative=g_derivative, lambda_=lambda_, maxiter=lbfgs_maxiter, learning_rate=learning_rate, dropout=dropout, lbfgs=lbfgs)\n",
      "    bs = BatchSeq( learner, size=batch_size, iterations=batch_iterations)\n",
      "    bs.fit(X, Y)\n",
      "    predicted = bs.predict(X_test)\n",
      "    return predicted"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 34
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def write_to_file(predicted, name):\n",
      "    answer = np.matrix(predicted.astype(str))\n",
      "    ids = np.matrix(range(1, answer.shape[0]+1))\n",
      "    answer = np.hstack((ids.T.astype(str), answer))\n",
      "    answer = np.vstack((np.array([\"id\",\"Class_1\",\"Class_2\",\"Class_3\",\"Class_4\",\"Class_5\",\"Class_6\",\"Class_7\",\"Class_8\",\"Class_9\"]), answer))\n",
      "    answer = answer.tolist()\n",
      "    answer = \"\\n\".join( [\",\".join(line) for line in answer] )\n",
      "\n",
      "    fo = open(\"../results2/\"+name+\".csv\", \"wt\")\n",
      "    fo.write(answer)\n",
      "    fo.close()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 31
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "predicted = predict(hidden=[30, 30], \n",
      "           g=sigmoid, \n",
      "           g_derivative=sigmoid_derivative, \n",
      "           lambda_=1e-5, \n",
      "           lbfgs_maxiter=1000000, \n",
      "           learning_rate=0.1, \n",
      "           dropout=0.1, \n",
      "           lbfgs=True,\n",
      "           \n",
      "           batch_size=1000, \n",
      "           batch_iterations=3, \n",
      "           )"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "info: {'funcalls': 38, 'grad': array([  1.62509339e-04,  -1.95785044e-04,  -5.70245537e-05, ...,\n",
        "         6.71054243e-04,  -5.97599178e-03,   7.32896881e-03]), 'warnflag': 0, 'task': b'CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH', 'nit': 24}\n",
        "info:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " {'funcalls': 40, 'grad': array([ -7.79620122e-05,   3.83728197e-05,   4.88795016e-05, ...,\n",
        "         9.18163608e-05,  -1.96322856e-03,   3.86673974e-04]), 'warnflag': 0, 'task': b'CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH', 'nit': 27}\n",
        "info:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " {'funcalls': 57, 'grad': array([  2.17207064e-04,  -2.80967415e-05,   7.10427788e-05, ...,\n",
        "        -2.38268934e-04,  -4.29011528e-04,   4.09648267e-05]), 'warnflag': 0, 'task': b'CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH', 'nit': 40}\n",
        "info:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " {'funcalls': 76, 'grad': array([ -7.55914032e-05,  -3.93268272e-06,   3.93321464e-05, ...,\n",
        "         1.38472590e-05,   8.06898016e-04,   1.00544144e-04]), 'warnflag': 0, 'task': b'CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH', 'nit': 62}\n",
        "info:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " {'funcalls': 48, 'grad': array([ -2.17454835e-05,   1.56949931e-05,   1.29993270e-04, ...,\n",
        "         6.34410883e-04,  -1.19461002e-03,   7.78571532e-04]), 'warnflag': 0, 'task': b'CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH', 'nit': 34}\n",
        "info:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " {'funcalls': 101, 'grad': array([ -1.43892612e-05,   6.31991330e-05,  -1.89865455e-04, ...,\n",
        "         6.82930405e-04,  -5.09210330e-04,   1.17515393e-03]), 'warnflag': 0, 'task': b'CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH', 'nit': 81}\n",
        "info:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " {'funcalls': 72, 'grad': array([ -3.84111255e-05,   6.42516923e-05,  -2.01487417e-04, ...,\n",
        "         6.11815634e-04,  -1.61395540e-04,   7.54709225e-04]), 'warnflag': 0, 'task': b'CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH', 'nit': 56}\n"
       ]
      },
      {
       "ename": "KeyboardInterrupt",
       "evalue": "",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
        "\u001b[1;32m<ipython-input-48-0ee0f6a5fc55>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m            \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m            \u001b[0mbatch_iterations\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m            )\n",
        "\u001b[1;32m<ipython-input-34-272bbdfb6b22>\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(hidden, g, g_derivative, lambda_, lbfgs_maxiter, learning_rate, dropout, lbfgs, batch_size, batch_iterations)\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[0mlearner\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mNeuralNetLearner\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0march\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mg\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mg_derivative\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mg_derivative\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlambda_\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlambda_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmaxiter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlbfgs_maxiter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdropout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdropout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlbfgs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlbfgs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[0mbs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBatchSeq\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0mlearner\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0miterations\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_iterations\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m     \u001b[0mbs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m     \u001b[0mpredicted\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mpredicted\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m<ipython-input-29-ceebbf54b2c2>\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, Y)\u001b[0m\n\u001b[0;32m     42\u001b[0m             \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m             \u001b[0mprev_thetas\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mthetas\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m             \u001b[0mthetas\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_thetas\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mthetas\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m             \u001b[0mdiff\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mabs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mthetas\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mprev_thetas\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m<ipython-input-29-ceebbf54b2c2>\u001b[0m in \u001b[0;36mfit_thetas\u001b[1;34m(self, X, y_, thetas)\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mlo\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msplit\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m             \u001b[0mcounter\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m             \u001b[0mthetas\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearner\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlo\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mhi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlo\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mhi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mthetas\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mthetas\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mthetas\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m<ipython-input-41-46d5150d8ea5>\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, Y, thetas)\u001b[0m\n\u001b[0;32m    182\u001b[0m             self.thetas, fmin, info = fmin_l_bfgs_b(func = self.J,\n\u001b[0;32m    183\u001b[0m                                           \u001b[0mx0\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mthetas\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 184\u001b[1;33m                                            \u001b[0mfprime\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mJgradMatrix\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    185\u001b[0m                                            \u001b[1;31m#maxiter=self.maxiter,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    186\u001b[0m                                            \u001b[1;31m#factr=10\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32mC:\\Python34\\lib\\site-packages\\scipy\\optimize\\lbfgsb.py\u001b[0m in \u001b[0;36mfmin_l_bfgs_b\u001b[1;34m(func, x0, fprime, args, approx_grad, bounds, m, factr, pgtol, epsilon, iprint, maxfun, maxiter, disp, callback)\u001b[0m\n\u001b[0;32m    185\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    186\u001b[0m     res = _minimize_lbfgsb(fun, x0, args=args, jac=jac, bounds=bounds,\n\u001b[1;32m--> 187\u001b[1;33m                            **opts)\n\u001b[0m\u001b[0;32m    188\u001b[0m     d = {'grad': res['jac'],\n\u001b[0;32m    189\u001b[0m          \u001b[1;34m'task'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'message'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32mC:\\Python34\\lib\\site-packages\\scipy\\optimize\\lbfgsb.py\u001b[0m in \u001b[0;36m_minimize_lbfgsb\u001b[1;34m(fun, x0, args, jac, bounds, disp, maxcor, ftol, gtol, eps, maxfun, maxiter, iprint, callback, **unknown_options)\u001b[0m\n\u001b[0;32m    313\u001b[0m                 \u001b[1;31m# minimization routine wants f and g at the current x\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    314\u001b[0m                 \u001b[1;31m# Overwrite f and g:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 315\u001b[1;33m                 \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc_and_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    316\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mtask_str\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mb'NEW_X'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    317\u001b[0m             \u001b[1;31m# new iteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32mC:\\Python34\\lib\\site-packages\\scipy\\optimize\\lbfgsb.py\u001b[0m in \u001b[0;36mfunc_and_grad\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m    264\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    265\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mfunc_and_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 266\u001b[1;33m             \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    267\u001b[0m             \u001b[0mg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mjac\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    268\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mg\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32mC:\\Python34\\lib\\site-packages\\scipy\\optimize\\optimize.py\u001b[0m in \u001b[0;36mfunction_wrapper\u001b[1;34m(*wrapper_args)\u001b[0m\n\u001b[0;32m    280\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfunction_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mwrapper_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    281\u001b[0m         \u001b[0mncalls\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 282\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mfunction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwrapper_args\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    283\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    284\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mncalls\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunction_wrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m<ipython-input-41-46d5150d8ea5>\u001b[0m in \u001b[0;36mJ\u001b[1;34m(self, thetas)\u001b[0m\n\u001b[0;32m    131\u001b[0m         \u001b[0mm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    132\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0mm\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 133\u001b[1;33m             \u001b[0msum_\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mm\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msquare\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeedforward\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mthetas\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mY\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    134\u001b[0m         \u001b[0msum_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msum_\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0ml2_reg\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0mthetas\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    135\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0msum_\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m<ipython-input-41-46d5150d8ea5>\u001b[0m in \u001b[0;36mfeedforward\u001b[1;34m(self, a_, thetas)\u001b[0m\n\u001b[0;32m     45\u001b[0m         \u001b[0mactivations\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0ma_\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_levels\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 47\u001b[1;33m             \u001b[0mactivations\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mg\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0madd_one\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mactivations\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mthetas\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     48\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mactivations\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m<ipython-input-40-feb4e110830c>\u001b[0m in \u001b[0;36madd_one\u001b[1;34m(X)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0madd_one\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0madd_zero\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32mC:\\Python34\\lib\\site-packages\\numpy\\lib\\function_base.py\u001b[0m in \u001b[0;36mappend\u001b[1;34m(arr, values, axis)\u001b[0m\n\u001b[0;32m   3882\u001b[0m         \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mravel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3883\u001b[0m         \u001b[0maxis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3884\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mconcatenate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
        "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
       ]
      }
     ],
     "prompt_number": 48
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "write_to_file(predicted, \"NN_a30-30_s1000_i1_lambda-5_er-1\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 47
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}