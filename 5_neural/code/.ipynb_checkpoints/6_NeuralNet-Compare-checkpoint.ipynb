{
 "metadata": {
  "name": "",
  "signature": "sha256:d24a032f46880aeb185414456a8583a779ed598a9e8bddafc31a9092b8a47836"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from scipy.optimize import fmin_l_bfgs_b\n",
      "from sklearn import cross_validation\n",
      "from sklearn import preprocessing\n",
      "from sklearn import decomposition\n",
      "from sklearn import datasets\n",
      "import numpy as np\n",
      "import scipy as sp"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 21
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Load Data"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X_train_in = np.loadtxt(open(\"../data/train.csv\",\"rb\"),delimiter=\",\",skiprows=1, usecols=range(1,94))\n",
      "\n",
      "y_train_in = np.loadtxt(open(\"../data/train.csv\",\"rb\"),dtype=str,delimiter=\",\",skiprows=1, usecols=[94])\n",
      "y_train_in = np.array([int(c[-2]) for c in y_train_in])  # Parse classes from Class_1 into 1\n",
      "\n",
      "NUM_OF_CLASSES = len( np.unique(y_train_in))  # Warning: number of classes must go from 1 to n\n",
      "\n",
      "X_test_in = np.loadtxt(open(\"../data/test.csv\",\"rb\"),delimiter=\",\",skiprows=1, usecols=range(1,94))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 22
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "iris = datasets.load_iris()\n",
      "X_train_in = iris.data\n",
      "y_train_in = iris.target.T"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Preprocesse Data"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Shuffle\n",
      "XY = np.hstack((X_train_in, np.matrix(y_train_in).T))\n",
      "np.random.shuffle(XY)\n",
      "X_train = XY[:, :X_train_in.shape[1]]\n",
      "y_train = XY[:, X_train_in.shape[1]:]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 23
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Class 1 write as 1,0,0,0,0,0,0,0,0, class 2 write as 0,1,0,0,0,0,0,0,0, ...\n",
      "I = np.identity(NUM_OF_CLASSES)\n",
      "y_train = np.array( [ I[y[0,0]-1] for y in y_train])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 24
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Normalize\n",
      "norm = preprocessing.Normalizer()\n",
      "X_train = norm.fit_transform(X_train_in)\n",
      "X_test = norm.transform(X_test_in)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 25
    },
    {
     "cell_type": "heading",
     "level": 6,
     "metadata": {},
     "source": [
      "Test"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "y_train.shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 26,
       "text": [
        "(50000, 9)"
       ]
      }
     ],
     "prompt_number": 26
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "y_train[:10]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 27,
       "text": [
        "array([[ 0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
        "       [ 0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.],\n",
        "       [ 0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
        "       [ 0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.],\n",
        "       [ 0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.],\n",
        "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.],\n",
        "       [ 0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.],\n",
        "       [ 0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.],\n",
        "       [ 0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
        "       [ 0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.]])"
       ]
      }
     ],
     "prompt_number": 27
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X_train.shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 28,
       "text": [
        "(50000, 93)"
       ]
      }
     ],
     "prompt_number": 28
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X_train[:10,:4]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 29,
       "text": [
        "array([[ 0.06388766,  0.        ,  0.        ,  0.        ],\n",
        "       [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
        "       [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
        "       [ 0.01546166,  0.        ,  0.        ,  0.01546166],\n",
        "       [ 0.11527808,  0.05763904,  0.        ,  0.        ],\n",
        "       [ 0.25819889,  0.        ,  0.        ,  0.        ],\n",
        "       [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
        "       [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
        "       [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
        "       [ 0.        ,  0.05661385,  0.05661385,  0.1132277 ]])"
       ]
      }
     ],
     "prompt_number": 29
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Helper Functions"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def add_ones(X):\n",
      "    return np.column_stack((X, np.ones(X.shape[0])))\n",
      "\n",
      "def add_one(X):\n",
      "    return np.append(X, 1)\n",
      "\n",
      "def add_zero(X):\n",
      "    return np.append(X, 0)\n",
      "\n",
      "def del_one(X):\n",
      "    return X[:-1]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 30
    },
    {
     "cell_type": "heading",
     "level": 6,
     "metadata": {},
     "source": [
      "Sigmoid"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def sigmoid(z):\n",
      "    return 1/(1+np.exp(-z))\n",
      "\n",
      "def sigmoid_derivative(z):\n",
      "    return np.multiply(z , 1 - z)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 31
    },
    {
     "cell_type": "heading",
     "level": 6,
     "metadata": {},
     "source": [
      "Tansigmoid"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def tansigmoid(z):\n",
      "    return ( np.exp(z) - np.exp(-z) ) / (np.exp(z) + np.exp(-z))\n",
      "\n",
      "def tansigmoid_derivative(z):\n",
      "    return 1 - tansigmoid(z)**2"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 32
    },
    {
     "cell_type": "heading",
     "level": 6,
     "metadata": {},
     "source": [
      "Arctan (my try)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def arctan(z):\n",
      "    return np.arctan(2*z) / np.pi + 1/2\n",
      "\n",
      "def arctan_derivative(z):\n",
      "    return 2 / (((2*z)**2 + 1) * np.pi)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 33
    },
    {
     "cell_type": "heading",
     "level": 6,
     "metadata": {},
     "source": [
      "Test"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "m = np.array([1,2,3])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 34
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def grad_approx(J, thetas, e=1e-5):\n",
      "        return np.array([(J(thetas+eps) - J(thetas-eps))/(2*e)\n",
      "                         for eps in np.identity(len(thetas)) * e])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 35
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print(tansigmoid_derivative(m))\n",
      "grad_approx(tansigmoid, m).diagonal()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[ 0.41997434  0.07065082  0.00986604]\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 36,
       "text": [
        "array([ 0.41997434,  0.07065082,  0.00986604])"
       ]
      }
     ],
     "prompt_number": 36
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print(arctan_derivative(m))\n",
      "grad_approx(arctan, m).diagonal()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[ 0.12732395  0.03744822  0.01720594]\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 37,
       "text": [
        "array([ 0.12732395,  0.03744822,  0.01720594])"
       ]
      }
     ],
     "prompt_number": 37
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Classifier"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class NeuralNetClassifier():\n",
      "    \"\"\"Neural network classifier based on a set of binary classifiers.\"\"\"\n",
      "    def __init__(self, h, thetas):\n",
      "        self.thetas = thetas  # model parameters\n",
      "        self.h = h\n",
      "        \n",
      "    def predict(self, X):\n",
      "        return np.array( [ self.h(x_, self.thetas) for x_ in X])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 38
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Learner"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class NeuralNetLearner():\n",
      "    def __init__(self, arch, g=sigmoid, g_derivative=sigmoid_derivative, lambda_=1e-5):\n",
      "        self.arch = arch\n",
      "        self.n_levels = len(arch)\n",
      "        \n",
      "        self.g = g\n",
      "        self.g_derivative = g_derivative\n",
      "        \n",
      "        self.lambda_ = lambda_\n",
      "        self.theta_shape = np.array([ ( arch[i] + 1, arch[i + 1]) for i in range( len( arch) - 1)])\n",
      "        ind = np.array( [ sh1 * sh2 for sh1, sh2 in self.theta_shape])\n",
      "        self.theta_ind = np.cumsum( ind[:-1])  # last index falls out of the boundary\n",
      "        self.theta_len = sum(ind)\n",
      "        self.seed = False\n",
      "    \n",
      "    def init_thetas(self, epsilon=1e-5):\n",
      "        \"\"\"Return thetas with random values\"\"\"\n",
      "        if self.seed: \n",
      "            np.random.seed(42)\n",
      "        return np.random.rand(self.theta_len) * 2 * epsilon - epsilon\n",
      "    \n",
      "    def shape_thetas(self, thetas):\n",
      "        \"\"\"Return list of thetas at particular level\"\"\"\n",
      "        t = np.split(thetas, self.theta_ind)\n",
      "        return np.array( [t[i].reshape(shape) for i, \n",
      "                          shape in enumerate(self.theta_shape)])\n",
      "    \n",
      "    def feedforward(self, a_, thetas):\n",
      "        \"\"\"Feed forward, prediction, add ones to the end for bias\"\"\"\n",
      "        thetas = self.shape_thetas(thetas)\n",
      "        activations = [0] * self.n_levels\n",
      "        activations[0] = a_\n",
      "        for i in range( 1, self.n_levels): \n",
      "            activations[i] = self.g(np.dot(add_one(activations[i-1]), thetas[i-1]))\n",
      "        return np.array(activations)\n",
      "    \n",
      "    def backprop(self, x_, y_, thetas):\n",
      "        \"\"\"Add bias to the end\"\"\"\n",
      "        activations = self.feedforward( x_, thetas)\n",
      "        thetas = self.shape_thetas( thetas)\n",
      "        # output error\n",
      "        err = - np.multiply( y_ - activations[-1],  \n",
      "                             self.g_derivative( activations[-1])) \n",
      "               \n",
      "        errors = [0] * self.n_levels\n",
      "        errors[self.n_levels - 1] = err \n",
      "        \n",
      "        levels = list(range(0, self.n_levels-1))\n",
      "        for l in levels[::-1]:  # reverse\n",
      "            th_ = np.matrix(err) * thetas[l].T \n",
      "            ac_ = add_one( activations[l] ) # add bias\n",
      "            ac_ = np.multiply( ac_, (1 - ac_))\n",
      "            errors[l] = np.array( np.multiply( th_, ac_) )[0]\n",
      "            err = del_one(errors[l])\n",
      "\n",
      "        return np.array(errors), activations\n",
      "    \n",
      "    def l2_reg( self, thetas):\n",
      "        thetas2 = thetas * thetas\n",
      "        thetas2 = self.shape_thetas( thetas2)\n",
      "        sum_ = 0\n",
      "        for levelthetas in thetas2:\n",
      "            for i in range( levelthetas.shape[0] - 1):\n",
      "                sum_ += sum( levelthetas[i])\n",
      "        return sum_ * self.lambda_ / 2\n",
      "    \n",
      "    def l2_reg_grad( self, thetas):\n",
      "        thetasModified = self.shape_thetas( thetas)\n",
      "        for i in range( len( thetasModified)):\n",
      "            thetasModified[i][-1] *= 0\n",
      "        return self.lambda_ * thetasModified\n",
      "    \n",
      "    # TODO self.m, self.X, self.Y, self.lambda_\n",
      "    def J(self, thetas):\n",
      "        sum_ = 0\n",
      "        for i in range( self.X.shape[0]):\n",
      "            sum_ += sum((self.feedforward( self.X[i], thetas)[-1] - self.Y[i])**2)\n",
      "        sum_ = sum_ / (2 * self.X.shape[0]) + self.l2_reg( thetas)\n",
      "        return sum_\n",
      "\n",
      "    def Jgrad(self, thetas):\n",
      "        errorsANDactivations = map( lambda z: self.backprop( z[0], z[1], thetas), zip( self.X, self.Y))\n",
      "        grad = [0] * (self.n_levels-1)\n",
      "        for errors, activations in errorsANDactivations:\n",
      "            for l in range(self.n_levels-1):\n",
      "                err = del_one(errors[l+1]) if l != self.n_levels-2 else errors[l+1]\n",
      "                cur_grad = np.dot(np.matrix( add_one(activations[l]) ).T, np.matrix(err))\n",
      "                grad[l] = grad[l] + cur_grad\n",
      "        \n",
      "        grad = np.array(grad) / self.X.shape[0] + self.l2_reg_grad( thetas)\n",
      "        flatten = np.array([])\n",
      "        for matrix in grad:\n",
      "            flatten = np.append(flatten, matrix.ravel())\n",
      "        return flatten\n",
      "        \n",
      "    def grad_approx(self, thetas, e=1e-1):\n",
      "        return np.array([(self.J(thetas+eps) - self.J(thetas-eps))/(2*e)\n",
      "                         for eps in np.identity(len(thetas)) * e]).ravel()\n",
      "    \n",
      "    def set_data( self, X, Y, thetas=None):\n",
      "        self.X = X\n",
      "        self.Y = Y\n",
      "        self.thetas = thetas if thetas != None else self.init_thetas()\n",
      "    \n",
      "    def fit(self, X, Y, thetas=None):\n",
      "        \"\"\" x_ is a vector, y is a class between 1 and sth\"\"\"\n",
      "        self.set_data(X, Y, thetas)\n",
      "        self.seed = False\n",
      "        self.thetas, _, _ = fmin_l_bfgs_b(func = self.J,\n",
      "                                           x0 = self.thetas,\n",
      "                                           fprime = self.Jgrad,\n",
      "                                           factr=10,\n",
      "                                           maxiter=1000)   \n",
      "        return self.thetas\n",
      "    \n",
      "    def predict(self, X):\n",
      "        return np.array( [ self.feedforward(x_, self.thetas) for x_ in X])\n",
      "        \n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 83
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Testing"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "threshold = 5\n",
      "X = X_train[0:threshold, :4]\n",
      "Y = y_train[0:threshold]\n",
      "#lr = NeuralNetLearner([X.shape[1], 2,  9], tansigmoid, tansigmoid_derivative)  # use this for tangens\n",
      "#lr = NeuralNetLearner([X.shape[1], 2,  9], arctan, arctan_derivative)\n",
      "lr = NeuralNetLearner([X.shape[1], 2,  9])  # use this for logistic funciton\n",
      "r = lr.fit(X, Y)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 40
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Test cost function\n",
      "print( \"Cost function:\", lr.J(lr.thetas))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Cost function: 1.12500022158\n"
       ]
      }
     ],
     "prompt_number": 41
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "# Test atributes and some minor functions of a learner\n",
      "\n",
      "print( \"Testing shape\\n\", lr.theta_shape)\n",
      "print( \"Testing indices\\n\", lr.theta_ind)\n",
      "print( \"Testing theta lenght\\n\", lr.theta_len)\n",
      "print( \"Testing theta shape\\n\", lr.shape_thetas( thetas))"
     ]
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "# Test feed forward\n",
      "\n",
      "print( \"Test feed forward:\")\n",
      "print( lr.feedforward(lr.x_, lr.thetas))"
     ]
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "# Test back propagation\n",
      "\n",
      "print( \"Test errors and activations\")\n",
      "errors, acts = lr.backprop(lr.thetas)\n",
      "print( errors.shape, acts.shape)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Test gradient function \n",
      "\n",
      "print( \"Grad. function:\", lr.Jgrad(lr.thetas))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Grad. function: [  4.43698921e-09  -2.34965384e-08  -2.74407662e-09  -6.97873976e-09\n",
        "   5.44462778e-11  -5.70594149e-11   1.68901087e-09   9.58280170e-11\n",
        "   4.15990067e-07  -6.14506722e-07   6.24995414e-02   1.25001408e-02\n",
        "   6.24995799e-02   3.74997747e-02   3.75000673e-02   3.75000745e-02\n",
        "   6.24997274e-02   6.25000148e-02   6.24994341e-02   6.24996606e-02\n",
        "   1.25001679e-02   6.24996991e-02   3.74998475e-02   3.75001304e-02\n",
        "   3.75001500e-02   6.24998467e-02   6.25001341e-02   6.24995534e-02\n",
        "   1.24999665e-01   2.50004008e-02   1.24999742e-01   7.49998959e-02\n",
        "   7.50004847e-02   7.50004967e-02   1.25000037e-01   1.25000611e-01\n",
        "   1.24999450e-01]\n"
       ]
      }
     ],
     "prompt_number": 42
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Test gradient approximation\n",
      "\n",
      "print( \"Grad. approx:\", lr.grad_approx(lr.thetas, e=1e-5))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Grad. approx: [  4.42978987e-09  -2.34923192e-08  -2.74225087e-09  -6.97220059e-09\n",
        "   5.55111512e-11  -5.55111512e-11   1.67643677e-09   8.88178420e-11\n",
        "   4.15989465e-07  -6.14508444e-07   6.25000429e-02   1.24999796e-02\n",
        "   6.24999766e-02   3.74998015e-02   3.75001583e-02   3.75000711e-02\n",
        "   6.24998321e-02   6.25000673e-02   6.24999793e-02   6.25000524e-02\n",
        "   1.24999848e-02   6.24999860e-02   3.74998085e-02   3.75001556e-02\n",
        "   3.75000808e-02   6.24998417e-02   6.25000768e-02   6.24999888e-02\n",
        "   1.25000082e-01   2.49999613e-02   1.24999949e-01   7.49995983e-02\n",
        "   7.50003154e-02   7.50001387e-02   1.24999661e-01   1.25000131e-01\n",
        "   1.24999955e-01]\n"
       ]
      }
     ],
     "prompt_number": 43
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Test difference\n",
      "print( \"grad - grad.aprox =\", sum( lr.Jgrad(lr.thetas) - lr.grad_approx(lr.thetas, e=1e-5) ))  # = -2.310293595541139e-12"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "grad - grad.aprox = 4.64257733088e-11\n"
       ]
      }
     ],
     "prompt_number": 44
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Cross Validation"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn import metrics\n",
      "\n",
      "def evaluate(pred, real):\n",
      "    return metrics.log_loss(real, pred)\n",
      "\n",
      "def my_cross_validation(X, Y, model, k=3):\n",
      "    S = []\n",
      "    kf = cross_validation.KFold(len(Y), n_folds=k, shuffle=True)\n",
      "    for train_index, test_index in kf:\n",
      "        print(\"New circle\", arch)\n",
      "        # Split\n",
      "        X_train, X_test = X[train_index], X[test_index]\n",
      "        Y_train, Y_test = Y[train_index], Y[test_index]\n",
      "        \n",
      "        model.fit(X_train, Y_train)\n",
      "        predicted = model.predict(X_test)\n",
      "        print(predicted.shape)\n",
      "        # Evaluate\n",
      "        score = evaluate(predicted, Y_test)\n",
      "        print(\"Vmesni rezultat: \", score)\n",
      "        S.append( score )\n",
      "    S = np.array(S)\n",
      "    return(np.mean(S))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 88
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Batch Sequential"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# stohastic training:   size = 1\n",
      "# batch training:       size = num of samples \n",
      "# batch sequential:     otherwise\n",
      "class BatchSeq():\n",
      "    def __init__(self, learner, size=None, alpha=0.1):\n",
      "        self.learner = learner\n",
      "        self.alpha = alpha \n",
      "        self.size = size\n",
      "        \n",
      "    def fit_thetas(self, X, y_, thetas=None):\n",
      "        # num of samples = 1130 = 0, 100, ..., 900, 1000, 1130 (last block takes all remainded)\n",
      "        split = [ [lo, lo + self.size] \n",
      "                          for lo in range( 0, X.shape[0], self.size)\n",
      "                        ] if X.shape[0] >= self.size else [[0, 0]]\n",
      "        split[-1][-1] = X.shape[0]\n",
      "        counter = 0\n",
      "        for lo, hi in split:\n",
      "            print(lo)\n",
      "            counter += 1\n",
      "            thetas = self.learner.fit( X[lo:hi], y_[lo:hi], thetas)\n",
      "        return thetas\n",
      "    \n",
      "    def fit(self, X, y_):\n",
      "        if self.size == None:\n",
      "            self.size = X.shape[0]\n",
      "        eps = 1e-4\n",
      "        thetas = self.learner.init_thetas()\n",
      "        # run until the error is very small ... because lbfgs, we don't need this anymore\n",
      "        for i in range(1):\n",
      "            #prev = thetas\n",
      "            thetas = self.fit_thetas(X, y_, thetas)\n",
      "            #if (sum( abs(thetas - prev) > eps ) == 0):\n",
      "            #    break\n",
      "        #model = NeuralNetClassifier(self.learner.predict, thetas)\n",
      "        #return model         \n",
      "    \n",
      "    def predict(self, X):\n",
      "        return self.learner.predict(X)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 84
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "RUN"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Pca\n",
      "if False:\n",
      "    pca = decomposition.PCA(n_components=20)\n",
      "    X_train = pca.fit_transform(X_train)\n",
      "    X_test = pca.transform(X_test)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 49
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print(X_train.shape, y_train.shape)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(50000, 93) (50000, 9)\n"
       ]
      }
     ],
     "prompt_number": 50
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "threshold = 500\n",
      "X = X_train[:threshold, :]\n",
      "Y = y_train[:threshold]\n",
      "hidden = [ 20, 20 ]\n",
      "arch = [X.shape[1]] + hidden + [NUM_OF_CLASSES]\n",
      "\n",
      "learner = NeuralNetLearner(arch)\n",
      "bs = BatchSeq( learner)\n",
      "score = my_cross_validation(X, Y, bs)\n",
      "print(score)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "New circle [93, 20, 20, 9]\n",
        "0\n",
        "(167, 4)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      },
      {
       "ename": "ValueError",
       "evalue": "The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
        "\u001b[1;32m<ipython-input-89-1b731ee8bb42>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mlearner\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mNeuralNetLearner\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0march\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mbs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBatchSeq\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0mlearner\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0mscore\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmy_cross_validation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m<ipython-input-88-fecc37f76b5b>\u001b[0m in \u001b[0;36mmy_cross_validation\u001b[1;34m(X, Y, model, k)\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredicted\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[1;31m# Evaluate\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m         \u001b[0mscore\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredicted\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Vmesni rezultat: \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscore\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[0mS\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0mscore\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m<ipython-input-88-fecc37f76b5b>\u001b[0m in \u001b[0;36mevaluate\u001b[1;34m(pred, real)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreal\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreal\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mmy_cross_validation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32mC:\\Python34\\lib\\site-packages\\sklearn\\metrics\\metrics.py\u001b[0m in \u001b[0;36mlog_loss\u001b[1;34m(y_true, y_pred, eps, normalize)\u001b[0m\n\u001b[0;32m   1104\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1105\u001b[0m     \u001b[1;31m# Clipping\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1106\u001b[1;33m     \u001b[0mY\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0meps\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1107\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1108\u001b[0m     \u001b[1;31m# This happens in cases when elements in y_pred have type \"str\".\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32mC:\\Python34\\lib\\site-packages\\numpy\\core\\fromnumeric.py\u001b[0m in \u001b[0;36mclip\u001b[1;34m(a, a_min, a_max, out)\u001b[0m\n\u001b[0;32m   1626\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1627\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0m_wrapit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'clip'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ma_min\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ma_max\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1628\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mclip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma_min\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ma_max\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1629\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1630\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;31mValueError\u001b[0m: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()"
       ]
      }
     ],
     "prompt_number": 89
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Find best parameters with Cross Validation"
     ]
    },
    {
     "cell_type": "heading",
     "level": 5,
     "metadata": {},
     "source": [
      "Parameters to test"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "- lambda for regularization\n",
      "- levels\n",
      "- batchsequential size"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "threshold = 5000\n",
      "X = X_train[:threshold, :]\n",
      "Y = y_train_extended[:threshold]\n",
      "\n",
      "\n",
      "# Because the input is much larger than output. Then it is good idea\n",
      "# to build hidden levels in a way that number of hidden states falls \n",
      "# from the length of the input till the length of the output. We need\n",
      "# to specify the length and then compute number off hidden neurons at\n",
      "# each level.\n",
      "\n",
      "lambdas = [0.1, 0.5, 1, 5]\n",
      "lengths = [2, 10, 20]\n",
      "\n",
      "for length in lengths:\n",
      "    \n",
      "    k = - (X_train.shape[1] - NUM_OF_CLASSES) / (length+1)\n",
      "    hidden = [ int( k * j + X_train.shape[1]) for j in range(1, length+1)]\n",
      "    arch = [X.shape[1]] + hidden + [NUM_OF_CLASSES]\n",
      "    \n",
      "    for lam in lambdas:\n",
      "        #score = my_cross_validation(X, Y, arch, lam)\n",
      "        score = 10\n",
      "        with open(\"crossval.txt\", \"a\") as myfile:\n",
      "            myfile.write(str(score)+\"\\t lambda=\"+str(lam)+\"\\t length=\"+str(length)+\"\\t arch=\"+str(arch)+\"\\n\")\n",
      "            myfile.close()\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 252
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Final Fit-Predict"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "hidden = [30, 15]\n",
      "arch = [X_train.shape[1]] + hidden + [NUM_OF_CLASSES]\n",
      "learner = NeuralNetLearner(arch, lambda_=1e-5)\n",
      "bs = BatchSeq( learner, 50000)\n",
      "model = bs.fit(X_train, y_train)\n",
      "predicted = model.predict(X_test)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "predicted = model.predict(X_test)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def write_to_file(predicted, name):\n",
      "    answer = np.matrix(predicted.astype(str))\n",
      "    ids = np.matrix(range(1, answer.shape[0]+1))\n",
      "    answer = np.hstack((ids.T.astype(str), answer))\n",
      "    answer = np.vstack((np.array([\"id\",\"Class_1\",\"Class_2\",\"Class_3\",\"Class_4\",\"Class_5\",\"Class_6\",\"Class_7\",\"Class_8\",\"Class_9\"]), answer))\n",
      "    answer = answer.tolist()\n",
      "    answer = \"\\n\".join( [\",\".join(line) for line in answer] )\n",
      "\n",
      "    fo = open(\"../results/\"+name+\".csv\", \"wt\")\n",
      "    fo.write(answer)\n",
      "    fo.close()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "write_to_file(predicted, \"batch1\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "adaptivni gradient, pri drugem stohasti\u010dnem primeru. \n",
      "Vsako iteracijo manj pristejemo. \n",
      "\n",
      "Lbfgs 1 iteracija za batch seq. "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}