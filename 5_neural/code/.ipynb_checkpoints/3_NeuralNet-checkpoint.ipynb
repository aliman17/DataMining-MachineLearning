{
 "metadata": {
  "name": "",
  "signature": "sha256:b67bb583ffcfcaf660dc5ce9c746084b577a7079f7dc6d831910003743805d86"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from scipy.optimize import fmin_l_bfgs_b\n",
      "from sklearn import cross_validation\n",
      "from sklearn import preprocessing\n",
      "from sklearn import decomposition\n",
      "import numpy as np\n",
      "import scipy as sp"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 21
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Load Data"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "norm = preprocessing.Normalizer()\n",
      "X_train = np.loadtxt(open(\"../data/train.csv\",\"rb\"),delimiter=\",\",skiprows=1, usecols=range(1,94))\n",
      "X_train = norm.fit_transform(X_train)\n",
      "\n",
      "y_train = np.loadtxt(open(\"../data/train.csv\",\"rb\"),dtype=str,delimiter=\",\",skiprows=1, usecols=[94])\n",
      "y_train = np.array([int(c[-2]) for c in y_train])  # Parse classes from Class_1 into 1\n",
      "\n",
      "X_test = np.loadtxt(open(\"../data/test.csv\",\"rb\"),delimiter=\",\",skiprows=1, usecols=range(1,94))\n",
      "X_test = norm.transform(X_test)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 97
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Preprocesse Data"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Shuffle\n",
      "\n",
      "np.random.shuffle(X_train)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 98
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Pca\n",
      "\n",
      "pca = decomposition.PCA(n_components=20)\n",
      "X_train_pca = pca.fit_transform(X_train)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 101
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Normalize \n",
      "\n",
      "# with l1. We want to have linear corelation because we have multiple classes. \n",
      "X_train_normalized = preprocessing.normalize(X_train_pca, norm='l1', axis=0, copy=True)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 102
    },
    {
     "cell_type": "heading",
     "level": 6,
     "metadata": {},
     "source": [
      "Testing box"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Testing normalization\n",
      "m = np.matrix([[0.5,2.0,3.0], [0.5,8.0,3.0], [1,1,1]])\n",
      "normalized_m = preprocessing.normalize(m, axis=0)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 95
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Helper Functions"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def add_ones(X):\n",
      "    return np.column_stack(X, (np.ones(len(X))))\n",
      "\n",
      "def add_one(X):\n",
      "    return np.append(X, 1)\n",
      "\n",
      "def del_one(X):\n",
      "    return X[:-1]\n",
      "\n",
      "def sigmoid(z):\n",
      "    return 1/(1+np.exp(-z))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Learner"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class NeuralNetClassifier():\n",
      "    \"\"\"Neural network classifier based on a set of binary classifiers.\"\"\"\n",
      "    def __init__(self, h, thetas):\n",
      "        self.thetas = thetas  # model parameters\n",
      "        self.h = h\n",
      "        \n",
      "    def predict(self, X):\n",
      "        y_hat = np.ravel(self.h(X, self.thetas))\n",
      "        # following works only for binary classifiers, correct it for multiclass\n",
      "        return np.vstack((1-y_hat, y_hat)).T"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Learner"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class NeuralNetLearner():\n",
      "    def __init__(self, arch, g=sigmoid, lambda_=1e-5):\n",
      "        self.arch = arch\n",
      "        self.n_levels = len(arch)\n",
      "        self.g = g\n",
      "        self.lambda_ = lambda_\n",
      "        self.theta_shape = np.array([ ( arch[i] + 1, arch[i + 1]) for i in range( len( arch) - 1)])\n",
      "        ind = np.array( [ sh1 * sh2 for sh1, sh2 in self.theta_shape])\n",
      "        self.theta_ind = np.cumsum( ind[:-1])  # last index falls out of the boundary\n",
      "        self.theta_len = sum(ind)\n",
      "    \n",
      "    def init_thetas(self, epsilon=1e-5):\n",
      "        \"\"\"Return thetas with random values\"\"\"\n",
      "        np.random.seed(42)\n",
      "        return np.random.rand(self.theta_len) * 2 * epsilon - epsilon\n",
      "    \n",
      "    def shape_thetas(self, thetas):\n",
      "        \"\"\"Return list of thetas at particular level\"\"\"\n",
      "        t = np.split(thetas, self.theta_ind)\n",
      "        return np.array( [t[i].reshape(shape) for i, \n",
      "                          shape in enumerate(self.theta_shape)])\n",
      "    \n",
      "    def feedforward(self, thetas):\n",
      "        \"\"\"Feed forward, prediction, add ones to the end for bias\"\"\"\n",
      "        a_ = self.x_\n",
      "        thetas = self.shape_thetas(thetas)\n",
      "        activationsByLevel = [a_]\n",
      "        for theta in thetas: \n",
      "            a1_ = add_one(a_)  # add bias\n",
      "            a_ = self.g(a1_.dot(theta))\n",
      "            activationsByLevel.append(a_)\n",
      "        return np.array(activationsByLevel)\n",
      "    \n",
      "    def backprop(self, thetas):\n",
      "        \"\"\"Add bias to the end\"\"\"\n",
      "\n",
      "        activations = self.feedforward( thetas)\n",
      "        thetas = self.shape_thetas( thetas)\n",
      "        # output error\n",
      "        err = - np.multiply( self.y_out - activations[-1],  \n",
      "                             np.multiply( activations[-1], 1-activations[-1])) \n",
      "               \n",
      "        errors = [0] * self.n_levels\n",
      "        errors[self.n_levels - 1] = err \n",
      "        \n",
      "        levels = list(range(0, self.n_levels-1))\n",
      "        for l in levels[::-1]:  # reverse\n",
      "            th_ = thetas[l] * np.matrix(err).T\n",
      "            th_ = th_.T\n",
      "            ac_ = add_one( activations[l] ) # add bias\n",
      "            ac_ = np.multiply( ac_, (1 - ac_))\n",
      "            errors[l] = np.array( np.multiply( th_, ac_) )[0]\n",
      "            err = del_one(errors[l])\n",
      "\n",
      "        return np.array(errors), activations\n",
      "    \n",
      "    def J(self, thetas):\n",
      "        return 1/2 * sum(((self.feedforward( thetas)[-1] - self.y_out)**2)) \n",
      "        # add [0] because we have to remove matrix form first, then sum it together\n",
      "\n",
      "    def Jgrad(self, thetas):\n",
      "        errors, activations = self.backprop( thetas)\n",
      "        grad = np.array([])\n",
      "        for l in range(self.n_levels-1):\n",
      "            ac_ = add_one(activations[l])\n",
      "            err = del_one(errors[l+1]) if l != self.n_levels-2 else errors[l+1]\n",
      "            cur_grad = np.matrix(err).T.dot(np.matrix(ac_)).T\n",
      "            grad = np.append(grad , cur_grad)\n",
      "        return np.array(grad)\n",
      "        \n",
      "    def grad_approx(self, thetas, e=1e-1):\n",
      "        return np.array([(self.J(thetas+eps) - self.J(thetas-eps))/(2*e)\n",
      "                         for eps in np.identity(len(thetas)) * e])\n",
      "    \n",
      "    def fit(self, x_, y, thetas0=None):\n",
      "        \"\"\" x_ is a vector, y is a class between 1 and sth\"\"\"\n",
      "        self.x_ = x_\n",
      "        self.y = y  \n",
      "        \n",
      "        I = np.identity(self.arch[-1])                # identity matrix of the size of the output\n",
      "        self.y_out = I[y-1]       # classes starts from 1, so it needs to be substracted by 1\n",
      "        \n",
      "        thetas0 = thetas0 if thetas0 != None else self.init_thetas()\n",
      "        self.thetas = thetas0\n",
      "        thetas, _, _ = fmin_l_bfgs_b(func = self.J,\n",
      "                                           x0 = thetas0,\n",
      "                                           fprime = self.Jgrad,\n",
      "                                           factr=1000)\n",
      "        return thetas\n",
      "        # we have local min \n",
      "        #model = NeuralNetClassifier(self.feedforward, thetas)\n",
      "        \n",
      "        #return model\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 218
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Testing"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "x_ = X_train[0, :5]\n",
      "y = y_train[0]\n",
      "lr = NeuralNetLearner([len(x_), 2,  9])\n",
      "r = lr.fit(X, y)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 219
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "# Test atributes and some minor functions of a learner\n",
      "\n",
      "print( \"Testing shape\\n\", lr.theta_shape)\n",
      "print( \"Testing indices\\n\", lr.theta_ind)\n",
      "print( \"Testing theta lenght\\n\", lr.theta_len)\n",
      "print( \"Testing theta shape\\n\", lr.shape_thetas( thetas))\n",
      "print( \"Testing feedforward\\n\", lr.feedforward( thetas))"
     ]
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "# Test feed forward\n",
      "\n",
      "print( \"Test feed forward:\")\n",
      "print( lr.feedforward(lr.thetas))"
     ]
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "# Test back propagation\n",
      "\n",
      "print( \"Test errors and activations\")\n",
      "errors, acts = lr.backprop(lr.thetas)\n",
      "print( errors.shape, acts.shape)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Test cost function\n",
      "\n",
      "print( \"Cost function:\", lr.J(lr.thetas))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Cost function: 1.12499737142\n"
       ]
      }
     ],
     "prompt_number": 220
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Test gradient function \n",
      "\n",
      "print( \"Grad. function:\", lr.Jgrad(lr.thetas))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Grad. function: [  0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
        "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
        "   0.00000000e+00   0.00000000e+00  -9.94278082e-07  -2.41853855e-07\n",
        "  -6.24996419e-02   6.24993396e-02   6.24992873e-02   6.24998683e-02\n",
        "   6.25000193e-02   6.24998070e-02   6.24995614e-02   6.24994125e-02\n",
        "   6.24997087e-02  -6.25002352e-02   6.24999329e-02   6.24998807e-02\n",
        "   6.25004616e-02   6.25006127e-02   6.25004003e-02   6.25001548e-02\n",
        "   6.25000059e-02   6.25003021e-02  -1.24999883e-01   1.24999278e-01\n",
        "   1.24999174e-01   1.25000336e-01   1.25000638e-01   1.25000213e-01\n",
        "   1.24999722e-01   1.24999424e-01   1.25000017e-01]\n"
       ]
      }
     ],
     "prompt_number": 221
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Test gradient approximation\n",
      "\n",
      "print( \"Grad. approx:\", lr.grad_approx(lr.thetas, e=1e-5))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Grad. approx: [  0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
        "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
        "   0.00000000e+00   0.00000000e+00  -9.94282434e-07  -2.41850984e-07\n",
        "  -6.24996419e-02   6.24993396e-02   6.24992873e-02   6.24998683e-02\n",
        "   6.25000193e-02   6.24998070e-02   6.24995614e-02   6.24994125e-02\n",
        "   6.24997087e-02  -6.25002353e-02   6.24999329e-02   6.24998806e-02\n",
        "   6.25004616e-02   6.25006127e-02   6.25004003e-02   6.25001548e-02\n",
        "   6.25000059e-02   6.25003021e-02  -1.24999883e-01   1.24999278e-01\n",
        "   1.24999174e-01   1.25000336e-01   1.25000638e-01   1.25000213e-01\n",
        "   1.24999722e-01   1.24999424e-01   1.25000017e-01]\n"
       ]
      }
     ],
     "prompt_number": 222
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Test difference\n",
      "print( \"grad - grad.aprox =\", sum( lr.Jgrad(lr.thetas) - lr.grad_approx(lr.thetas, e=1e-5) ))  # = -2.310293595541139e-12"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "grad - grad.aprox = 5.07137172612e-11\n"
       ]
      }
     ],
     "prompt_number": 223
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Batch Sequential"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# stohastic training:   size = 1\n",
      "# batch training:       size = num of samples .... TU MORAS PAZITI. SESTEVATI MORAS GRADIENTE, NE THERE\n",
      "# batch sequential:     otherwise\n",
      "class BatchSeq():\n",
      "    def __init__(self, learner, size=100):\n",
      "        self.learner = learner\n",
      "        self.size = size\n",
      "        \n",
      "    def update_thetas(self, X, y_, thetas=None):\n",
      "        list_of_thetas = [ self.learner.fit( X[i], y_[i], thetas) for i in range( X.shape[0])]   # list of thetas\n",
      "        new_thetas = sum( list_of_thetas) / len(list_of_thetas)\n",
      "        return new_thetas\n",
      "        \n",
      "    def fit_thetas(self, X, y_, thetas=None):\n",
      "        # devide into subset of 100 elements\n",
      "        # we have 50 000 / 100 = 500 subsets\n",
      "        size = self.size\n",
      "        thetas = None\n",
      "        # num of samples = 1130 = 0, 100, ..., 900, 1000, 1130 (last block takes all remainded)\n",
      "        split = [ [lo, lo+size] \n",
      "                          for lo in range( 0, X.shape[0], size)\n",
      "                        ] if X.shape[0] >= size else [[0, 0]]\n",
      "        split[-1][-1] = X.shape[0]\n",
      "        for lo, hi in split:   # we remove 2, cuz we will examine it at the end\n",
      "            thetas = self.update_thetas( X[lo:hi, :], y_[lo:hi], thetas)\n",
      "        return thetas\n",
      "    \n",
      "    def fit(self, X, y_):\n",
      "        thetas = self.fit_thetas(X, y_)\n",
      "        return thetas\n",
      "            \n",
      "            \n",
      "            "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 285
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "size = 100\n",
      "split_indices = [ [lo, lo+size] for lo in range( 0, 1130-size, size)]\n",
      "split_indices[-1][-1] = 1130\n",
      "split_indices"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "threshold = 10000\n",
      "X = X_train_normalized[:threshold, :]\n",
      "y_ = y_train[:threshold]\n",
      "lr = NeuralNetLearner([X.shape[1], 1,  9])\n",
      "batchseq = BatchSeq(lr, 1)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 296
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "res = batchseq.fit(X, y_)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 297
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "res"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 298,
       "text": [
        "array([ -9.59000145e-05,  -1.94189556e-04,  -2.90389731e-05,\n",
        "        -3.22698026e-04,  -2.11552877e-04,   1.14962537e-05,\n",
        "        -1.48928055e-04,  -4.52265398e-05,   3.98630522e-06,\n",
        "        -9.64719837e-05,  -1.75468721e-05,   9.84062787e-06,\n",
        "         2.19870977e-05,  -9.27571277e-05,  -5.85746675e-05,\n",
        "         3.28110450e-05,   4.85961350e-05,   6.01264173e-05,\n",
        "         2.27684885e-05,   4.26463336e-05,   3.14348692e+00,\n",
        "        -7.75391566e+00,   7.75394650e+00,  -2.67714651e+00,\n",
        "        -2.67714594e+00,  -2.67714110e+00,  -2.67714960e+00,\n",
        "        -2.67714453e+00,  -2.67714320e+00,  -2.67715176e+00,\n",
        "        -7.36678770e+00,   7.36683876e+00,  -3.24352353e+00,\n",
        "        -3.24353279e+00,  -3.24353838e+00,  -3.24352731e+00,\n",
        "        -3.24352808e+00,  -3.24352755e+00,  -3.24352368e+00])"
       ]
      }
     ],
     "prompt_number": 298
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "res"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 295,
       "text": [
        "array([  9.26477440e-06,   1.12618426e-06,   2.46193663e-05,\n",
        "        -5.03164339e-06,   4.09258312e-06,  -8.07191761e-06,\n",
        "        -2.15567366e-05,   1.34700608e-06,  -9.05150864e-06,\n",
        "         2.03443849e-06,  -2.50874375e-05,   3.07945658e-05,\n",
        "         8.98638181e-06,   8.11308359e-06,  -1.47493597e-05,\n",
        "        -6.44056479e-06,   8.04564260e-06,  -1.27038391e-05,\n",
        "         8.45403707e-06,  -5.84980834e-06,   4.10418330e+00,\n",
        "        -3.19182962e+00,   3.19183091e+00,  -2.89216877e+00,\n",
        "        -2.89216542e+00,  -2.89216032e+00,  -2.89216969e+00,\n",
        "        -2.89216596e+00,  -2.89216521e+00,  -2.89217232e+00,\n",
        "        -2.73558536e+00,   2.73560659e+00,  -3.46254189e+00,\n",
        "        -3.46254832e+00,  -3.46255364e+00,  -3.46254345e+00,\n",
        "        -3.46254559e+00,  -3.46254566e+00,  -3.46254032e+00])"
       ]
      }
     ],
     "prompt_number": 295
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}