{
 "metadata": {
  "name": "",
  "signature": "sha256:1807332b4e54c34beeeafb46fd6caec3d1b9ccf8d278148ed879e1a71e9460cb"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "import scipy as sp\n",
      "from scipy.optimize import fmin_l_bfgs_b\n",
      "from sklearn import cross_validation\n",
      "from sklearn import preprocessing"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 13
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Load Data"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "norm = preprocessing.Normalizer()\n",
      "X_train = np.loadtxt(open(\"../data/train.csv\",\"rb\"),delimiter=\",\",skiprows=1, usecols=range(1,94))\n",
      "X_train = norm.fit_transform(X_train)\n",
      "\n",
      "y_train = np.loadtxt(open(\"../data/train.csv\",\"rb\"),dtype=str,delimiter=\",\",skiprows=1, usecols=[94])\n",
      "y_train = np.array([int(c[-2]) for c in y_train])  # Parse classes from Class_1 into 1\n",
      "\n",
      "X_test = np.loadtxt(open(\"../data/test.csv\",\"rb\"),delimiter=\",\",skiprows=1, usecols=range(1,94))\n",
      "X_test = norm.transform(X_test)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Helper Functions"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def add_ones(X):\n",
      "    return np.column_stack(X, (np.ones(len(X))))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def add_one(X):\n",
      "    return np.append(X, 1)\n",
      "\n",
      "def del_one(X):\n",
      "    return X[:-1]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 69
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def sigmoid(z):\n",
      "    return 1/(1+np.exp(-z))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Learner"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class NeuralNetClassifier():\n",
      "    \"\"\"Neural network classifier based on a set of binary classifiers.\"\"\"\n",
      "    def __init__(self, h, thetas):\n",
      "        self.thetas = thetas  # model parameters\n",
      "        self.h = h\n",
      "        \n",
      "    def predict(self, X):\n",
      "        y_hat = np.ravel(self.h(X, self.thetas))\n",
      "        # following works only for binary classifiers, correct it for multiclass\n",
      "        return np.vstack((1-y_hat, y_hat)).T"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Learner"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class NeuralNetLearner():\n",
      "    def __init__(self, arch, g=sigmoid, lambda_=1e-5):\n",
      "        self.arch = arch\n",
      "        self.n_levels = len(arch)\n",
      "        self.g = g\n",
      "        self.lambda_ = lambda_\n",
      "        self.theta_shape = np.array([ ( arch[i] + 1, arch[i + 1]) for i in range( len( arch) - 1)])\n",
      "        ind = np.array( [ sh1 * sh2 for sh1, sh2 in self.theta_shape])\n",
      "        self.theta_ind = np.cumsum( ind[:-1])  # last index falls out of the boundary\n",
      "        self.theta_len = sum(ind)\n",
      "    \n",
      "    def init_thetas(self, epsilon=1e-5):\n",
      "        \"\"\"Return thetas with random values\"\"\"\n",
      "        np.random.seed(42)\n",
      "        return np.random.rand(self.theta_len) * 2 * epsilon - epsilon\n",
      "    \n",
      "    def shape_thetas(self, thetas):\n",
      "        \"\"\"Return list of thetas at particular level\"\"\"\n",
      "        t = np.split(thetas, self.theta_ind)\n",
      "        return np.array( [t[i].reshape(shape) for i, shape in enumerate(self.theta_shape)])\n",
      "    \n",
      "    def feedforward(self, thetas):\n",
      "        \"\"\"Feed forward, prediction, add ones to the end for bias\"\"\"\n",
      "        a = self.x_\n",
      "        thetas = self.shape_thetas(thetas)\n",
      "        #######################################################\n",
      "        activationsByLevel = [a]\n",
      "        # compute from level 2, 3, 4 ... and store such\n",
      "        for theta in thetas: \n",
      "            a = add_one(a)\n",
      "            a = self.g(a.dot(theta))\n",
      "            activationsByLevel.append(a)\n",
      "        return np.array(activationsByLevel)\n",
      "    \n",
      "    def backprop(self, thetas):\n",
      "        \"\"\"Add bias to the end\"\"\"\n",
      "        x = self.x_\n",
      "        y = self.y_out_\n",
      "        \n",
      "        activations = self.feedforward( thetas)\n",
      "        thetas = self.shape_thetas( thetas)\n",
      "        # output error\n",
      "        err = - np.multiply( y - activations[-1],  np.multiply( activations[-1], 1-activations[-1])) \n",
      "               \n",
      "        errors = [0] * self.n_levels\n",
      "        errors[self.n_levels - 1] = err \n",
      "        \n",
      "        levels = list(range(0, self.n_levels-1))\n",
      "        for l in levels[::-1]:  # reverse\n",
      "            th_ = thetas[l] * np.matrix(err).T\n",
      "            th_ = th_.T\n",
      "            act = activations[l]  \n",
      "            ac_ = add_one( act ) # add bias\n",
      "            ac_ = np.multiply( ac_, (1 - ac_))\n",
      "            errors[l] = np.array( np.multiply( th_, ac_) )[0]\n",
      "            err = del_one(errors[l])\n",
      "\n",
      "        return np.array(errors), activations\n",
      "    \n",
      "    def J(self, thetas):\n",
      "        # try for one example\n",
      "        return 1/2 * (self.feedforward( thetas)[-1] - self.y_out_)**2\n",
      "\n",
      "    def Jgrad(self, thetas):\n",
      "        errors, activations = self.backprop( thetas)\n",
      "        grad = np.array([])\n",
      "        for l in range(self.n_levels-1):\n",
      "            ac_ = add_one(activations[l])\n",
      "            err = del_one(errors[l+1]) if l != self.n_levels-2 else errors[l+1]\n",
      "            cur_grad = np.matrix(err).T.dot(np.matrix(ac_)).T\n",
      "            grad = np.append(grad , cur_grad)\n",
      "        return np.array(grad)\n",
      "        \n",
      "    def grad_approx(self, thetas, e=1e-1):\n",
      "        #for eps in np.identity(len(thetas)) * e:\n",
      "        #    print(((self.J(thetas+eps) - self.J(thetas-eps))/(2*e)))\n",
      "        return np.array([(self.J(thetas+eps) - self.J(thetas-eps))/(2*e)\n",
      "                         for eps in np.identity(len(thetas)) * e])\n",
      "\n",
      "    \n",
      "    \n",
      "    def fit(self, x_, y):\n",
      "        \"\"\" x_ is a vector, y is a class between 1 and sth\"\"\"\n",
      "        self.x_ = x_\n",
      "        self.y = y  \n",
      "        \n",
      "        I = np.identity(self.arch[-1])                # identity matrix of the size of the output\n",
      "        self.y_out_ = [ I[y-1] for y in self.y]       # classes starts from 1, so it needs to be substracted by 1\n",
      "        \n",
      "        self.thetas = self.init_thetas()\n",
      "\n",
      "        #thetas, fmin, info = fmin_l_bfgs_b(func = lambda thetas: self.J(self.X, self.y_bin[0], thetas),\n",
      "        #                                   x0 = thetas)\n",
      "        # we have local min \n",
      "        #model = NeuralNetClassifier(self.feedforward, thetas)\n",
      "        \n",
      "        #return model\n",
      "\n",
      "    def test(self, a):\n",
      "        thetas = np.array([-30, 10, 20, -20, 20, -20, -10, 20, 20])\n",
      "        print(self.h(a, thetas))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 286
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Testing"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X = X_train[:1, :5]\n",
      "y = y_train[:1]\n",
      "lr = NeuralNetLearner([X.shape[1], 2,  1])\n",
      "r = lr.fit(X[0], y)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 287
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "print( \"Testing shape\\n\", lr.theta_shape)\n",
      "print( \"Testing indices\\n\", lr.theta_ind)\n",
      "print( \"Testing theta lenght\\n\", lr.theta_len)\n",
      "print( \"Testing theta shape\\n\", lr.shape_thetas( thetas))\n",
      "print( \"Testing feedforward\\n\", lr.feedforward( thetas))"
     ]
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "lr.feedforward(lr.thetas)"
     ]
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "errors, acts = lr.backprop(lr.thetas)\n",
      "print( errors.shape, acts.shape)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "grad = lr.Jgrad(lr.thetas)\n",
      "grad"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 296,
       "text": [
        "array([ -1.32744025e-08,   1.14862715e-08,   0.00000000e+00,\n",
        "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
        "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
        "         0.00000000e+00,  -2.07777265e-07,   1.79788588e-07,\n",
        "        -6.24998802e-02,  -6.25004966e-02,  -1.25000370e-01])"
       ]
      }
     ],
     "prompt_number": 296
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "grad_approx = lr.grad_approx(lr.thetas, e=1e-8)\n",
      "grad_approx.T[0, 0]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 306,
       "text": [
        "array([ -1.11022302e-08,   1.11022302e-08,   0.00000000e+00,\n",
        "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
        "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
        "         0.00000000e+00,  -2.05391260e-07,   1.77635684e-07,\n",
        "        -6.24998817e-02,  -6.25004923e-02,  -1.25000366e-01])"
       ]
      }
     ],
     "prompt_number": 306
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# GRADIENT CHECK one output\n",
      "sum( grad - grad_approx.T[0, 0] )"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 308,
       "text": [
        "-8.9750826362621827e-09"
       ]
      }
     ],
     "prompt_number": 308
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}