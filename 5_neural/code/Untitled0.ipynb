{
 "metadata": {
  "name": "",
  "signature": "sha256:4a297b0df5302861abf16f66dafddb5a60763cd6af4e274f4ea4f9499afe136d"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from scipy.optimize import fmin_l_bfgs_b\n",
      "from sklearn import cross_validation\n",
      "from sklearn import preprocessing\n",
      "from sklearn import decomposition\n",
      "from sklearn import datasets\n",
      "import numpy as np\n",
      "import scipy as sp\n",
      "import random\n",
      "import time"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Load Data"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "NUM_OF_CLASSES = 0\n",
      "def load_data1():\n",
      "    \"\"\"Load keggle\"\"\"\n",
      "    global NUM_OF_CLASSES\n",
      "    X_train = np.loadtxt(open(\"../data/train.csv\",\"rb\"),delimiter=\",\",skiprows=1, usecols=range(1,94))\n",
      "\n",
      "    y_train = np.loadtxt(open(\"../data/train.csv\",\"rb\"),dtype=str,delimiter=\",\",skiprows=1, usecols=[94])\n",
      "    y_train = np.array([int(c[-2])-1 for c in y_train])  # Parse classes from Class_1 into 1\n",
      "\n",
      "      # Warning: number of classes must go from 1 to n\n",
      "\n",
      "    X_test = np.loadtxt(open(\"../data/test.csv\",\"rb\"),delimiter=\",\",skiprows=1, usecols=range(1,94))\n",
      "    \n",
      "    return X_train, y_train, X_test\n",
      "\n",
      "def load_data2():\n",
      "    \"\"\"Load iris\"\"\"\n",
      "    iris = datasets.load_iris()\n",
      "    X_train = iris.data\n",
      "    y_train = iris.target.T\n",
      "    return X_train, y_train, None"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 165
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X_train, Y_train, X_test = load_data2()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 166
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "NUM_OF_CLASSES = len( np.unique(Y_train))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 167
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def binaryClassRepresentation(Y):\n",
      "    \"\"\" Class 1 write as 1,0,0,0,0,0,0,0,0, class 2 write as 0,1,0,0,0,0,0,0,0, ... \"\"\"\n",
      "    I = np.identity(NUM_OF_CLASSES)\n",
      "    Y = np.array( [ I[y] for y in Y])\n",
      "    return Y"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 168
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "Y_train = binaryClassRepresentation(Y_train)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 169
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Preprocesse Data"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def shuffle(X, Y):\n",
      "    ind = np.array(list(range(X.shape[0])))\n",
      "    np.random.shuffle(ind)\n",
      "    X = np.array( [ X[i] for i in ind] )\n",
      "    Y = np.array( [ Y[i] for i in ind] )\n",
      "    return X, Y"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X_train, Y_train = shuffle(X_train, Y_train)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Normalize\n",
      "norm = preprocessing.Normalizer()\n",
      "X_train = norm.fit_transform(X_train)\n",
      "if X_test != None:\n",
      "    X_test = norm.transform(X_test)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 44
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Helper functions"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def add_ones(X):\n",
      "    return np.column_stack((X, np.ones(X.shape[0])))\n",
      "\n",
      "def add_one(X):\n",
      "    return np.append(X, 1)\n",
      "\n",
      "def add_zero(X):\n",
      "    return np.append(X, 0)\n",
      "\n",
      "def del_one(X):\n",
      "    return X[:-1]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "heading",
     "level": 6,
     "metadata": {},
     "source": [
      "Sigmoid"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def sigmoid(z):\n",
      "    return 1/(1+np.exp(-z))\n",
      "\n",
      "def sigmoid_derivative(z):\n",
      "    return np.multiply(z , 1 - z)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 8
    },
    {
     "cell_type": "heading",
     "level": 6,
     "metadata": {},
     "source": [
      "Tansigmoid"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def tansigmoid(z):\n",
      "    return ( np.exp(z) - np.exp(-z) ) / (np.exp(z) + np.exp(-z))\n",
      "\n",
      "def tansigmoid_derivative(z):\n",
      "    return 1 - tansigmoid(z)**2"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 9
    },
    {
     "cell_type": "heading",
     "level": 6,
     "metadata": {},
     "source": [
      "Arctan"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Trying to make our own function\n",
      "def arctan(z):\n",
      "    return np.arctan(2*z) / np.pi + 1/2\n",
      "\n",
      "def arctan_derivative(z):\n",
      "    return 2 / (((2*z)**2 + 1) * np.pi)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 10
    },
    {
     "cell_type": "heading",
     "level": 6,
     "metadata": {},
     "source": [
      "Test"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def grad_approx(J, thetas, e=1e-5):\n",
      "        return np.array([(J(thetas+eps) - J(thetas-eps))/(2*e)\n",
      "                         for eps in np.identity(len(thetas)) * e])\n",
      "    \n",
      "m = np.array([1,2,3])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 11
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print(\"tansigmoid error\", tansigmoid_derivative(m) - grad_approx(tansigmoid, m).diagonal())\n",
      "print(\"arctan error\", arctan_derivative(m) - grad_approx(arctan, m).diagonal())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "tansigmoid error [ -1.15246146e-11  -9.51938528e-12   5.97277783e-12]\n",
        "arctan error [ -6.15821283e-12  -2.79085088e-12   4.11880252e-12]\n"
       ]
      }
     ],
     "prompt_number": 12
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Learner"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class NeuralNetLearner():\n",
      "    def __init__(self, arch, g=sigmoid, g_derivative=sigmoid_derivative, lambda_=1e-5, maxiter=1, learning_rate=1, dropout=0.1):\n",
      "        self.arch = arch\n",
      "        self.g = g\n",
      "        self.g_derivative = g_derivative\n",
      "        self.lambda_ = lambda_\n",
      "        self.maxiter = maxiter\n",
      "        self.learning_rate = learning_rate\n",
      "        self.dropout = dropout\n",
      "        \n",
      "        self.n_levels = len(arch)\n",
      "        \n",
      "        self.theta_shape = [ ( arch[i] + 1, arch[i + 1]) for i in range( len( arch) - 1)]\n",
      "        ind = [ sh1 * sh2 for sh1, sh2 in self.theta_shape]\n",
      "        self.theta_ind = np.cumsum( ind[:-1])  # last index falls out of the boundary\n",
      "        self.theta_len = sum(ind)\n",
      "        self.seed = False\n",
      "        \n",
      "    \n",
      "    def init_thetas(self, epsilon=1e-2):\n",
      "        \"\"\"Return thetas with random values\"\"\"\n",
      "        #if self.seed: \n",
      "        #np.random.seed(42)\n",
      "        return np.random.rand(self.theta_len) * 2 * epsilon - epsilon\n",
      "    \n",
      "    def shape_thetas(self, thetas):\n",
      "        \"\"\"Return list of thetas at particular level\"\"\"\n",
      "        t = np.split(thetas, self.theta_ind)\n",
      "        return [t[i].reshape(shape) for i, \n",
      "                          shape in enumerate(self.theta_shape)]\n",
      "    \n",
      "    def add_ones(self, X):\n",
      "        return np.column_stack((X, np.ones(X.shape[0])))\n",
      "    \n",
      "    def del_ones(self, X):\n",
      "        return X[:, :-1]\n",
      "    \n",
      "    def feedforward(self, a_, thetas):\n",
      "        \"\"\"Feed forward, prediction, add ones to the end for bias\"\"\"\n",
      "        thetas = self.shape_thetas(thetas)\n",
      "        activations = [0] * self.n_levels\n",
      "        activations[0] = a_\n",
      "        for i in range(self.n_levels - 1): \n",
      "            activations[i+1] = self.g(np.dot(add_one(activations[i]), thetas[i]))\n",
      "        return activations\n",
      "    \n",
      "    \n",
      "    def feedforwardMatrix(self, A, thetas):\n",
      "        \"\"\"Feed forward, prediction, add ones to the end for bias\"\"\"\n",
      "        thetas = self.shape_thetas(thetas)\n",
      "        activations = [0] * self.n_levels\n",
      "        activations[0] = A\n",
      "        for i in range(self.n_levels - 1): \n",
      "            activations[i+1] = self.g(self.add_ones(activations[i]).dot(thetas[i]))\n",
      "        return activations\n",
      "    \n",
      "    \n",
      "    def backprop(self, x_, y_, thetas):\n",
      "        \"\"\"Add bias to the end\"\"\"\n",
      "        # Compute activations for each level. It includes first level too\n",
      "        activations = self.feedforward( x_, thetas)\n",
      "        # Compute error at the last level\n",
      "        err = - np.multiply( y_ - activations[-1],  \n",
      "                             self.g_derivative( activations[-1])) \n",
      "        # Prepare space for computing errors on all layers and insert output level\n",
      "        errors = [0] * self.n_levels\n",
      "        errors[self.n_levels - 1] = err \n",
      "        # Reshape thetas\n",
      "        thetas = self.shape_thetas( thetas)\n",
      "        # Compute errors on hidden layers and input layer. Start from behind\n",
      "        for l in range(self.n_levels - 2, -1, -1):  \n",
      "            th_ = err.dot(thetas[l].T) \n",
      "            ac_ = add_one( activations[l] ) # add bias\n",
      "            ac_ = np.multiply( ac_, (1 - ac_))\n",
      "            errors[l] = np.multiply( th_, ac_)\n",
      "            err = del_one(errors[l])\n",
      "\n",
      "        return errors, activations\n",
      "    \n",
      "    \n",
      "    def backpropMatrix(self, X, Y, thetas):\n",
      "        \"\"\"Add bias to the end\"\"\"\n",
      "        # Compute activations for each level. It includes first level too\n",
      "        activations = self.feedforwardMatrix( X, thetas)\n",
      "        # Compute error at the last level\n",
      "        err = - np.multiply( Y - activations[-1],  \n",
      "                             self.g_derivative( activations[-1])) \n",
      "        # Prepare space for computing errors on all layers and insert output level\n",
      "        errors = [0] * self.n_levels\n",
      "        errors[self.n_levels - 1] = err \n",
      "        # Reshape thetas\n",
      "        thetas = self.shape_thetas( thetas)\n",
      "        # Compute errors on hidden layers and input layer. Start from behind\n",
      "        for l in range(self.n_levels - 2, 0, -1):\n",
      "            new_err = err.dot(thetas[l].T) \n",
      "            act = self.add_ones( activations[l] ) # add bias\n",
      "            act = self.g_derivative( act )\n",
      "            #print(\"Act\", act)\n",
      "            #print(\"NewErr\", new_err)\n",
      "            errors[l] = np.multiply( new_err, act)\n",
      "            err = errors[l]\n",
      "            err = self.del_ones(errors[l])\n",
      "\n",
      "        return errors, activations\n",
      "    \n",
      "    \n",
      "    def l2_reg( self, thetas):\n",
      "        \"\"\"Regularization\"\"\"\n",
      "        thetas2 = thetas * thetas\n",
      "        thetas2 = self.shape_thetas( thetas2)\n",
      "        sum_ = 0\n",
      "        for levelthetas in thetas2:\n",
      "            for i in range( levelthetas.shape[0] - 1):\n",
      "                sum_ += np.sum( levelthetas[i])\n",
      "        return sum_ * self.lambda_ / 2\n",
      "    \n",
      "    \n",
      "    def l2_reg_grad( self, thetas):\n",
      "        \"\"\"Gradient of regularization\"\"\"\n",
      "        thetasModified = np.array(self.shape_thetas( thetas))\n",
      "        for i in range( len( thetasModified)):\n",
      "            thetasModified[i][-1] *= 0\n",
      "        return self.lambda_ * thetasModified\n",
      "    \n",
      "    \n",
      "    # TODO self.m, self.X, self.Y, self.lambda_\n",
      "    def J(self, thetas):\n",
      "        \"\"\"Cost funciton\"\"\"\n",
      "        sum_ = 0\n",
      "        m = self.X.shape[0]\n",
      "        for i in range( m):\n",
      "            sum_ += 1/(2*m) * np.sum( np.square( self.feedforward( self.X[i], thetas)[-1] - self.Y[i])) \n",
      "        sum_ = sum_ + self.l2_reg( thetas)\n",
      "        return sum_\n",
      "\n",
      "    \n",
      "    def Jgrad(self, thetas):\n",
      "        \"\"\"Gradient\"\"\"\n",
      "        \n",
      "        m = self.X.shape[0]\n",
      "        errorsANDactivations = [ self.backprop( self.X[i], self.Y[i], thetas) for i in range(self.X.shape[0])]\n",
      "        \n",
      "        #print(errorsANDactivations)\n",
      "        \n",
      "        #errorsANDactivations = map( lambda z: self.backprop( z[0], z[1], thetas), zip( self.X, self.Y))\n",
      "        grad = [0] * (self.n_levels-1)\n",
      "        for errors, activations in errorsANDactivations:\n",
      "            for l in range(self.n_levels-1):\n",
      "                err = del_one(errors[l+1]) if l != self.n_levels-2 else errors[l+1]\n",
      "                cur_grad = np.matrix(add_one(activations[l])).T.dot(np.matrix(err))\n",
      "                grad[l] = grad[l] + cur_grad\n",
      "        \n",
      "        grad = np.array(grad) / m + self.l2_reg_grad( thetas)\n",
      "        flatten = np.array([])\n",
      "        for matrix in grad:\n",
      "            flatten = np.append(flatten, matrix.ravel())\n",
      "        return flatten\n",
      "    \n",
      "    \n",
      "    def JgradMatrix(self, thetas):\n",
      "        \"\"\"Gradient\"\"\"\n",
      "        \n",
      "        m = self.X.shape[0]\n",
      "        errors, activations = self.backpropMatrix( self.X, self.Y, thetas) \n",
      "            \n",
      "        grad = [0] * (self.n_levels-1)\n",
      "        for i in range(m):\n",
      "            for l in range(self.n_levels-1):\n",
      "                err = del_one(errors[l+1][i]) if l != self.n_levels-2 else errors[l+1][i]\n",
      "                cur_grad = np.matrix(add_one(activations[l][i])).T.dot(np.matrix(err))\n",
      "                grad[l] = grad[l] + cur_grad\n",
      "        \n",
      "        grad = np.array(grad) / m + self.l2_reg_grad( thetas)\n",
      "        flatten = np.array([])\n",
      "        for matrix in grad:\n",
      "            flatten = np.append(flatten, matrix.ravel())\n",
      "        return flatten\n",
      "    \n",
      "    def set_data( self, X, Y, thetas=None):\n",
      "        self.X = X\n",
      "        self.Y = Y\n",
      "        self.thetas = thetas if thetas != None else self.init_thetas()\n",
      "    \n",
      "    def fit(self, X, Y, thetas=None):\n",
      "        \n",
      "        self.set_data(X, Y, thetas)\n",
      "        self.seed = True   \n",
      "        \n",
      "        if False:\n",
      "            grad = self.Jgrad(self.thetas)\n",
      "            grad = np.array([g if random.random() > self.dropout else 0 for g in grad])\n",
      "            #self.learning_rate = np.sum(np.abs(grad))\n",
      "\n",
      "            #print(self.learning_rate)\n",
      "            self.thetas = self.thetas - self.learning_rate * grad\n",
      "            \n",
      "        if True:\n",
      "            self.thetas, _, _ = fmin_l_bfgs_b(func = self.J,\n",
      "                                          x0 = self.thetas,\n",
      "                                           fprime = self.Jgrad,\n",
      "                                           maxiter=self.maxiter,\n",
      "                                           factr=10)   \n",
      "        return self.thetas\n",
      "    \n",
      "    def predict(self, X):\n",
      "        #print(\"pred\", X.shape, np.array( [ self.feedforward(x_, self.thetas) for x_ in X]).shape)\n",
      "        return np.array( [ self.feedforward(x_, self.thetas)[-1] for x_ in X])\n",
      "    \n",
      "    def get_params(self, *args, **argss):\n",
      "        return {'arch':self.arch, 'g':self.g,\n",
      "                'g_derivative': self.g_derivative,\n",
      "                'lambda_':self.lambda_,\n",
      "                'maxiter':self.maxiter,\n",
      "                'learning_rate':self.learning_rate,\n",
      "                'dropout':self.dropout}\n",
      "                    \n",
      "    def predict_proba(self, X):\n",
      "        return self.predict(X)\n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 259
    },
    {
     "cell_type": "heading",
     "level": 6,
     "metadata": {},
     "source": [
      "Test feed forward and cost function"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "threshold = 1\n",
      "lenght = 4\n",
      "\n",
      "X = X_train[0:threshold, :lenght]\n",
      "Y = Y_train[0:threshold]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 222
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 223,
       "text": [
        "array([[ 5.1,  3.5,  1.4,  0.2]])"
       ]
      }
     ],
     "prompt_number": 223
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Test feed forward OK\n",
      "\n",
      "lr = NeuralNetLearner([X.shape[1], 2,  NUM_OF_CLASSES]) \n",
      "thetas = np.array([1.5] * lr.theta_len)\n",
      "print( \"Test feedforward:\\n\", lr.feedforward(X, thetas))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Test feedforward:\n",
        " [array([[ 5.1,  3.5,  1.4,  0.2]]), array([ 0.99999995,  0.99999995]), array([ 0.98901306,  0.98901306,  0.98901306])]\n"
       ]
      }
     ],
     "prompt_number": 224
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Test feed forward matrix OK\n",
      "\n",
      "lr = NeuralNetLearner([X.shape[1], 2,  NUM_OF_CLASSES]) \n",
      "thetas = np.array([1.5] * lr.theta_len)\n",
      "print( \"Test feedforward:\\n\", lr.feedforwardMatrix(X, thetas))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Test feedforward:\n",
        " [array([[ 5.1,  3.5,  1.4,  0.2]]), array([[ 0.99999995,  0.99999995]]), array([[ 0.98901306,  0.98901306,  0.98901306]])]\n"
       ]
      }
     ],
     "prompt_number": 225
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Test regularization OK\n",
      "\n",
      "lr = NeuralNetLearner([X.shape[1], 2,  NUM_OF_CLASSES]) \n",
      "lr.lambda_ = 1e-5\n",
      "thetas = np.array([1.5] * lr.theta_len)\n",
      "lr.l2_reg(thetas)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 226,
       "text": [
        "0.00015750000000000001"
       ]
      }
     ],
     "prompt_number": 226
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Test cost function OK\n",
      "\n",
      "lr = NeuralNetLearner([X.shape[1], 2,  NUM_OF_CLASSES]) \n",
      "thetas = np.array([1.5] * lr.theta_len)\n",
      "lr.X = X\n",
      "lr.Y = Y\n",
      "print( \"Cost function:\", lr.J(thetas))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Cost function: 0.978364680859\n"
       ]
      }
     ],
     "prompt_number": 227
    },
    {
     "cell_type": "heading",
     "level": 6,
     "metadata": {},
     "source": [
      "Test backprop and gradient"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "lr = NeuralNetLearner([X.shape[1], 2,  NUM_OF_CLASSES]) \n",
      "thetas = np.array([1.5] * lr.theta_len)\n",
      "lr.X = X\n",
      "lr.Y = Y\n",
      "lr.backpropMatrix(X, Y, thetas)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 232,
       "text": [
        "([0,\n",
        "  array([[  1.62119731e-09,   1.62119731e-09,   0.00000000e+00]]),\n",
        "  array([[-0.00011939,  0.01074684,  0.01074684]])],\n",
        " [array([[ 5.1,  3.5,  1.4,  0.2]]),\n",
        "  array([[ 0.99999995,  0.99999995]]),\n",
        "  array([[ 0.98901306,  0.98901306,  0.98901306]])])"
       ]
      }
     ],
     "prompt_number": 232
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Test Jgrad OK\n",
      "\n",
      "lr = NeuralNetLearner([X.shape[1], 2,  NUM_OF_CLASSES]) \n",
      "thetas = np.array([1.5] * lr.theta_len)\n",
      "lr.X = X\n",
      "lr.Y = Y\n",
      "print( \"grad =\", lr.Jgrad(thetas))  # = -2.310293595541139e-12"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "grad = [  1.50082681e-05   1.50082681e-05   1.50056742e-05   1.50056742e-05\n",
        "   1.50022697e-05   1.50022697e-05   1.50003242e-05   1.50003242e-05\n",
        "   1.62119731e-09   1.62119731e-09  -1.04386672e-04   1.07618441e-02\n",
        "   1.07618441e-02  -1.04386672e-04   1.07618441e-02   1.07618441e-02\n",
        "  -1.19386678e-04   1.07468447e-02   1.07468447e-02]\n"
       ]
      }
     ],
     "prompt_number": 228
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def grad_approx(lr, thetas, e=1e-4):\n",
      "    \"\"\"Aproximation\"\"\"\n",
      "    return np.array([(lr.J(thetas+eps) - lr.J(thetas-eps))/(2*e)\n",
      "                     for eps in np.identity(len(thetas)) * e]).ravel()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 229
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Test grad aprox OK\n",
      "\n",
      "lr = NeuralNetLearner([X.shape[1], 2,  NUM_OF_CLASSES]) \n",
      "thetas = np.array([1.5] * lr.theta_len)\n",
      "lr.X = X\n",
      "lr.Y = Y\n",
      "print( \"grad =\\n\", grad_approx(lr, thetas))  # = -2.310293595541139e-12"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "grad =\n",
        " [  1.50082685e-05   1.50082685e-05   1.50056739e-05   1.50056739e-05\n",
        "   1.50022694e-05   1.50022694e-05   1.50003238e-05   1.50003238e-05\n",
        "   1.61926028e-09   1.61926028e-09  -1.04386673e-04   1.07618441e-02\n",
        "   1.07618441e-02  -1.04386673e-04   1.07618441e-02   1.07618441e-02\n",
        "  -1.19386679e-04   1.07468447e-02   1.07468447e-02]\n"
       ]
      }
     ],
     "prompt_number": 230
    },
    {
     "cell_type": "heading",
     "level": 6,
     "metadata": {},
     "source": [
      "JgradMatrix"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "lr = NeuralNetLearner([X.shape[1], 2,  NUM_OF_CLASSES]) \n",
      "thetas = np.array([1.5] * lr.theta_len)\n",
      "lr.X = X\n",
      "lr.Y = Y\n",
      "lr.JgradMatrix(thetas)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 257,
       "text": [
        "array([  1.50168258e-05,   1.50168258e-05,   1.50113637e-05,\n",
        "         1.50113637e-05,   1.50048464e-05,   1.50048464e-05,\n",
        "         1.50006110e-05,   1.50006110e-05,   3.64105963e-09,\n",
        "         3.64105963e-09,  -1.04386709e-04,   1.07618454e-02,\n",
        "         1.07618454e-02,  -1.04386709e-04,   1.07618454e-02,\n",
        "         1.07618454e-02,  -1.19386723e-04,   1.07468466e-02,\n",
        "         1.07468466e-02])"
       ]
      }
     ],
     "prompt_number": 257
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "lr = NeuralNetLearner([X.shape[1], 7,  NUM_OF_CLASSES]) \n",
      "thetas = np.array([1.5] * lr.theta_len)\n",
      "lr.X = X\n",
      "lr.Y = Y\n",
      "lr.JgradMatrix(thetas)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 258,
       "text": [
        "array([  1.50000097e-05,   1.50000097e-05,   1.50000097e-05,\n",
        "         1.50000097e-05,   1.50000097e-05,   1.50000097e-05,\n",
        "         1.50000097e-05,   1.50000065e-05,   1.50000065e-05,\n",
        "         1.50000065e-05,   1.50000065e-05,   1.50000065e-05,\n",
        "         1.50000065e-05,   1.50000065e-05,   1.50000028e-05,\n",
        "         1.50000028e-05,   1.50000028e-05,   1.50000028e-05,\n",
        "         1.50000028e-05,   1.50000028e-05,   1.50000028e-05,\n",
        "         1.50000004e-05,   1.50000004e-05,   1.50000004e-05,\n",
        "         1.50000004e-05,   1.50000004e-05,   1.50000004e-05,\n",
        "         1.50000004e-05,   2.09326301e-12,   2.09326301e-12,\n",
        "         2.09326301e-12,   2.09326301e-12,   2.09326301e-12,\n",
        "         2.09326301e-12,   2.09326301e-12,   1.49999622e-05,\n",
        "         2.11441057e-05,   2.11441057e-05,   1.49999622e-05,\n",
        "         2.11441057e-05,   2.11441057e-05,   1.49999622e-05,\n",
        "         2.11441057e-05,   2.11441057e-05,   1.49999622e-05,\n",
        "         2.11441057e-05,   2.11441057e-05,   1.49999622e-05,\n",
        "         2.11441057e-05,   2.11441057e-05,   1.49999622e-05,\n",
        "         2.11441057e-05,   2.11441057e-05,   1.49999622e-05,\n",
        "         2.11441057e-05,   2.11441057e-05,  -3.77507396e-11,\n",
        "         6.14410643e-06,   6.14410643e-06])"
       ]
      }
     ],
     "prompt_number": 258
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Difference OK 9.16699325959e-11\n",
      "# This may not work after dropout application\n",
      "\n",
      "lr = NeuralNetLearner([X.shape[1], 2,  NUM_OF_CLASSES]) \n",
      "thetas = np.array([1.5] * lr.theta_len)\n",
      "lr.X = X\n",
      "lr.Y = Y\n",
      "print( \"Grad diff =\", np.sum( grad_approx(lr, thetas) - lr.Jgrad(thetas) ) )"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Grad diff = 9.5251561158e-11\n"
       ]
      }
     ],
     "prompt_number": 255
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Cross Validation"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn import metrics\n",
      "\n",
      "def evaluate(pred, real):\n",
      "    return metrics.log_loss(real, pred)\n",
      "\n",
      "def my_cross_validation(X, Y, model, k=3):\n",
      "    S = []\n",
      "    kf = cross_validation.KFold(len(Y), n_folds=k, shuffle=False)\n",
      "    for train_index, test_index in kf:\n",
      "        print(\"New fold\")\n",
      "        # Split\n",
      "        X_train, X_test = X[train_index], X[test_index]\n",
      "        Y_train, Y_test = Y[train_index], Y[test_index]\n",
      "        \n",
      "        model.fit(X_train, Y_train)\n",
      "        predicted = model.predict(X_test)\n",
      "        #print(predicted.shape)\n",
      "        # Evaluate\n",
      "        score = evaluate(predicted, Y_test)\n",
      "        print(\"                                             ### Vmesni rezultat ###: \", score)\n",
      "        S.append( score )\n",
      "    S = np.array(S)\n",
      "    return(np.mean(S))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Batch Stohastic"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# stohastic training:   size = 1\n",
      "# batch training:       size = num of samples \n",
      "# batch sequential:     otherwise\n",
      "class BatchSeq():\n",
      "    def __init__(self, learner, size=0, grad_start=5, iterations=500):\n",
      "        self.learner = learner\n",
      "        self.size = size\n",
      "        self.grad_start = grad_start\n",
      "        self.iterations = iterations\n",
      "\n",
      "        \n",
      "    def fit_thetas(self, X, y_, thetas=None):\n",
      "        # num of samples = 1130 = 0, 100, ..., 900, 1000, 1130 (last block takes all remainded)\n",
      "        split = [ [lo, lo + self.size] \n",
      "                          for lo in range( 0, X.shape[0], self.size)\n",
      "                        ] if X.shape[0] >= self.size else [[0, 0]]\n",
      "        split[-1][-1] = X.shape[0]\n",
      "        counter = 0\n",
      "        for lo, hi in split:\n",
      "            counter += 1\n",
      "            thetas = self.learner.fit( X[lo:hi], y_[lo:hi], thetas=thetas)\n",
      "        return thetas\n",
      "    \n",
      "    def fit(self, X, Y):\n",
      "        # batch vs stohastic\n",
      "        if self.size <= 0:\n",
      "            self.size = X.shape[0]\n",
      "        \n",
      "        # stop criteria\n",
      "        eps = 1e-7\n",
      "        \n",
      "        # init thetas and grad. first step\n",
      "        thetas = self.learner.init_thetas()\n",
      "        self.learner.grad_step = self.grad_start\n",
      "        \n",
      "        random.seed(42)\n",
      "        \n",
      "        for i in range(self.iterations):\n",
      "            start = time.time()\n",
      "            \n",
      "            # decrease grad step\n",
      "            #self.learner.grad_step *= 0.95 \n",
      "            # shuffle a bit\n",
      "            \n",
      "            X, Y = shuffle(X, Y)\n",
      "            prev_thetas = thetas\n",
      "            thetas = self.fit_thetas(X, Y, thetas)   \n",
      "            diff = abs(thetas - prev_thetas)\n",
      "            \n",
      "            end = time.time()\n",
      "            print(\"Iter:\", i, \" \\n        Time:\", round( end - start, 2) , \"s    Theta diff:\",sum( diff ))\n",
      "            if (sum( diff > eps ) == 0):\n",
      "                print(\"OK Convergent\")\n",
      "                break        \n",
      "    \n",
      "    def predict(self, X):\n",
      "        return self.learner.predict(X)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn import metrics\n",
      "lr = NeuralNetLearner([X.shape[1], 7,  NUM_OF_CLASSES]) \n",
      "thetas = np.array([1.5] * lr.theta_len)\n",
      "lr.X = X\n",
      "lr.Y = Y\n",
      "scores = cross_validation.cross_val_score(lr, X, Y, cv=5, scoring='log_loss')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 260
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "scores"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 261,
       "text": [
        "array([-0.16668018, -0.16650092, -0.16889817, -0.1640906 , -0.15962525])"
       ]
      }
     ],
     "prompt_number": 261
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# napak prvega nivoja ne rabimo racunat"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}