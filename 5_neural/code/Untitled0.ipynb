{
 "metadata": {
  "name": "",
  "signature": "sha256:1e491d14dcadd5cebdbd7dcf95085dc977d52fb8d624109f3fe1672f243e579e"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from scipy.optimize import fmin_l_bfgs_b\n",
      "from sklearn import cross_validation\n",
      "from sklearn import preprocessing\n",
      "from sklearn import decomposition\n",
      "from sklearn import datasets\n",
      "import numpy as np\n",
      "import scipy as sp\n",
      "import random\n",
      "import time"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Load Data"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "NUM_OF_CLASSES = 0\n",
      "def load_data1():\n",
      "    \"\"\"Load keggle\"\"\"\n",
      "    global NUM_OF_CLASSES\n",
      "    X_train = np.loadtxt(open(\"../data/train.csv\",\"rb\"),delimiter=\",\",skiprows=1, usecols=range(1,94))\n",
      "\n",
      "    y_train = np.loadtxt(open(\"../data/train.csv\",\"rb\"),dtype=str,delimiter=\",\",skiprows=1, usecols=[94])\n",
      "    y_train = np.array([int(c[-2])-1 for c in y_train])  # Parse classes from Class_1 into 1\n",
      "\n",
      "      # Warning: number of classes must go from 1 to n\n",
      "\n",
      "    X_test = np.loadtxt(open(\"../data/test.csv\",\"rb\"),delimiter=\",\",skiprows=1, usecols=range(1,94))\n",
      "    \n",
      "    return X_train, y_train, X_test\n",
      "\n",
      "def load_data2():\n",
      "    \"\"\"Load iris\"\"\"\n",
      "    iris = datasets.load_iris()\n",
      "    X_train = iris.data\n",
      "    y_train = iris.target.T\n",
      "    return X_train, y_train, None"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 165
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X_train, Y_train, X_test = load_data2()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 166
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "NUM_OF_CLASSES = len( np.unique(Y_train))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 167
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def binaryClassRepresentation(Y):\n",
      "    \"\"\" Class 1 write as 1,0,0,0,0,0,0,0,0, class 2 write as 0,1,0,0,0,0,0,0,0, ... \"\"\"\n",
      "    I = np.identity(NUM_OF_CLASSES)\n",
      "    Y = np.array( [ I[y] for y in Y])\n",
      "    return Y"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 168
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "Y_train = binaryClassRepresentation(Y_train)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 169
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Preprocesse Data"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def shuffle(X, Y):\n",
      "    ind = np.array(list(range(X.shape[0])))\n",
      "    np.random.shuffle(ind)\n",
      "    X = np.array( [ X[i] for i in ind] )\n",
      "    Y = np.array( [ Y[i] for i in ind] )\n",
      "    return X, Y"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X_train, Y_train = shuffle(X_train, Y_train)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Normalize\n",
      "norm = preprocessing.Normalizer()\n",
      "X_train = norm.fit_transform(X_train)\n",
      "if X_test != None:\n",
      "    X_test = norm.transform(X_test)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 44
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Helper functions"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def add_ones(X):\n",
      "    return np.column_stack((X, np.ones(X.shape[0])))\n",
      "\n",
      "def add_one(X):\n",
      "    return np.append(X, 1)\n",
      "\n",
      "def add_zero(X):\n",
      "    return np.append(X, 0)\n",
      "\n",
      "def del_one(X):\n",
      "    return X[:-1]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "heading",
     "level": 6,
     "metadata": {},
     "source": [
      "Sigmoid"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def sigmoid(z):\n",
      "    return 1/(1+np.exp(-z))\n",
      "\n",
      "def sigmoid_derivative(z):\n",
      "    return np.multiply(z , 1 - z)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 8
    },
    {
     "cell_type": "heading",
     "level": 6,
     "metadata": {},
     "source": [
      "Tansigmoid"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def tansigmoid(z):\n",
      "    return ( np.exp(z) - np.exp(-z) ) / (np.exp(z) + np.exp(-z))\n",
      "\n",
      "def tansigmoid_derivative(z):\n",
      "    return 1 - tansigmoid(z)**2"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 9
    },
    {
     "cell_type": "heading",
     "level": 6,
     "metadata": {},
     "source": [
      "Arctan"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Trying to make our own function\n",
      "def arctan(z):\n",
      "    return np.arctan(2*z) / np.pi + 1/2\n",
      "\n",
      "def arctan_derivative(z):\n",
      "    return 2 / (((2*z)**2 + 1) * np.pi)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 10
    },
    {
     "cell_type": "heading",
     "level": 6,
     "metadata": {},
     "source": [
      "Test"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def grad_approx(J, thetas, e=1e-5):\n",
      "        return np.array([(J(thetas+eps) - J(thetas-eps))/(2*e)\n",
      "                         for eps in np.identity(len(thetas)) * e])\n",
      "    \n",
      "m = np.array([1,2,3])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 11
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print(\"tansigmoid error\", tansigmoid_derivative(m) - grad_approx(tansigmoid, m).diagonal())\n",
      "print(\"arctan error\", arctan_derivative(m) - grad_approx(arctan, m).diagonal())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "tansigmoid error [ -1.15246146e-11  -9.51938528e-12   5.97277783e-12]\n",
        "arctan error [ -6.15821283e-12  -2.79085088e-12   4.11880252e-12]\n"
       ]
      }
     ],
     "prompt_number": 12
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Learner"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class NeuralNetLearner():\n",
      "    def __init__(self, arch, g=sigmoid, g_derivative=sigmoid_derivative, lambda_=1e-5, maxiter=1, learning_rate=1, dropout=0.1):\n",
      "        self.arch = arch\n",
      "        self.g = g\n",
      "        self.g_derivative = g_derivative\n",
      "        self.lambda_ = lambda_\n",
      "        self.maxiter = maxiter\n",
      "        self.learning_rate = learning_rate\n",
      "        self.dropout = dropout\n",
      "        \n",
      "        self.n_levels = len(arch)\n",
      "        \n",
      "        self.theta_shape = [ ( arch[i] + 1, arch[i + 1]) for i in range( len( arch) - 1)]\n",
      "        ind = [ sh1 * sh2 for sh1, sh2 in self.theta_shape]\n",
      "        self.theta_ind = np.cumsum( ind[:-1])  # last index falls out of the boundary\n",
      "        self.theta_len = sum(ind)\n",
      "        self.seed = False\n",
      "        \n",
      "    \n",
      "    def init_thetas(self, epsilon=1e-2):\n",
      "        \"\"\"Return thetas with random values\"\"\"\n",
      "        #if self.seed: \n",
      "        #np.random.seed(42)\n",
      "        return np.random.rand(self.theta_len) * 2 * epsilon - epsilon\n",
      "    \n",
      "    def shape_thetas(self, thetas):\n",
      "        \"\"\"Return list of thetas at particular level\"\"\"\n",
      "        t = np.split(thetas, self.theta_ind)\n",
      "        return [t[i].reshape(shape) for i, \n",
      "                          shape in enumerate(self.theta_shape)]\n",
      "    \n",
      "    def add_ones(self, X):\n",
      "        return np.column_stack((X, np.ones(X.shape[0])))\n",
      "    \n",
      "    def del_ones(self, X):\n",
      "        return X[:, :-1]\n",
      "    \n",
      "    def feedforward(self, a_, thetas):\n",
      "        \"\"\"Feed forward, prediction, add ones to the end for bias\"\"\"\n",
      "        thetas = self.shape_thetas(thetas)\n",
      "        activations = [0] * self.n_levels\n",
      "        activations[0] = a_\n",
      "        for i in range(self.n_levels - 1): \n",
      "            activations[i+1] = self.g(np.dot(add_one(activations[i]), thetas[i]))\n",
      "        return activations\n",
      "    \n",
      "    \n",
      "    def feedforwardMatrix(self, A, thetas):\n",
      "        \"\"\"Feed forward, prediction, add ones to the end for bias\"\"\"\n",
      "        thetas = self.shape_thetas(thetas)\n",
      "        activations = [0] * self.n_levels\n",
      "        activations[0] = A\n",
      "        for i in range(self.n_levels - 1): \n",
      "            activations[i+1] = self.g(self.add_ones(activations[i]).dot(thetas[i]))\n",
      "        return activations\n",
      "    \n",
      "    \n",
      "    def backprop(self, x_, y_, thetas):\n",
      "        \"\"\"Add bias to the end\"\"\"\n",
      "        # Compute activations for each level. It includes first level too\n",
      "        activations = self.feedforward( x_, thetas)\n",
      "        # Compute error at the last level\n",
      "        err = - np.multiply( y_ - activations[-1],  \n",
      "                             self.g_derivative( activations[-1])) \n",
      "        # Prepare space for computing errors on all layers and insert output level\n",
      "        errors = [0] * self.n_levels\n",
      "        errors[self.n_levels - 1] = err \n",
      "        # Reshape thetas\n",
      "        thetas = self.shape_thetas( thetas)\n",
      "        # Compute errors on hidden layers and input layer. Start from behind\n",
      "        for l in range(self.n_levels - 2, -1, -1):  \n",
      "            th_ = err.dot(thetas[l].T) \n",
      "            ac_ = add_one( activations[l] ) # add bias\n",
      "            ac_ = np.multiply( ac_, (1 - ac_))\n",
      "            errors[l] = np.multiply( th_, ac_)\n",
      "            err = del_one(errors[l])\n",
      "\n",
      "        return errors, activations\n",
      "    \n",
      "    \n",
      "    def backpropMatrix(self, X, Y, thetas):\n",
      "        \"\"\"Add bias to the end\"\"\"\n",
      "        # Compute activations for each level. It includes first level too\n",
      "        activations = self.feedforwardMatrix( X, thetas)\n",
      "        # Compute error at the last level\n",
      "        err = - np.multiply( Y - activations[-1],  \n",
      "                             self.g_derivative( activations[-1])) \n",
      "        # Prepare space for computing errors on all layers and insert output level\n",
      "        errors = [0] * self.n_levels\n",
      "        errors[self.n_levels - 1] = err \n",
      "        # Reshape thetas\n",
      "        thetas = self.shape_thetas( thetas)\n",
      "        # Compute errors on hidden layers and input layer. Start from behind\n",
      "        for l in range(self.n_levels - 2, 0, -1):\n",
      "            new_err = err.dot(thetas[l].T) \n",
      "            act = self.add_ones( activations[l] ) # add bias\n",
      "            act = self.g_derivative( act )\n",
      "            #print(\"Act\", act)\n",
      "            #print(\"NewErr\", new_err)\n",
      "            errors[l] = np.multiply( new_err, act)\n",
      "            err = errors[l]\n",
      "            err = self.del_ones(errors[l])\n",
      "\n",
      "        return errors, activations\n",
      "    \n",
      "    \n",
      "    def l2_reg( self, thetas):\n",
      "        \"\"\"Regularization\"\"\"\n",
      "        thetas2 = thetas * thetas\n",
      "        thetas2 = self.shape_thetas( thetas2)\n",
      "        sum_ = 0\n",
      "        for levelthetas in thetas2:\n",
      "            for i in range( levelthetas.shape[0] - 1):\n",
      "                sum_ += np.sum( levelthetas[i])\n",
      "        return sum_ * self.lambda_ / 2\n",
      "    \n",
      "    \n",
      "    def l2_reg_grad( self, thetas):\n",
      "        \"\"\"Gradient of regularization\"\"\"\n",
      "        thetasModified = np.array(self.shape_thetas( thetas))\n",
      "        for i in range( len( thetasModified)):\n",
      "            thetasModified[i][-1] *= 0\n",
      "        return self.lambda_ * thetasModified\n",
      "    \n",
      "    \n",
      "    # TODO self.m, self.X, self.Y, self.lambda_\n",
      "    def J(self, thetas):\n",
      "        \"\"\"Cost funciton\"\"\"\n",
      "        sum_ = 0\n",
      "        m = self.X.shape[0]\n",
      "        for i in range( m):\n",
      "            sum_ += 1/(2*m) * np.sum( np.square( self.feedforward( self.X[i], thetas)[-1] - self.Y[i])) \n",
      "        sum_ = sum_ + self.l2_reg( thetas)\n",
      "        return sum_\n",
      "\n",
      "    \n",
      "    def Jgrad(self, thetas):\n",
      "        \"\"\"Gradient\"\"\"\n",
      "        \n",
      "        m = self.X.shape[0]\n",
      "        errorsANDactivations = [ self.backprop( self.X[i], self.Y[i], thetas) for i in range(self.X.shape[0])]\n",
      "        \n",
      "        #print(errorsANDactivations)\n",
      "        \n",
      "        #errorsANDactivations = map( lambda z: self.backprop( z[0], z[1], thetas), zip( self.X, self.Y))\n",
      "        grad = [0] * (self.n_levels-1)\n",
      "        for errors, activations in errorsANDactivations:\n",
      "            for l in range(self.n_levels-1):\n",
      "                err = del_one(errors[l+1]) if l != self.n_levels-2 else errors[l+1]\n",
      "                cur_grad = np.matrix(add_one(activations[l])).T.dot(np.matrix(err))\n",
      "                grad[l] = grad[l] + cur_grad\n",
      "        \n",
      "        grad = np.array(grad) / m + self.l2_reg_grad( thetas)\n",
      "        flatten = np.array([])\n",
      "        for matrix in grad:\n",
      "            flatten = np.append(flatten, matrix.ravel())\n",
      "        return flatten\n",
      "    \n",
      "    \n",
      "    def JgradMatrix(self, thetas):\n",
      "        \"\"\"Gradient\"\"\"\n",
      "        \n",
      "        m = self.X.shape[0]\n",
      "        errors, activations = self.backpropMatrix( self.X, self.Y, thetas) \n",
      "            \n",
      "        grad = [0] * (self.n_levels-1)\n",
      "        for i in range(m):\n",
      "            for l in range(self.n_levels-1):\n",
      "                err = del_one(errors[l+1][i]) if l != self.n_levels-2 else errors[l+1][i]\n",
      "                cur_grad = np.matrix(add_one(activations[l][i])).T.dot(np.matrix(err))\n",
      "                grad[l] = grad[l] + cur_grad\n",
      "        \n",
      "        grad = np.array(grad) / m + self.l2_reg_grad( thetas)\n",
      "        flatten = np.array([])\n",
      "        for matrix in grad:\n",
      "            flatten = np.append(flatten, matrix.ravel())\n",
      "        return flatten\n",
      "    \n",
      "    def set_data( self, X, Y, thetas=None):\n",
      "        self.X = X\n",
      "        self.Y = Y\n",
      "        self.thetas = thetas if thetas != None else self.init_thetas()\n",
      "    \n",
      "    def fit(self, X, Y, thetas=None):\n",
      "        \n",
      "        self.set_data(X, Y, thetas)\n",
      "        self.seed = True   \n",
      "        \n",
      "        if False:\n",
      "            grad = self.Jgrad(self.thetas)\n",
      "            grad = np.array([g if random.random() > self.dropout else 0 for g in grad])\n",
      "            #self.learning_rate = np.sum(np.abs(grad))\n",
      "\n",
      "            #print(self.learning_rate)\n",
      "            self.thetas = self.thetas - self.learning_rate * grad\n",
      "            \n",
      "        if True:\n",
      "            self.thetas, _, _ = fmin_l_bfgs_b(func = self.J,\n",
      "                                          x0 = self.thetas,\n",
      "                                           fprime = self.Jgrad,\n",
      "                                           maxiter=self.maxiter,\n",
      "                                           factr=10)   \n",
      "        return self.thetas\n",
      "    \n",
      "    def predict(self, X):\n",
      "        #print(\"pred\", X.shape, np.array( [ self.feedforward(x_, self.thetas) for x_ in X]).shape)\n",
      "        return np.array( [ self.feedforward(x_, self.thetas)[-1] for x_ in X])\n",
      "    \n",
      "    def get_params(self, *args, **argss):\n",
      "        return {'arch':self.arch, 'g':self.g,\n",
      "                'g_derivative': self.g_derivative,\n",
      "                'lambda_':self.lambda_,\n",
      "                'maxiter':self.maxiter,\n",
      "                'learning_rate':self.learning_rate,\n",
      "                'dropout':self.dropout}\n",
      "                    \n",
      "    def predict_proba(self, X):\n",
      "        self.predict(X)\n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 252
    },
    {
     "cell_type": "heading",
     "level": 6,
     "metadata": {},
     "source": [
      "Test feed forward and cost function"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "threshold = 1\n",
      "lenght = 4\n",
      "\n",
      "X = X_train[0:threshold, :lenght]\n",
      "Y = Y_train[0:threshold]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 222
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 223,
       "text": [
        "array([[ 5.1,  3.5,  1.4,  0.2]])"
       ]
      }
     ],
     "prompt_number": 223
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Test feed forward OK\n",
      "\n",
      "lr = NeuralNetLearner([X.shape[1], 2,  NUM_OF_CLASSES]) \n",
      "thetas = np.array([1.5] * lr.theta_len)\n",
      "print( \"Test feedforward:\\n\", lr.feedforward(X, thetas))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Test feedforward:\n",
        " [array([[ 5.1,  3.5,  1.4,  0.2]]), array([ 0.99999995,  0.99999995]), array([ 0.98901306,  0.98901306,  0.98901306])]\n"
       ]
      }
     ],
     "prompt_number": 224
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Test feed forward matrix OK\n",
      "\n",
      "lr = NeuralNetLearner([X.shape[1], 2,  NUM_OF_CLASSES]) \n",
      "thetas = np.array([1.5] * lr.theta_len)\n",
      "print( \"Test feedforward:\\n\", lr.feedforwardMatrix(X, thetas))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Test feedforward:\n",
        " [array([[ 5.1,  3.5,  1.4,  0.2]]), array([[ 0.99999995,  0.99999995]]), array([[ 0.98901306,  0.98901306,  0.98901306]])]\n"
       ]
      }
     ],
     "prompt_number": 225
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Test regularization OK\n",
      "\n",
      "lr = NeuralNetLearner([X.shape[1], 2,  NUM_OF_CLASSES]) \n",
      "lr.lambda_ = 1e-5\n",
      "thetas = np.array([1.5] * lr.theta_len)\n",
      "lr.l2_reg(thetas)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 226,
       "text": [
        "0.00015750000000000001"
       ]
      }
     ],
     "prompt_number": 226
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Test cost function OK\n",
      "\n",
      "lr = NeuralNetLearner([X.shape[1], 2,  NUM_OF_CLASSES]) \n",
      "thetas = np.array([1.5] * lr.theta_len)\n",
      "lr.X = X\n",
      "lr.Y = Y\n",
      "print( \"Cost function:\", lr.J(thetas))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Cost function: 0.978364680859\n"
       ]
      }
     ],
     "prompt_number": 227
    },
    {
     "cell_type": "heading",
     "level": 6,
     "metadata": {},
     "source": [
      "Test backprop and gradient"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "lr = NeuralNetLearner([X.shape[1], 2,  NUM_OF_CLASSES]) \n",
      "thetas = np.array([1.5] * lr.theta_len)\n",
      "lr.X = X\n",
      "lr.Y = Y\n",
      "lr.backpropMatrix(X, Y, thetas)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 232,
       "text": [
        "([0,\n",
        "  array([[  1.62119731e-09,   1.62119731e-09,   0.00000000e+00]]),\n",
        "  array([[-0.00011939,  0.01074684,  0.01074684]])],\n",
        " [array([[ 5.1,  3.5,  1.4,  0.2]]),\n",
        "  array([[ 0.99999995,  0.99999995]]),\n",
        "  array([[ 0.98901306,  0.98901306,  0.98901306]])])"
       ]
      }
     ],
     "prompt_number": 232
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Test Jgrad OK\n",
      "\n",
      "lr = NeuralNetLearner([X.shape[1], 2,  NUM_OF_CLASSES]) \n",
      "thetas = np.array([1.5] * lr.theta_len)\n",
      "lr.X = X\n",
      "lr.Y = Y\n",
      "print( \"grad =\", lr.Jgrad(thetas))  # = -2.310293595541139e-12"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "grad = [  1.50082681e-05   1.50082681e-05   1.50056742e-05   1.50056742e-05\n",
        "   1.50022697e-05   1.50022697e-05   1.50003242e-05   1.50003242e-05\n",
        "   1.62119731e-09   1.62119731e-09  -1.04386672e-04   1.07618441e-02\n",
        "   1.07618441e-02  -1.04386672e-04   1.07618441e-02   1.07618441e-02\n",
        "  -1.19386678e-04   1.07468447e-02   1.07468447e-02]\n"
       ]
      }
     ],
     "prompt_number": 228
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def grad_approx(lr, thetas, e=1e-4):\n",
      "    \"\"\"Aproximation\"\"\"\n",
      "    return np.array([(lr.J(thetas+eps) - lr.J(thetas-eps))/(2*e)\n",
      "                     for eps in np.identity(len(thetas)) * e]).ravel()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 229
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Test grad aprox OK\n",
      "\n",
      "lr = NeuralNetLearner([X.shape[1], 2,  NUM_OF_CLASSES]) \n",
      "thetas = np.array([1.5] * lr.theta_len)\n",
      "lr.X = X\n",
      "lr.Y = Y\n",
      "print( \"grad =\\n\", grad_approx(lr, thetas))  # = -2.310293595541139e-12"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "grad =\n",
        " [  1.50082685e-05   1.50082685e-05   1.50056739e-05   1.50056739e-05\n",
        "   1.50022694e-05   1.50022694e-05   1.50003238e-05   1.50003238e-05\n",
        "   1.61926028e-09   1.61926028e-09  -1.04386673e-04   1.07618441e-02\n",
        "   1.07618441e-02  -1.04386673e-04   1.07618441e-02   1.07618441e-02\n",
        "  -1.19386679e-04   1.07468447e-02   1.07468447e-02]\n"
       ]
      }
     ],
     "prompt_number": 230
    },
    {
     "cell_type": "heading",
     "level": 6,
     "metadata": {},
     "source": [
      "JgradMatrix"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "lr = NeuralNetLearner([X.shape[1], 2,  NUM_OF_CLASSES]) \n",
      "thetas = np.array([1.5] * lr.theta_len)\n",
      "lr.X = X\n",
      "lr.Y = Y\n",
      "lr.JgradMatrix(thetas)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 257,
       "text": [
        "array([  1.50168258e-05,   1.50168258e-05,   1.50113637e-05,\n",
        "         1.50113637e-05,   1.50048464e-05,   1.50048464e-05,\n",
        "         1.50006110e-05,   1.50006110e-05,   3.64105963e-09,\n",
        "         3.64105963e-09,  -1.04386709e-04,   1.07618454e-02,\n",
        "         1.07618454e-02,  -1.04386709e-04,   1.07618454e-02,\n",
        "         1.07618454e-02,  -1.19386723e-04,   1.07468466e-02,\n",
        "         1.07468466e-02])"
       ]
      }
     ],
     "prompt_number": 257
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "lr = NeuralNetLearner([X.shape[1], 7,  NUM_OF_CLASSES]) \n",
      "thetas = np.array([1.5] * lr.theta_len)\n",
      "lr.X = X\n",
      "lr.Y = Y\n",
      "lr.JgradMatrix(thetas)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 258,
       "text": [
        "array([  1.50000097e-05,   1.50000097e-05,   1.50000097e-05,\n",
        "         1.50000097e-05,   1.50000097e-05,   1.50000097e-05,\n",
        "         1.50000097e-05,   1.50000065e-05,   1.50000065e-05,\n",
        "         1.50000065e-05,   1.50000065e-05,   1.50000065e-05,\n",
        "         1.50000065e-05,   1.50000065e-05,   1.50000028e-05,\n",
        "         1.50000028e-05,   1.50000028e-05,   1.50000028e-05,\n",
        "         1.50000028e-05,   1.50000028e-05,   1.50000028e-05,\n",
        "         1.50000004e-05,   1.50000004e-05,   1.50000004e-05,\n",
        "         1.50000004e-05,   1.50000004e-05,   1.50000004e-05,\n",
        "         1.50000004e-05,   2.09326301e-12,   2.09326301e-12,\n",
        "         2.09326301e-12,   2.09326301e-12,   2.09326301e-12,\n",
        "         2.09326301e-12,   2.09326301e-12,   1.49999622e-05,\n",
        "         2.11441057e-05,   2.11441057e-05,   1.49999622e-05,\n",
        "         2.11441057e-05,   2.11441057e-05,   1.49999622e-05,\n",
        "         2.11441057e-05,   2.11441057e-05,   1.49999622e-05,\n",
        "         2.11441057e-05,   2.11441057e-05,   1.49999622e-05,\n",
        "         2.11441057e-05,   2.11441057e-05,   1.49999622e-05,\n",
        "         2.11441057e-05,   2.11441057e-05,   1.49999622e-05,\n",
        "         2.11441057e-05,   2.11441057e-05,  -3.77507396e-11,\n",
        "         6.14410643e-06,   6.14410643e-06])"
       ]
      }
     ],
     "prompt_number": 258
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Difference OK 9.16699325959e-11\n",
      "# This may not work after dropout application\n",
      "\n",
      "lr = NeuralNetLearner([X.shape[1], 2,  NUM_OF_CLASSES]) \n",
      "thetas = np.array([1.5] * lr.theta_len)\n",
      "lr.X = X\n",
      "lr.Y = Y\n",
      "print( \"Grad diff =\", np.sum( grad_approx(lr, thetas) - lr.Jgrad(thetas) ) )"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Grad diff = 9.5251561158e-11\n"
       ]
      }
     ],
     "prompt_number": 255
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Cross Validation"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn import metrics\n",
      "\n",
      "def evaluate(pred, real):\n",
      "    return metrics.log_loss(real, pred)\n",
      "\n",
      "def my_cross_validation(X, Y, model, k=3):\n",
      "    S = []\n",
      "    kf = cross_validation.KFold(len(Y), n_folds=k, shuffle=False)\n",
      "    for train_index, test_index in kf:\n",
      "        print(\"New fold\")\n",
      "        # Split\n",
      "        X_train, X_test = X[train_index], X[test_index]\n",
      "        Y_train, Y_test = Y[train_index], Y[test_index]\n",
      "        \n",
      "        model.fit(X_train, Y_train)\n",
      "        predicted = model.predict(X_test)\n",
      "        #print(predicted.shape)\n",
      "        # Evaluate\n",
      "        score = evaluate(predicted, Y_test)\n",
      "        print(\"                                             ### Vmesni rezultat ###: \", score)\n",
      "        S.append( score )\n",
      "    S = np.array(S)\n",
      "    return(np.mean(S))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Batch Stohastic"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# stohastic training:   size = 1\n",
      "# batch training:       size = num of samples \n",
      "# batch sequential:     otherwise\n",
      "class BatchSeq():\n",
      "    def __init__(self, learner, size=0, grad_start=5, iterations=500):\n",
      "        self.learner = learner\n",
      "        self.size = size\n",
      "        self.grad_start = grad_start\n",
      "        self.iterations = iterations\n",
      "\n",
      "        \n",
      "    def fit_thetas(self, X, y_, thetas=None):\n",
      "        # num of samples = 1130 = 0, 100, ..., 900, 1000, 1130 (last block takes all remainded)\n",
      "        split = [ [lo, lo + self.size] \n",
      "                          for lo in range( 0, X.shape[0], self.size)\n",
      "                        ] if X.shape[0] >= self.size else [[0, 0]]\n",
      "        split[-1][-1] = X.shape[0]\n",
      "        counter = 0\n",
      "        for lo, hi in split:\n",
      "            counter += 1\n",
      "            thetas = self.learner.fit( X[lo:hi], y_[lo:hi], thetas=thetas)\n",
      "        return thetas\n",
      "    \n",
      "    def fit(self, X, Y):\n",
      "        # batch vs stohastic\n",
      "        if self.size <= 0:\n",
      "            self.size = X.shape[0]\n",
      "        \n",
      "        # stop criteria\n",
      "        eps = 1e-7\n",
      "        \n",
      "        # init thetas and grad. first step\n",
      "        thetas = self.learner.init_thetas()\n",
      "        self.learner.grad_step = self.grad_start\n",
      "        \n",
      "        random.seed(42)\n",
      "        \n",
      "        for i in range(self.iterations):\n",
      "            start = time.time()\n",
      "            \n",
      "            # decrease grad step\n",
      "            #self.learner.grad_step *= 0.95 \n",
      "            # shuffle a bit\n",
      "            \n",
      "            X, Y = shuffle(X, Y)\n",
      "            prev_thetas = thetas\n",
      "            thetas = self.fit_thetas(X, Y, thetas)   \n",
      "            diff = abs(thetas - prev_thetas)\n",
      "            \n",
      "            end = time.time()\n",
      "            print(\"Iter:\", i, \" \\n        Time:\", round( end - start, 2) , \"s    Theta diff:\",sum( diff ))\n",
      "            if (sum( diff > eps ) == 0):\n",
      "                print(\"OK Convergent\")\n",
      "                break        \n",
      "    \n",
      "    def predict(self, X):\n",
      "        return self.learner.predict(X)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn import metrics\n",
      "lr = NeuralNetLearner([X.shape[1], 7,  NUM_OF_CLASSES]) \n",
      "thetas = np.array([1.5] * lr.theta_len)\n",
      "lr.X = X\n",
      "lr.Y = Y\n",
      "scores = cross_validation.cross_val_score(lr, X, Y, cv=5, scoring='log_loss')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "TypeError",
       "evalue": "unorderable types: NoneType() <= float()",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
        "\u001b[1;32m<ipython-input-253-94ef631d46bc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mY\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mscores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcross_validation\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcross_val_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'log_loss'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
        "\u001b[1;32mC:\\Python34\\lib\\site-packages\\sklearn\\cross_validation.py\u001b[0m in \u001b[0;36mcross_val_score\u001b[1;34m(estimator, X, y, scoring, cv, n_jobs, verbose, fit_params, score_func, pre_dispatch)\u001b[0m\n\u001b[0;32m   1149\u001b[0m                                               \u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1150\u001b[0m                                               fit_params)\n\u001b[1;32m-> 1151\u001b[1;33m                       for train, test in cv)\n\u001b[0m\u001b[0;32m   1152\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1153\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32mC:\\Python34\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m    651\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    652\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mfunction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 653\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    654\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    655\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpre_dispatch\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"all\"\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mn_jobs\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32mC:\\Python34\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36mdispatch\u001b[1;34m(self, func, args, kwargs)\u001b[0m\n\u001b[0;32m    398\u001b[0m         \"\"\"\n\u001b[0;32m    399\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pool\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 400\u001b[1;33m             \u001b[0mjob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImmediateApply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    401\u001b[0m             \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    402\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0m_verbosity_filter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32mC:\\Python34\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, func, args, kwargs)\u001b[0m\n\u001b[0;32m    136\u001b[0m         \u001b[1;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    137\u001b[0m         \u001b[1;31m# arguments in memory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 138\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    139\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    140\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32mC:\\Python34\\lib\\site-packages\\sklearn\\cross_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[1;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters)\u001b[0m\n\u001b[0;32m   1238\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1239\u001b[0m         \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1240\u001b[1;33m     \u001b[0mtest_score\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscorer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1241\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mreturn_train_score\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1242\u001b[0m         \u001b[0mtrain_score\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscorer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32mC:\\Python34\\lib\\site-packages\\sklearn\\cross_validation.py\u001b[0m in \u001b[0;36m_score\u001b[1;34m(estimator, X_test, y_test, scorer)\u001b[0m\n\u001b[0;32m   1294\u001b[0m         \u001b[0mscore\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscorer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1295\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1296\u001b[1;33m         \u001b[0mscore\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscorer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1297\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnumbers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mNumber\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1298\u001b[0m         raise ValueError(\"scoring must return a number, got %s (%s) instead.\"\n",
        "\u001b[1;32mC:\\Python34\\lib\\site-packages\\sklearn\\metrics\\scorer.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, clf, X, y)\u001b[0m\n\u001b[0;32m    104\u001b[0m         \"\"\"\n\u001b[0;32m    105\u001b[0m         \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 106\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sign\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_score_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    107\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    108\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_factory_args\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32mC:\\Python34\\lib\\site-packages\\sklearn\\metrics\\metrics.py\u001b[0m in \u001b[0;36mlog_loss\u001b[1;34m(y_true, y_pred, eps, normalize)\u001b[0m\n\u001b[0;32m   1104\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1105\u001b[0m     \u001b[1;31m# Clipping\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1106\u001b[1;33m     \u001b[0mY\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0meps\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1107\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1108\u001b[0m     \u001b[1;31m# This happens in cases when elements in y_pred have type \"str\".\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32mC:\\Python34\\lib\\site-packages\\numpy\\core\\fromnumeric.py\u001b[0m in \u001b[0;36mclip\u001b[1;34m(a, a_min, a_max, out)\u001b[0m\n\u001b[0;32m   1625\u001b[0m         \u001b[0mclip\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclip\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1626\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1627\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_wrapit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'clip'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ma_min\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ma_max\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1628\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mclip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma_min\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ma_max\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1629\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32mC:\\Python34\\lib\\site-packages\\numpy\\core\\fromnumeric.py\u001b[0m in \u001b[0;36m_wrapit\u001b[1;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[0;32m     43\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m         \u001b[0mwrap\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 45\u001b[1;33m     \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     46\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mwrap\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;31mTypeError\u001b[0m: unorderable types: NoneType() <= float()"
       ]
      }
     ],
     "prompt_number": 253
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# napak prvega nivoja ne rabimo racunat"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}