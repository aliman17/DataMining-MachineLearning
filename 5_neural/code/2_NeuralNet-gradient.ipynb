{
 "metadata": {
  "name": "",
  "signature": "sha256:c33691e19d38bec46ae631620b6a91e8dc33d8ce6312464c74d035949f903604"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from scipy.optimize import fmin_l_bfgs_b\n",
      "from sklearn import cross_validation\n",
      "from sklearn import preprocessing\n",
      "import numpy as np\n",
      "import scipy as sp"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Load Data"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "norm = preprocessing.Normalizer()\n",
      "X_train = np.loadtxt(open(\"../data/train.csv\",\"rb\"),delimiter=\",\",skiprows=1, usecols=range(1,94))\n",
      "X_train = norm.fit_transform(X_train)\n",
      "\n",
      "y_train = np.loadtxt(open(\"../data/train.csv\",\"rb\"),dtype=str,delimiter=\",\",skiprows=1, usecols=[94])\n",
      "y_train = np.array([int(c[-2]) for c in y_train])  # Parse classes from Class_1 into 1\n",
      "\n",
      "X_test = np.loadtxt(open(\"../data/test.csv\",\"rb\"),delimiter=\",\",skiprows=1, usecols=range(1,94))\n",
      "X_test = norm.transform(X_test)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Helper Functions"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def add_ones(X):\n",
      "    return np.column_stack(X, (np.ones(len(X))))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def add_one(X):\n",
      "    return np.append(X, 1)\n",
      "\n",
      "def del_one(X):\n",
      "    return X[:-1]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def sigmoid(z):\n",
      "    return 1/(1+np.exp(-z))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Learner"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class NeuralNetClassifier():\n",
      "    \"\"\"Neural network classifier based on a set of binary classifiers.\"\"\"\n",
      "    def __init__(self, h, thetas):\n",
      "        self.thetas = thetas  # model parameters\n",
      "        self.h = h\n",
      "        \n",
      "    def predict(self, X):\n",
      "        y_hat = np.ravel(self.h(X, self.thetas))\n",
      "        # following works only for binary classifiers, correct it for multiclass\n",
      "        return np.vstack((1-y_hat, y_hat)).T"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Learner"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class NeuralNetLearner():\n",
      "    def __init__(self, arch, g=sigmoid, lambda_=1e-5):\n",
      "        self.arch = arch\n",
      "        self.n_levels = len(arch)\n",
      "        self.g = g\n",
      "        self.lambda_ = lambda_\n",
      "        self.theta_shape = np.array([ ( arch[i] + 1, arch[i + 1]) for i in range( len( arch) - 1)])\n",
      "        ind = np.array( [ sh1 * sh2 for sh1, sh2 in self.theta_shape])\n",
      "        self.theta_ind = np.cumsum( ind[:-1])  # last index falls out of the boundary\n",
      "        self.theta_len = sum(ind)\n",
      "    \n",
      "    def init_thetas(self, epsilon=1e-5):\n",
      "        \"\"\"Return thetas with random values\"\"\"\n",
      "        np.random.seed(42)\n",
      "        return np.random.rand(self.theta_len) * 2 * epsilon - epsilon\n",
      "    \n",
      "    def shape_thetas(self, thetas):\n",
      "        \"\"\"Return list of thetas at particular level\"\"\"\n",
      "        t = np.split(thetas, self.theta_ind)\n",
      "        return np.array( [t[i].reshape(shape) for i, shape in enumerate(self.theta_shape)])\n",
      "    \n",
      "    def feedforward(self, thetas):\n",
      "        \"\"\"Feed forward, prediction, add ones to the end for bias\"\"\"\n",
      "        a = self.x_\n",
      "        thetas = self.shape_thetas(thetas)\n",
      "        activationsByLevel = [a]\n",
      "        # compute from level 2, 3, 4 ... and store such\n",
      "        for theta in thetas: \n",
      "            a = add_one(a)\n",
      "            a = self.g(a.dot(theta))\n",
      "            activationsByLevel.append(a)\n",
      "        return np.array(activationsByLevel)\n",
      "    \n",
      "    def backprop(self, thetas):\n",
      "        \"\"\"Add bias to the end\"\"\"\n",
      "        x = self.x_\n",
      "        y = self.y_out_\n",
      "        \n",
      "        activations = self.feedforward( thetas)\n",
      "        thetas = self.shape_thetas( thetas)\n",
      "        # output error\n",
      "        err = - np.multiply( y - activations[-1],  np.multiply( activations[-1], 1-activations[-1])) \n",
      "               \n",
      "        errors = [0] * self.n_levels\n",
      "        errors[self.n_levels - 1] = err \n",
      "        \n",
      "        levels = list(range(0, self.n_levels-1))\n",
      "        for l in levels[::-1]:  # reverse\n",
      "            th_ = thetas[l] * np.matrix(err).T\n",
      "            th_ = th_.T\n",
      "            act = activations[l]  \n",
      "            ac_ = add_one( act ) # add bias\n",
      "            ac_ = np.multiply( ac_, (1 - ac_))\n",
      "            errors[l] = np.array( np.multiply( th_, ac_) )[0]\n",
      "            err = del_one(errors[l])\n",
      "\n",
      "        return np.array(errors), activations\n",
      "    \n",
      "    def J(self, thetas):\n",
      "        # try for one example\n",
      "        #######################################################\n",
      "        return 1/2 * sum(((self.feedforward( thetas)[-1] - self.y_out_)**2)[0]) # add [0] because we get matrix form\n",
      "\n",
      "    def Jgrad(self, thetas):\n",
      "        errors, activations = self.backprop( thetas)\n",
      "        grad = np.array([])\n",
      "        for l in range(self.n_levels-1):\n",
      "            ac_ = add_one(activations[l])\n",
      "            err = del_one(errors[l+1]) if l != self.n_levels-2 else errors[l+1]\n",
      "            cur_grad = np.matrix(err).T.dot(np.matrix(ac_)).T\n",
      "            grad = np.append(grad , cur_grad)\n",
      "        return np.array(grad)\n",
      "        \n",
      "    def grad_approx(self, thetas, e=1e-1):\n",
      "        #for eps in np.identity(len(thetas)) * e:\n",
      "        #    print(((self.J(thetas+eps) - self.J(thetas-eps))/(2*e)))\n",
      "        return np.array([(self.J(thetas+eps) - self.J(thetas-eps))/(2*e)\n",
      "                         for eps in np.identity(len(thetas)) * e])\n",
      "\n",
      "    \n",
      "    \n",
      "    def fit(self, x_, y):\n",
      "        \"\"\" x_ is a vector, y is a class between 1 and sth\"\"\"\n",
      "        self.x_ = x_\n",
      "        self.y = y  \n",
      "        \n",
      "        I = np.identity(self.arch[-1])                # identity matrix of the size of the output\n",
      "        self.y_out_ = [ I[y-1] for y in self.y]       # classes starts from 1, so it needs to be substracted by 1\n",
      "        \n",
      "        self.thetas = self.init_thetas()\n",
      "\n",
      "        #thetas, fmin, info = fmin_l_bfgs_b(func = lambda thetas: self.J(self.X, self.y_bin[0], thetas),\n",
      "        #                                   x0 = thetas)\n",
      "        # we have local min \n",
      "        #model = NeuralNetClassifier(self.feedforward, thetas)\n",
      "        \n",
      "        #return model\n",
      "\n",
      "    def test(self, a):\n",
      "        thetas = np.array([-30, 10, 20, -20, 20, -20, -10, 20, 20])\n",
      "        print(self.h(a, thetas))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 67
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Testing"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X = X_train[:1, :5]\n",
      "y = y_train[:1]\n",
      "lr = NeuralNetLearner([X.shape[1], 2,  9])\n",
      "r = lr.fit(X[0], y)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 68
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "# Test atributes and some minor functions of a learner\n",
      "\n",
      "print( \"Testing shape\\n\", lr.theta_shape)\n",
      "print( \"Testing indices\\n\", lr.theta_ind)\n",
      "print( \"Testing theta lenght\\n\", lr.theta_len)\n",
      "print( \"Testing theta shape\\n\", lr.shape_thetas( thetas))\n",
      "print( \"Testing feedforward\\n\", lr.feedforward( thetas))"
     ]
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "# Test feed forward\n",
      "\n",
      "print( \"Test feed forward:\")\n",
      "print( lr.feedforward(lr.thetas))"
     ]
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "# Test back propagation\n",
      "\n",
      "print( \"Test errors and activations\")\n",
      "errors, acts = lr.backprop(lr.thetas)\n",
      "print( errors.shape, acts.shape)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Test cost function\n",
      "\n",
      "print( \"Cost function:\", lr.J(lr.thetas))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Cost function: 1.12499737142\n"
       ]
      }
     ],
     "prompt_number": 107
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Test gradient function \n",
      "\n",
      "print( \"Grad. function:\", lr.Jgrad(lr.thetas))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Grad. function: [ -6.35220965e-08  -1.54514760e-08   0.00000000e+00   0.00000000e+00\n",
        "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
        "   0.00000000e+00   0.00000000e+00  -9.94278082e-07  -2.41853855e-07\n",
        "  -6.24996369e-02   6.24993346e-02   6.24992823e-02   6.24998632e-02\n",
        "   6.25000143e-02   6.24998020e-02   6.24995564e-02   6.24994075e-02\n",
        "   6.24997037e-02  -6.25002532e-02   6.24999509e-02   6.24998987e-02\n",
        "   6.25004796e-02   6.25006307e-02   6.25004183e-02   6.25001728e-02\n",
        "   6.25000239e-02   6.25003201e-02  -1.24999883e-01   1.24999278e-01\n",
        "   1.24999174e-01   1.25000336e-01   1.25000638e-01   1.25000213e-01\n",
        "   1.24999722e-01   1.24999424e-01   1.25000017e-01]\n"
       ]
      }
     ],
     "prompt_number": 108
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Test gradient approximation\n",
      "\n",
      "print( \"Grad. approx:\", lr.grad_approx(lr.thetas, e=1e-5))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Grad. approx: [ -6.35158592e-08  -1.54654067e-08   0.00000000e+00   0.00000000e+00\n",
        "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
        "   0.00000000e+00   0.00000000e+00  -9.94282434e-07  -2.41839881e-07\n",
        "  -6.24996369e-02   6.24993346e-02   6.24992823e-02   6.24998632e-02\n",
        "   6.25000143e-02   6.24998020e-02   6.24995564e-02   6.24994075e-02\n",
        "   6.24997037e-02  -6.25002532e-02   6.24999509e-02   6.24998986e-02\n",
        "   6.25004796e-02   6.25006306e-02   6.25004183e-02   6.25001728e-02\n",
        "   6.25000239e-02   6.25003201e-02  -1.24999883e-01   1.24999278e-01\n",
        "   1.24999174e-01   1.25000336e-01   1.25000638e-01   1.25000213e-01\n",
        "   1.24999722e-01   1.24999424e-01   1.25000017e-01]\n"
       ]
      }
     ],
     "prompt_number": 109
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Test difference\n",
      "print( \"grad - grad.aprox =\", sum( lr.Jgrad(lr.thetas) - lr.grad_approx(lr.thetas, e=1e-5) ))  # = -2.310293595541139e-12"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "grad - grad.aprox = -2.31029359554e-12\n"
       ]
      }
     ],
     "prompt_number": 115
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}