{
 "metadata": {
  "name": "",
  "signature": "sha256:9eda87b77e4cd5f42018918cc9914c446e4c1d69eaa6f382d6a6dbb22f2414e3"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from scipy.optimize import fmin_l_bfgs_b\n",
      "from sklearn import cross_validation\n",
      "from sklearn import preprocessing\n",
      "from sklearn import decomposition\n",
      "from sklearn import datasets\n",
      "import numpy as np\n",
      "import scipy as sp\n",
      "import random\n",
      "import time"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Load Data"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "NUM_OF_CLASSES = 0\n",
      "def load_data1():\n",
      "    \"\"\"Load keggle\"\"\"\n",
      "    global NUM_OF_CLASSES\n",
      "    X_train = np.loadtxt(open(\"../data/train.csv\",\"rb\"),delimiter=\",\",skiprows=1, usecols=range(1,94))\n",
      "    X_test = np.loadtxt(open(\"../data/test.csv\",\"rb\"),delimiter=\",\",skiprows=1, usecols=range(1,94))\n",
      "    \n",
      "    y_train = np.loadtxt(open(\"../data/train.csv\",\"rb\"),dtype=str,delimiter=\",\",skiprows=1, usecols=[94])\n",
      "    y_train = np.array([int(c[-2])-1 for c in y_train])  # Parse classes from Class_1 into 1\n",
      "    return X_train, y_train, X_test\n",
      "\n",
      "def load_data2():\n",
      "    \"\"\"Load iris\"\"\"\n",
      "    iris = datasets.load_iris()\n",
      "    X_train = iris.data\n",
      "    y_train = iris.target.T\n",
      "    return X_train, y_train, None"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 23
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "data = 1\n",
      "if data == 1:\n",
      "    X_train, Y_train, X_test = load_data1()\n",
      "else:\n",
      "    X_train, Y_train, X_test = load_data2()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 24
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X_train[:5, :4]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 25,
       "text": [
        "array([[ 1.,  0.,  0.,  0.],\n",
        "       [ 0.,  0.,  0.,  0.],\n",
        "       [ 0.,  0.,  0.,  0.],\n",
        "       [ 1.,  0.,  0.,  1.],\n",
        "       [ 2.,  1.,  0.,  0.]])"
       ]
      }
     ],
     "prompt_number": 25
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "NUM_OF_CLASSES = len( np.unique(Y_train))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 26
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Preprocesse Data"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "For smaller sets it is usefull to shuffle data and take first 1000 or more."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def shuffle(X, Y):\n",
      "    ind = np.array(list(range(X.shape[0])))\n",
      "    np.random.shuffle(ind)\n",
      "    X = np.array( [ X[i] for i in ind] )\n",
      "    Y = np.array( [ Y[i] for i in ind] )\n",
      "    return X, Y"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 27
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X_train, Y_train = shuffle(X_train, Y_train)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 28
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "std = preprocessing.StandardScaler()\n",
      "X_train = std.fit_transform(X_train)\n",
      "X_test = std.transform(X_test)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 29
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X_train = np.matrix(X_train)\n",
      "X_test = np.matrix(X_test)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 30
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Y must be extended"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def binaryClassRepresentation(Y):\n",
      "    \"\"\" Class 1 write as 1,0,0,0,0,0,0,0,0, class 2 write as 0,1,0,0,0,0,0,0,0, ... \"\"\"\n",
      "    I = np.identity(NUM_OF_CLASSES)\n",
      "    Y = np.array( [ I[y] for y in Y])\n",
      "    return Y"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 31
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "Y_train = binaryClassRepresentation(Y_train)\n",
      "Y_train = np.matrix(Y_train)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 32
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Helper functions"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def add_ones(X):\n",
      "    return np.column_stack((X, np.ones(X.shape[0])))\n",
      "\n",
      "def add_one(X):\n",
      "    return np.append(X, 1)\n",
      "\n",
      "def add_zero(X):\n",
      "    return np.append(X, 0)\n",
      "\n",
      "def del_one(X):\n",
      "    return X[:-1]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 33
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def grad_approx(J, thetas, e=1e-5):\n",
      "        return np.array([(J(thetas+eps) - J(thetas-eps))/(2*e)\n",
      "                         for eps in np.identity(len(thetas)) * e])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 34
    },
    {
     "cell_type": "heading",
     "level": 6,
     "metadata": {},
     "source": [
      "Sigmoid"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def sigmoid(z):\n",
      "    return 1/(1+np.exp(-z))\n",
      "\n",
      "def sigmoid_derivative(z):\n",
      "    return np.multiply(z , 1 - z)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 35
    },
    {
     "cell_type": "heading",
     "level": 6,
     "metadata": {},
     "source": [
      "Tansigmoid"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def tansigmoid(z):\n",
      "    z = z\n",
      "    return ( np.exp(z) - np.exp(-z) ) / (np.exp(z) + np.exp(-z))\n",
      "\n",
      "def tansigmoid_derivative(z):\n",
      "    return (1 -  np.square(tansigmoid(z))) "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 36
    },
    {
     "cell_type": "heading",
     "level": 6,
     "metadata": {},
     "source": [
      "Arctan"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Trying to make our own function\n",
      "def arctan(z):\n",
      "    return np.arctan(2*z) / np.pi + 1/2\n",
      "\n",
      "def arctan_derivative(z):\n",
      "    return 2 / ((np.square(2*z) + 1) * np.pi)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 37
    },
    {
     "cell_type": "heading",
     "level": 6,
     "metadata": {},
     "source": [
      "Test gradient"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "m = np.array([1,2,3])\n",
      "print(\"tansigmoid error\", tansigmoid_derivative(m) - grad_approx(tansigmoid, m).diagonal())\n",
      "print(\"arctan error\", arctan_derivative(m) - grad_approx(arctan, m).diagonal())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "tansigmoid error [ -1.15246146e-11  -9.51938528e-12   5.97277783e-12]\n",
        "arctan error [ -6.15821283e-12  -2.79085088e-12   4.11880252e-12]\n"
       ]
      }
     ],
     "prompt_number": 40
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Learner"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class NeuralNetLearner():\n",
      "    def __init__(self, arch, g=sigmoid, g_derivative=sigmoid_derivative, lambda_=1e-4, maxiter=1, learning_rate=1, dropout_prob=0.0, lbfgs=True, print_=True):\n",
      "        \n",
      "        self.arch = arch\n",
      "        self.g = g\n",
      "        self.g_derivative = g_derivative\n",
      "        self.lambda_ = lambda_\n",
      "        self.maxiter = maxiter\n",
      "        self.learning_rate = learning_rate\n",
      "        self.dropout_prob = dropout_prob\n",
      "        \n",
      "        self.n_levels = len(arch)\n",
      "        \n",
      "        self.theta_shape = [ ( arch[i] + 1, arch[i + 1]) for i in range( len( arch) - 1)]\n",
      "        ind = [ sh1 * sh2 for sh1, sh2 in self.theta_shape]\n",
      "        self.theta_ind = np.cumsum( ind[:-1])  # last index falls out of the boundary\n",
      "        self.theta_len = sum(ind)\n",
      "        self.lbfgs = lbfgs\n",
      "        self.print_ = print_\n",
      "        \n",
      "    \n",
      "    def init_thetas(self, epsilon=1):\n",
      "        \"\"\"Return thetas with random values\"\"\"\n",
      "        #np.random.seed(42)\n",
      "        return np.random.rand(self.theta_len) * 2 * epsilon - epsilon\n",
      "    \n",
      "    \n",
      "    def shape_thetas(self, thetas):\n",
      "        \"\"\"Return list of thetas at particular level\"\"\"\n",
      "        t = np.split(thetas, self.theta_ind)\n",
      "        return [t[i].reshape(shape) for i, \n",
      "                          shape in enumerate(self.theta_shape)]\n",
      "    \n",
      "    \n",
      "    def add_ones(self, X):\n",
      "        if len(X.shape) == 1:\n",
      "            return np.append(X, 1)\n",
      "        else:\n",
      "            return np.column_stack((X, np.ones(X.shape[0])))\n",
      "    \n",
      "    \n",
      "    def del_ones(self, X):\n",
      "        if len(X.shape) == 1:\n",
      "            return X[:-1]\n",
      "        else:\n",
      "            return X[:, :-1]\n",
      "    \n",
      "    \n",
      "    def feedforward(self, a_, thetas):\n",
      "        \"\"\"Feed forward, prediction, add ones to the end for bias\"\"\"\n",
      "        thetas = self.shape_thetas(thetas)\n",
      "        activations = [0] * self.n_levels\n",
      "        activations[0] = a_\n",
      "        for i in range(self.n_levels - 1): \n",
      "            activations[i+1] = self.g(self.add_ones(activations[i]).dot(thetas[i]))\n",
      "        return activations\n",
      "    \n",
      "    \n",
      "    def feedforwardMatrix(self, A, thetas):\n",
      "        \"\"\"Feed forward, prediction, add ones to the end for bias\"\"\"\n",
      "        thetas = self.shape_thetas(thetas)\n",
      "        activations = [0] * self.n_levels\n",
      "        activations[0] = A\n",
      "        for i in range(self.n_levels - 1): \n",
      "            activations[i+1] = self.g(self.add_ones(activations[i]).dot(thetas[i]))\n",
      "        return activations\n",
      "    \n",
      "    \n",
      "    def dropout_mask(self, shape, testTime=False):\n",
      "        if testTime:\n",
      "            return 1 - self.dropout_prob\n",
      "        else:\n",
      "            return np.random.random(shape) > self.dropout_prob\n",
      "    \n",
      "    \n",
      "    def feedforwardMatrixDropout(self, A, thetas, testTime=False):\n",
      "        \"\"\"Feed forward, prediction, add ones to the end for bias\"\"\"   \n",
      "        thetas = self.shape_thetas(thetas)\n",
      "        activations = [0] * self.n_levels\n",
      "        dropmask = [0] * self.n_levels\n",
      "        activations[0] = A\n",
      "        for i in range(self.n_levels - 1): \n",
      "            activations[i+1] = self.g(self.add_ones(activations[i]).dot(thetas[i]))\n",
      "            # Dropout: input layer will be untouched. We need to skip last layer, se we leave 0 for the last layer!\n",
      "            dropmask[i+1] = 1 if ( i == self.n_levels - 2) else self.dropout_mask(activations[i+1].shape, testTime) \n",
      "            activations[i+1] = np.multiply( activations[i+1], dropmask[i+1])\n",
      "        return activations, dropmask\n",
      "    \n",
      "    \n",
      "    def backprop(self, x_, y_, thetas):\n",
      "        \"\"\"Add bias to the end\"\"\"\n",
      "        # Compute activations for each level. It includes first level too\n",
      "        activations = self.feedforward( x_, thetas)\n",
      "        # Compute error at the last level\n",
      "        err = - np.multiply( y_ - activations[-1],  \n",
      "                             self.g_derivative( activations[-1])) \n",
      "        # Prepare space for computing errors on all layers and insert output level\n",
      "        errors = [0] * self.n_levels\n",
      "        errors[self.n_levels - 1] = err \n",
      "        # Reshape thetas\n",
      "        thetas = self.shape_thetas( thetas)\n",
      "        # Compute errors on hidden layers and input layer. Start from behind\n",
      "        for l in range(self.n_levels - 2, -1, -1):  \n",
      "            th_ = err.dot(thetas[l].T) \n",
      "            ac_ = self.add_ones( activations[l] ) # add bias\n",
      "            ac_ = np.multiply( ac_, (1 - ac_))  # here is derivative which should be updated to general derivative\n",
      "                                                # however, we are not using this method\n",
      "            errors[l] = np.multiply( th_, ac_)\n",
      "            err = self.del_ones(errors[l])\n",
      "\n",
      "        return errors, activations\n",
      "    \n",
      "    \n",
      "    def backpropMatrix(self, X, Y, thetas):\n",
      "        \"\"\"Add bias to the end\"\"\"\n",
      "        # Compute activations for each level. It includes first level too\n",
      "        activations = self.feedforwardMatrix( X, thetas)\n",
      "        # Compute error at the last level\n",
      "        err = - np.multiply( Y - activations[-1],  \n",
      "                             self.g_derivative( activations[-1])) \n",
      "        # Prepare space for computing errors on all layers and insert output level\n",
      "        errors = [0] * self.n_levels\n",
      "        errors[self.n_levels - 1] = err \n",
      "        # Reshape thetas\n",
      "        thetas = self.shape_thetas( thetas)\n",
      "        # Compute errors on hidden layers and input layer. Start from behind\n",
      "        for l in range(self.n_levels - 2, 0, -1):\n",
      "            new_err = err.dot(thetas[l].T) \n",
      "            act = self.add_ones( activations[l] ) # add bias\n",
      "            act = self.g_derivative( act )\n",
      "            errors[l] = np.multiply( new_err, act)\n",
      "            err = self.del_ones(errors[l])\n",
      "\n",
      "        return errors, activations\n",
      "    \n",
      "    \n",
      "    def backpropMatrixDropout(self, X, Y, thetas):\n",
      "        \"\"\"Add bias to the end\"\"\"\n",
      "        # Compute activations for each level. It includes first level too\n",
      "        activations, dropmasks = self.feedforwardMatrixDropout( X, thetas)\n",
      "        # Compute error at the last level\n",
      "        err_last_level = - np.multiply( Y - activations[-1],  \n",
      "                             self.g_derivative( activations[-1])) \n",
      "        # Prepare space for computing errors on all layers and insert output level\n",
      "        errors = [0] * self.n_levels\n",
      "        errors[self.n_levels - 1] = err_last_level \n",
      "        # Reshape thetas\n",
      "        thetas = self.shape_thetas( thetas)\n",
      "        # Compute errors on hidden layers, whitout input layer. Start from behind\n",
      "        prev_err = err_last_level\n",
      "        for l in range(self.n_levels - 2, 0, -1):\n",
      "            err = prev_err.dot(thetas[l].T) \n",
      "            act_with_bias = self.add_ones( activations[l] ) # add bias\n",
      "            derivative = self.g_derivative( act_with_bias )\n",
      "            errors[l] = np.multiply( err, derivative)\n",
      "            # use mask for dropout\n",
      "            dropmask_with_bias = self.add_ones( dropmasks[l])\n",
      "            errors[l] = np.multiply( errors[l], dropmask_with_bias)\n",
      "            # for the next round\n",
      "            prev_err = self.del_ones(errors[l])\n",
      "\n",
      "        return errors, activations\n",
      "    \n",
      "    \n",
      "    \n",
      "    def l2_reg( self, thetas):\n",
      "        \"\"\"Regularization\"\"\"\n",
      "        thetas2 = thetas * thetas\n",
      "        thetas2 = self.shape_thetas( thetas2)\n",
      "        sum_ = 0\n",
      "        for levelthetas in thetas2:\n",
      "            for i in range( levelthetas.shape[0] - 1):\n",
      "                sum_ += np.sum( levelthetas[i])\n",
      "        return sum_ * self.lambda_ / 2\n",
      "    \n",
      "    \n",
      "    def l2_reg_grad( self, thetas):\n",
      "        \"\"\"Gradient of regularization\"\"\"\n",
      "        thetasModified = np.array(self.shape_thetas( thetas))\n",
      "        for i in range( len( thetasModified)):\n",
      "            thetasModified[i][-1] *= 0\n",
      "        return self.lambda_ * thetasModified\n",
      "    \n",
      "    \n",
      "    def J(self, thetas):\n",
      "        \"\"\"Cost funciton\"\"\"\n",
      "        sum_ = 0\n",
      "        m = self.X.shape[0]\n",
      "        for i in range( m):\n",
      "            sum_ += 1/(2*m) * np.sum( np.square( self.feedforward( self.X[i], thetas)[-1] - self.Y[i])) \n",
      "        sum_ = sum_ + self.l2_reg( thetas)\n",
      "        return sum_\n",
      "    \n",
      "    def JDropout(self, thetas):\n",
      "        \"\"\"Cost funciton\"\"\"\n",
      "        sum_ = 0\n",
      "        m = self.X.shape[0]\n",
      "        for i in range( m):\n",
      "            sum_ += 1/(2*m) * np.sum( np.square( self.feedforwardMatrixDropout( self.X[i], thetas, testTime=True)[0][-1] - self.Y[i])) \n",
      "        sum_ = sum_ + self.l2_reg( thetas)\n",
      "        return sum_\n",
      "\n",
      "    \n",
      "    def Jgrad(self, thetas):\n",
      "        \"\"\"Gradient\"\"\"\n",
      "        m = self.X.shape[0]\n",
      "        errorsANDactivations = [ self.backprop( self.X[i], self.Y[i], thetas) for i in range(self.X.shape[0])]\n",
      "        grad = [0] * (self.n_levels-1)\n",
      "        for errors, activations in errorsANDactivations:\n",
      "            for l in range(self.n_levels-1):\n",
      "                err = self.del_ones(errors[l+1]) if l != self.n_levels-2 else errors[l+1]\n",
      "                cur_grad = np.matrix(self.add_ones(activations[l])).T.dot(np.matrix(err))\n",
      "                grad[l] = grad[l] + cur_grad\n",
      "        \n",
      "        grad = np.array(grad) / m + self.l2_reg_grad( thetas)\n",
      "        flatten = np.array([])\n",
      "        for matrix in grad:\n",
      "            flatten = np.append(flatten, matrix.ravel())\n",
      "        return flatten\n",
      "    \n",
      "    \n",
      "    def JgradMatrix(self, thetas):\n",
      "        \"\"\"Gradient\"\"\"\n",
      "        m = self.X.shape[0]\n",
      "        errors, activations = self.backpropMatrix( self.X, self.Y, thetas)     \n",
      "        grad = [0] * (self.n_levels-1)\n",
      "        for i in range(m):\n",
      "            for l in range(self.n_levels-1):\n",
      "                err = self.del_ones(errors[l+1][i]) if l != self.n_levels-2 else errors[l+1][i]\n",
      "                cur_grad = self.add_ones(activations[l][i]).T.dot(err)\n",
      "                grad[l] = grad[l] + cur_grad\n",
      "        \n",
      "        grad = np.array(grad) / m + self.l2_reg_grad( thetas)\n",
      "        flatten = np.array([])\n",
      "        for matrix in grad:\n",
      "            flatten = np.append(flatten, matrix.ravel())\n",
      "        return flatten\n",
      "    \n",
      "    def JgradMatrixDropout(self, thetas):\n",
      "        \"\"\"Gradient\"\"\"\n",
      "        m = self.X.shape[0]\n",
      "        errors, activations = self.backpropMatrixDropout( self.X, self.Y, thetas)    \n",
      "        grad = [0] * (self.n_levels-1)\n",
      "        for i in range(m):\n",
      "            for l in range(self.n_levels-1):\n",
      "                err = self.del_ones(errors[l+1][i]) if l != self.n_levels-2 else errors[l+1][i]\n",
      "                err = err if type(err) is np.matrix else np.matrix(err)\n",
      "                act = activations[l][i] if type(activations[l][i]) is np.matrix else np.matrix(activations[l][i])\n",
      "                cur_grad = self.add_ones(act).T.dot(err)\n",
      "                grad[l] = grad[l] + cur_grad\n",
      "        \n",
      "        grad = np.array(grad) / m + self.l2_reg_grad( thetas)\n",
      "        flatten = np.array([])\n",
      "        for matrix in grad:\n",
      "            flatten = np.append(flatten, matrix.ravel())\n",
      "        return flatten\n",
      "    \n",
      "    \n",
      "    def fit(self, X, Y, thetas=None):\n",
      "        self.thetas = thetas if thetas != None else self.init_thetas() \n",
      "            \n",
      "        if self.lbfgs:\n",
      "            self.X = X\n",
      "            self.Y = Y\n",
      "            self.thetas, fmin, info = fmin_l_bfgs_b(func = self.JDropout,\n",
      "                                          x0 = self.thetas,\n",
      "                                           fprime = self.JgradMatrixDropout,\n",
      "                                           maxiter=self.maxiter,\n",
      "                                           #factr=10\n",
      "                                           )   \n",
      "            if self.print_:\n",
      "                print(\"Lbfgs iterations:\", info['funcalls'])\n",
      "        else:\n",
      "            # STOHASTIC\n",
      "            self.X = X\n",
      "            self.Y = Y\n",
      "            grad = self.JgradMatrixDropout(self.thetas)\n",
      "            self.thetas -= self.learning_rate  * grad\n",
      "                \n",
      "        return self.thetas\n",
      "    \n",
      "    def predict(self, X):\n",
      "        pred = self.feedforwardMatrixDropout(X, self.thetas, testTime=True)[0][:][-1]\n",
      "        return pred\n",
      "    \n",
      "    def get_params(self, *args, **argss):\n",
      "        return {'arch':self.arch, 'g':self.g,\n",
      "                'g_derivative': self.g_derivative,\n",
      "                'lambda_':self.lambda_,\n",
      "                'maxiter':self.maxiter,\n",
      "                'learning_rate':self.learning_rate,\n",
      "                'dropout_prob':self.dropout_prob,\n",
      "                'lbfgs':self.lbfgs,\n",
      "                'print_':self.print_}\n",
      "                    \n",
      "    def predict_proba(self, X):\n",
      "        return self.predict(X)\n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 41
    },
    {
     "cell_type": "heading",
     "level": 6,
     "metadata": {},
     "source": [
      "Feed forward"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Test feedforward OK\n",
      "\n",
      "threshold = 1\n",
      "lenght = 4\n",
      "\n",
      "X = X_train[0:threshold, :lenght]\n",
      "Y = Y_train[0:threshold]\n",
      "lr = NeuralNetLearner([X.shape[1], 2,  NUM_OF_CLASSES]) \n",
      "thetas = np.array([1.5] * lr.theta_len)\n",
      "print( \"Test feedforward:\\n\", lr.feedforward(X, thetas))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Test feedforward:\n",
        " [matrix([[-0.25210872, -0.20982247, -0.30640235, -0.2816132 ]]), matrix([[ 0.48127873,  0.48127873]]), matrix([[ 0.94997136,  0.94997136,  0.94997136,  0.94997136,  0.94997136,\n",
        "          0.94997136,  0.94997136,  0.94997136,  0.94997136]])]\n"
       ]
      }
     ],
     "prompt_number": 42
    },
    {
     "cell_type": "heading",
     "level": 6,
     "metadata": {},
     "source": [
      "Regularization"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Test regularization OK\n",
      "\n",
      "threshold = 1\n",
      "lenght = 4\n",
      "\n",
      "X = X_train[0:threshold, :lenght]\n",
      "Y = Y_train[0:threshold]\n",
      "lr = NeuralNetLearner([X.shape[1], 2,  NUM_OF_CLASSES]) \n",
      "lr.lambda_ = 1e-5\n",
      "thetas = np.array([1.5] * lr.theta_len)\n",
      "lr.l2_reg(thetas)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 322,
       "text": [
        "0.00029250000000000001"
       ]
      }
     ],
     "prompt_number": 322
    },
    {
     "cell_type": "heading",
     "level": 6,
     "metadata": {},
     "source": [
      "Cost function"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Test cost function OK\n",
      "\n",
      "threshold = 1\n",
      "lenght = 4\n",
      "\n",
      "X = X_train[0:threshold, :lenght]\n",
      "Y = Y_train[0:threshold]\n",
      "lr = NeuralNetLearner([X.shape[1], 2,  NUM_OF_CLASSES]) \n",
      "thetas = np.array([1.5] * lr.theta_len)\n",
      "lr.X = X\n",
      "lr.Y = Y\n",
      "print( \"Cost function:\", lr.J(thetas))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Cost function: 3.61395876626\n"
       ]
      }
     ],
     "prompt_number": 323
    },
    {
     "cell_type": "heading",
     "level": 6,
     "metadata": {},
     "source": [
      "Gradient"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Test Jgrad OK\n",
      "\n",
      "threshold = 1\n",
      "lenght = 4\n",
      "\n",
      "X = X_train[0:threshold, :lenght]\n",
      "Y = Y_train[0:threshold]\n",
      "lr = NeuralNetLearner([X.shape[1], 2,  NUM_OF_CLASSES]) \n",
      "thetas = np.array([1.5] * lr.theta_len)\n",
      "lr.X = X\n",
      "lr.Y = Y\n",
      "print( \"grad =\", lr.Jgrad(thetas))  # = -2.310293595541139e-12"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "grad = [-0.03372437 -0.03372437 -0.02804261 -0.02804261 -0.04101948 -0.04101948\n",
        " -0.03768871 -0.03768871  0.13436412  0.13436412  0.02187883  0.02187883\n",
        "  0.02187883  0.02187883  0.02187883 -0.00099431  0.02187883  0.02187883\n",
        "  0.02187883  0.02187883  0.02187883  0.02187883  0.02187883  0.02187883\n",
        " -0.00099431  0.02187883  0.02187883  0.02187883  0.04514813  0.04514813\n",
        "  0.04514813  0.04514813  0.04514813 -0.00237765  0.04514813  0.04514813\n",
        "  0.04514813]\n"
       ]
      }
     ],
     "prompt_number": 324
    },
    {
     "cell_type": "heading",
     "level": 6,
     "metadata": {},
     "source": [
      "Gradient aprox"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Test grad aprox OK\n",
      "\n",
      "threshold = 1\n",
      "lenght = 4\n",
      "\n",
      "X = X_train[0:threshold, :lenght]\n",
      "Y = Y_train[0:threshold]\n",
      "lr = NeuralNetLearner([X.shape[1], 2,  NUM_OF_CLASSES]) \n",
      "thetas = np.array([1.5] * lr.theta_len)\n",
      "lr.X = X\n",
      "lr.Y = Y\n",
      "print( \"grad =\\n\", grad_approx(lr.J, thetas))  # = -2.310293595541139e-12"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "grad =\n",
        " [-0.03372437 -0.03372437 -0.02804261 -0.02804261 -0.04101948 -0.04101948\n",
        " -0.03768871 -0.03768871  0.13436412  0.13436412  0.02187883 -0.00099431\n",
        "  0.02187883  0.02187883  0.02187883  0.02187883  0.02187883  0.02187883\n",
        "  0.02187883  0.02187883 -0.00099431  0.02187883  0.02187883  0.02187883\n",
        "  0.02187883  0.02187883  0.02187883  0.02187883  0.04514813 -0.00237765\n",
        "  0.04514813  0.04514813  0.04514813  0.04514813  0.04514813  0.04514813\n",
        "  0.04514813]\n"
       ]
      }
     ],
     "prompt_number": 71
    },
    {
     "cell_type": "heading",
     "level": 6,
     "metadata": {},
     "source": [
      "Gradient diffefence"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Difference OK 9.16699325959e-11\n",
      "\n",
      "threshold = 1\n",
      "lenght = 4\n",
      "\n",
      "X = X_train[0:threshold, :lenght]\n",
      "Y = Y_train[0:threshold]\n",
      "lr = NeuralNetLearner([X.shape[1], 2,  NUM_OF_CLASSES]) \n",
      "thetas = np.array([1.5] * lr.theta_len)\n",
      "lr.X = X\n",
      "lr.Y = Y\n",
      "print( \"Grad diff =\", np.sum( grad_approx(lr.J, thetas) - lr.Jgrad(thetas) ) )"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Grad diff = -6.25290364192e-10\n"
       ]
      }
     ],
     "prompt_number": 72
    },
    {
     "cell_type": "heading",
     "level": 6,
     "metadata": {},
     "source": [
      "Let's optimize a bit with matrix"
     ]
    },
    {
     "cell_type": "heading",
     "level": 6,
     "metadata": {},
     "source": [
      "Feed forward matrix"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Test feed forward matrix OK\n",
      "\n",
      "threshold = 1\n",
      "lenght = 4\n",
      "\n",
      "X = X_train[0:threshold, :lenght]\n",
      "Y = Y_train[0:threshold]\n",
      "lr = NeuralNetLearner([X.shape[1], 2,  NUM_OF_CLASSES]) \n",
      "thetas = np.array([1.5] * lr.theta_len)\n",
      "print( \"Test feedforward:\\n\", lr.feedforwardMatrix(X, thetas))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Test feedforward:\n",
        " [matrix([[-0.25210872, -0.20982247, -0.30640235, -0.2816132 ]]), matrix([[ 0.48127873,  0.48127873]]), matrix([[ 0.94997136,  0.94997136,  0.94997136,  0.94997136,  0.94997136,\n",
        "          0.94997136,  0.94997136,  0.94997136,  0.94997136]])]\n"
       ]
      }
     ],
     "prompt_number": 328
    },
    {
     "cell_type": "heading",
     "level": 6,
     "metadata": {},
     "source": [
      "Backpropagation matrix"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Test backprop matrix OK\n",
      "\n",
      "threshold = 1\n",
      "lenght = 4\n",
      "\n",
      "X = X_train[0:threshold, :lenght]\n",
      "Y = Y_train[0:threshold]\n",
      "lr = NeuralNetLearner([X.shape[1], 2,  NUM_OF_CLASSES]) \n",
      "thetas = np.array([1.5] * lr.theta_len)\n",
      "lr.X = X\n",
      "lr.Y = Y\n",
      "lr.backpropMatrix(X, Y, thetas)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 329,
       "text": [
        "([0,\n",
        "  matrix([[ 0.13436412,  0.13436412,  0.        ]]),\n",
        "  matrix([[ 0.04514813,  0.04514813,  0.04514813,  0.04514813,  0.04514813,\n",
        "           -0.00237765,  0.04514813,  0.04514813,  0.04514813]])],\n",
        " [matrix([[-0.25210872, -0.20982247, -0.30640235, -0.2816132 ]]),\n",
        "  matrix([[ 0.48127873,  0.48127873]]),\n",
        "  matrix([[ 0.94997136,  0.94997136,  0.94997136,  0.94997136,  0.94997136,\n",
        "            0.94997136,  0.94997136,  0.94997136,  0.94997136]])])"
       ]
      }
     ],
     "prompt_number": 329
    },
    {
     "cell_type": "heading",
     "level": 6,
     "metadata": {},
     "source": [
      "Gradient matrix"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Gradient matrix\n",
      "\n",
      "lr = NeuralNetLearner([X.shape[1], 2,  NUM_OF_CLASSES]) \n",
      "thetas = np.array([1.5] * lr.theta_len)\n",
      "lr.X = X\n",
      "lr.Y = Y\n",
      "lr.JgradMatrix(thetas)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 330,
       "text": [
        "array([-0.03372437, -0.03372437, -0.02804261, -0.02804261, -0.04101948,\n",
        "       -0.04101948, -0.03768871, -0.03768871,  0.13436412,  0.13436412,\n",
        "        0.02187883,  0.02187883,  0.02187883,  0.02187883,  0.02187883,\n",
        "       -0.00099431,  0.02187883,  0.02187883,  0.02187883,  0.02187883,\n",
        "        0.02187883,  0.02187883,  0.02187883,  0.02187883, -0.00099431,\n",
        "        0.02187883,  0.02187883,  0.02187883,  0.04514813,  0.04514813,\n",
        "        0.04514813,  0.04514813,  0.04514813, -0.00237765,  0.04514813,\n",
        "        0.04514813,  0.04514813])"
       ]
      }
     ],
     "prompt_number": 330
    },
    {
     "cell_type": "heading",
     "level": 6,
     "metadata": {},
     "source": [
      "Gradient difference"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Difference OK 3.37317745025e-10\n",
      "\n",
      "threshold = 1\n",
      "lenght = 4\n",
      "\n",
      "X = X_train[0:threshold, :lenght]\n",
      "Y = Y_train[0:threshold]\n",
      "lr = NeuralNetLearner([X.shape[1], 2,  NUM_OF_CLASSES]) \n",
      "thetas = np.array([1.5] * lr.theta_len)\n",
      "lr.X = X\n",
      "lr.Y = Y\n",
      "print( \"Grad diff =\", np.sum( grad_approx(lr.J, thetas) - lr.JgradMatrix(thetas) ) )"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Grad diff = -6.25290364192e-10\n"
       ]
      }
     ],
     "prompt_number": 73
    },
    {
     "cell_type": "heading",
     "level": 6,
     "metadata": {},
     "source": [
      "Dropout"
     ]
    },
    {
     "cell_type": "heading",
     "level": 6,
     "metadata": {},
     "source": [
      "Feed forward dropout"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Test feedforward matrix OK\n",
      "# Set dropout_prob to 0 to see if the implementation is OK\n",
      "\n",
      "threshold = 1\n",
      "lenght = 4\n",
      "dropout_prob = 0.5\n",
      "\n",
      "X = X_train[0:threshold, :lenght]\n",
      "Y = Y_train[0:threshold]\n",
      "lr = NeuralNetLearner([X.shape[1], 3,  NUM_OF_CLASSES]) \n",
      "lr.dropout_prob = dropout_prob\n",
      "thetas = np.array([1.5] * lr.theta_len)\n",
      "activations, dropmasks = lr.feedforwardMatrixDropout(X, thetas)\n",
      "thetas = np.array([1.5] * lr.theta_len)\n",
      "activations_without = lr.feedforwardMatrix(X, thetas)\n",
      "thetas = np.array([1.5] * lr.theta_len)\n",
      "activations, dropmasks = lr.feedforwardMatrixDropout(X, thetas, testTime=True)\n",
      "print( \"Test feedforward without dropout:\\n\\n\", activations_without)\n",
      "print( \"\\nTest feedforward with dropout:\\n\\n\", activations)\n",
      "print( \"\\nTest feedforward test(expectation):\\n\\n\", activations_without)\n",
      "print( \"\\nTest dropuot:\\n\", dropmasks)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 6,
     "metadata": {},
     "source": [
      "Backpropagation dropout"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Test backprop matrix OK\n",
      "# Set dropout_prob to 0 to see if the implementation is OK\n",
      "\n",
      "threshold = 1\n",
      "lenght = 4\n",
      "dropout_prob = 0.0\n",
      "\n",
      "X = X_train[0:threshold, :lenght]\n",
      "Y = Y_train[0:threshold]\n",
      "lr = NeuralNetLearner([X.shape[1], 2,  NUM_OF_CLASSES]) \n",
      "thetas = np.array([1.5] * lr.theta_len)\n",
      "lr.dropout_prob = dropout_prob\n",
      "lr.X = X\n",
      "lr.Y = Y\n",
      "\n",
      "backprop = lr.backpropMatrixDropout(X, Y, thetas)\n",
      "backprop_without = lr.backpropMatrix(X, Y, thetas)\n",
      "print( \"Test backprop without dropout:\\n\", backprop_without)\n",
      "print( \"Test backprop with dropout:\\n\", backprop)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Test backprop without dropout:\n",
        " ([0, matrix([[ 0.13436412,  0.13436412,  0.        ]]), matrix([[ 0.04514813,  0.04514813,  0.04514813,  0.04514813,  0.04514813,\n",
        "         -0.00237765,  0.04514813,  0.04514813,  0.04514813]])], [matrix([[-0.25210872, -0.20982247, -0.30640235, -0.2816132 ]]), matrix([[ 0.48127873,  0.48127873]]), matrix([[ 0.94997136,  0.94997136,  0.94997136,  0.94997136,  0.94997136,\n",
        "          0.94997136,  0.94997136,  0.94997136,  0.94997136]])])\n",
        "Test backprop with dropout:\n",
        " ([0, matrix([[ 0.13436412,  0.13436412,  0.        ]]), matrix([[ 0.04514813,  0.04514813,  0.04514813,  0.04514813,  0.04514813,\n",
        "         -0.00237765,  0.04514813,  0.04514813,  0.04514813]])], [matrix([[-0.25210872, -0.20982247, -0.30640235, -0.2816132 ]]), matrix([[ 0.48127873,  0.48127873]]), matrix([[ 0.94997136,  0.94997136,  0.94997136,  0.94997136,  0.94997136,\n",
        "          0.94997136,  0.94997136,  0.94997136,  0.94997136]])])\n"
       ]
      }
     ],
     "prompt_number": 333
    },
    {
     "cell_type": "heading",
     "level": 6,
     "metadata": {},
     "source": [
      "Gradient dropout"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Grad Dropout OK\n",
      "# Set dropout_prob to 0 to see if the implementation is OK\n",
      "\n",
      "threshold = 3\n",
      "lenght = 4\n",
      "dropout_prob = 0.5\n",
      "\n",
      "X = X_train[0:threshold, :lenght]\n",
      "Y = Y_train[0:threshold]\n",
      "lr = NeuralNetLearner([X.shape[1], 2,  NUM_OF_CLASSES]) \n",
      "lr.dropout_prob = dropout_prob\n",
      "lr.X = X\n",
      "lr.Y = Y\n",
      "thetas = np.array([1.5] * lr.theta_len)\n",
      "print(\"No dropout\", lr.JgradMatrix(thetas))\n",
      "thetas = np.array([1.5] * lr.theta_len)\n",
      "print(\"With dropout\", lr.JgradMatrixDropout(thetas))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "No dropout [-0.03372437 -0.03372437 -0.02804261 -0.02804261 -0.04101948 -0.04101948\n",
        " -0.03768871 -0.03768871  0.13436412  0.13436412  0.02187883  0.02187883\n",
        "  0.02187883  0.01425445  0.02187883  0.00663007  0.02187883  0.02187883\n",
        "  0.02187883  0.02187883  0.02187883  0.02187883  0.01425445  0.02187883\n",
        "  0.00663007  0.02187883  0.02187883  0.02187883  0.04514813  0.04514813\n",
        "  0.04514813  0.0293062   0.04514813  0.01346428  0.04514813  0.04514813\n",
        "  0.04514813]\n",
        "With dropout [-0.01114146 -0.01114146 -0.00924754 -0.00924754 -0.01357316 -0.01357316\n",
        " -0.0124629  -0.0124629   0.04478804  0.04478804  0.00739294  0.00739294\n",
        "  0.00739294 -0.00023144  0.00739294  0.00739294  0.00739294  0.00739294\n",
        "  0.00739294  0.00739294  0.00739294  0.00739294 -0.00023144  0.00739294\n",
        "  0.00739294  0.00739294  0.00739294  0.00739294  0.0963416   0.0963416\n",
        "  0.0963416   0.08049967  0.0963416  -0.00308937  0.0963416   0.0963416\n",
        "  0.0963416 ]\n"
       ]
      }
     ],
     "prompt_number": 334
    },
    {
     "cell_type": "heading",
     "level": 6,
     "metadata": {},
     "source": [
      "Criteria function dropout"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Test set - replace dropout with expectation\n",
      "\n",
      "threshold = 3\n",
      "lenght = 6\n",
      "dropout_prob = 0.5\n",
      "\n",
      "X = X_train[0:threshold, :lenght]\n",
      "Y = Y_train[0:threshold]\n",
      "lr = NeuralNetLearner([X.shape[1], 2,  NUM_OF_CLASSES]) \n",
      "lr.dropout_prob = dropout_prob\n",
      "lr.X = X\n",
      "lr.Y = Y\n",
      "lr.fit(X, Y)\n",
      "thetas = np.array([1.5] * lr.theta_len)\n",
      "#X = np.matrix([[1,1,1,1], [1,2,2,2], [1, 2, 4,4],[1,2,5,5]])\n",
      "lr.theta_shape\n",
      "lr.predict(X_test[:threshold,:lenght])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(3, 6) <class 'numpy.matrixlib.defmatrix.matrix'>\n",
        "Lbfgs iterations: 10\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 335,
       "text": [
        "matrix([[ 0.36719613,  0.52651242,  0.44200227,  0.4600304 ,  0.43231833,\n",
        "          0.54914059,  0.40675802,  0.39511308,  0.39994243],\n",
        "        [ 0.37274036,  0.53315543,  0.45567889,  0.46628444,  0.42849858,\n",
        "          0.55454141,  0.41251359,  0.39340691,  0.41457845],\n",
        "        [ 0.44740966,  0.54318013,  0.52491228,  0.50358981,  0.44484049,\n",
        "          0.55104418,  0.47113233,  0.43197782,  0.50586835]])"
       ]
      }
     ],
     "prompt_number": 335
    },
    {
     "cell_type": "heading",
     "level": 6,
     "metadata": {},
     "source": [
      "Gradient difference dropout"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Difference OK -6.08023376249e-10\n",
      "# Important to take a lot of hidden states that the random numer of dropped \n",
      "# units limitate to the expectation. At least 50 hidden units required.\n",
      "\n",
      "threshold = 1\n",
      "lenght = 4\n",
      "dropout_prob = 0.7\n",
      "\n",
      "X = X_train[0:threshold, :lenght]\n",
      "Y = Y_train[0:threshold]\n",
      "lr = NeuralNetLearner([X.shape[1], 100,  NUM_OF_CLASSES]) \n",
      "lr.dropout_prob = dropout_prob\n",
      "thetas = np.array([1.5] * lr.theta_len)\n",
      "lr.X = X\n",
      "lr.Y = Y\n",
      "aprox = grad_approx(lr.JDropout, thetas)\n",
      "thetas = np.array([1.5] * lr.theta_len)\n",
      "grad = lr.JgradMatrixDropout(thetas)\n",
      "print( \"Grad diff =\", np.sum( aprox - grad ) )"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Grad diff = -8.38989736688e-08\n"
       ]
      }
     ],
     "prompt_number": 74
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Cross Validation"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "WARNING We always split in the same way because of shuffle=False. \n",
      "All models are always evaluated on the same data in the same order. \n",
      "So, it is valid to compare different models between themselves.\n",
      "There are also prints to check result for each fold. We also use random seed to set our initial theta to the same values."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn import metrics\n",
      "\n",
      "def evaluate(pred, real):\n",
      "    return metrics.log_loss(real, pred)\n",
      "\n",
      "\n",
      "def my_cross_validation(X, Y, model, k=3):\n",
      "    S = []\n",
      "    kf = cross_validation.KFold(len(Y), n_folds=k, shuffle=False)\n",
      "    for train_index, test_index in kf:\n",
      "        print(\"New fold\")\n",
      "        start = time.time()  \n",
      "        # Split\n",
      "        X_train, X_test = X[train_index], X[test_index]\n",
      "        Y_train, Y_test = Y[train_index], Y[test_index]\n",
      "        \n",
      "        model.fit(X_train, Y_train)\n",
      "        predicted = model.predict(X_test)\n",
      "        # Evaluate\n",
      "        score = evaluate(predicted, Y_test)\n",
      "         \n",
      "        end = time.time()\n",
      "        \n",
      "        print(\"        Time:\", round(end - start, 2), \"s                                     ### Vmesni rezultat ###: \",  score)\n",
      "        S.append( score )\n",
      "    S = np.array(S)\n",
      "    return(np.mean(S))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 55
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Batch Stohastic"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "- Stohastic - set mini_batch_size to 1\n",
      "- Batch - set mini_batch_size to 0 or less or the size of the input samples\n",
      "- Batch Sequence - set mini_batch_size to some value"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class BatchSeq():\n",
      "    def __init__(self, learner, mini_batch_size=0, iterations=500, learning_rate=1, print_=True):\n",
      "        self.learner = learner\n",
      "        self.mini_batch_size = mini_batch_size if mini_batch_size > 0 else 0\n",
      "        self.iterations = iterations   # how many times it goes over the whole set\n",
      "        self.learning_rate = learning_rate\n",
      "        self.print_ = print_\n",
      "\n",
      "        \n",
      "    def fit_thetas(self, X, Y, thetas=None):\n",
      "        # SPLIT DATA\n",
      "        # num of samples = 1130 => 0, 100, ..., 900, 1000, 1130 (last block takes all remainded)\n",
      "        split = []\n",
      "        if X.shape[0] <= self.mini_batch_size or self.mini_batch_size <= 0:\n",
      "            split = [[0,X.shape[0]]]\n",
      "        else:\n",
      "            split = [ [lo, lo + self.mini_batch_size] for lo in range( 0, X.shape[0], self.mini_batch_size)]\n",
      "            split[-1][-1] = X.shape[0]\n",
      "\n",
      "        # STOHASTIC\n",
      "        i = 0\n",
      "        t0 = 1/self.learning_rate\n",
      "        for lo, hi in split:\n",
      "            self.learner.learning_rate = 1/(t0 + i)\n",
      "            thetas = self.learner.fit( np.matrix(X[lo:hi, :]), np.matrix(Y[lo:hi, :]), thetas=thetas)\n",
      "            i += 1\n",
      "            #if ( ((100*i+41) % (X.shape[0] + 10)) == 0 ):\n",
      "            #    print(\"Theta diff\", np.sum(np.abs(grad)))\n",
      "        return thetas\n",
      "    \n",
      "    def fit(self, X, Y):\n",
      "        \n",
      "        # stop criteria\n",
      "        eps = 1e-7\n",
      "        \n",
      "        # init thetas and grad. first step\n",
      "        np.random.seed(42)\n",
      "        thetas = self.learner.init_thetas()\n",
      "        \n",
      "        for i in range(self.iterations):\n",
      "            start = time.time()            \n",
      "            X, Y = shuffle(X, Y)\n",
      "            thetas = self.fit_thetas(X, Y, thetas)   \n",
      "            end = time.time()\n",
      "            if self.print_:\n",
      "                print(\"Iter:\", i, \" \\n        Time:\", round( end - start, 2) )    \n",
      "                \n",
      "    \n",
      "    def predict(self, X):\n",
      "        return self.learner.predict(X)\n",
      "    \n",
      "    def get_params(self, *args, **argss):\n",
      "        return {'learner':self.learner, 'mini_batch_size':self.mini_batch_size,\n",
      "                'iterations': self.iterations,\n",
      "                'print_':self.print_\n",
      "                }\n",
      "                \n",
      "        \n",
      "    def predict_proba(self, X):\n",
      "        return self.predict(X)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 43
    },
    {
     "cell_type": "heading",
     "level": 6,
     "metadata": {},
     "source": [
      "Let's try sklearn cross validation for the first time (for fun)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "threshold = 500\n",
      "hidden = [40]\n",
      "\n",
      "X = X_train[:threshold]\n",
      "Y = Y_train[:threshold]\n",
      "\n",
      "arch = [X.shape[1]] + hidden + [NUM_OF_CLASSES]\n",
      "lr = NeuralNetLearner(arch, lbfgs=False) \n",
      "scores = - cross_validation.cross_val_score(lr, X, Y, cv=3, scoring='log_loss')\n",
      "scores"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 29,
       "text": [
        "array([ 2.29367807,  2.69020061,  2.99772937])"
       ]
      }
     ],
     "prompt_number": 29
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Ultimate tester"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def tester(myCV=False, \n",
      "           threshold=10000, \n",
      "           hidden=[90, 30], \n",
      "           g=sigmoid, \n",
      "           g_derivative=sigmoid_derivative, \n",
      "           lambda_=1e-4, \n",
      "           descent='stohastic',\n",
      "           lbfgs_maxiter=3000, \n",
      "           learning_rate=1, \n",
      "           dropout_prob=0.0,            \n",
      "           mini_batch_size=500, \n",
      "           batch_iterations=500, \n",
      "           print_=True\n",
      "           ):\n",
      "    \n",
      "    \n",
      "    lbfgs = True if descent == 'lbfgs' else False\n",
      "    \n",
      "    X = X_train[:threshold]\n",
      "    Y = Y_train[:threshold]\n",
      "    arch = [X.shape[1]] + hidden + [NUM_OF_CLASSES]\n",
      "    \n",
      "    print(\"Architecture is: \", arch)\n",
      "\n",
      "    learner = NeuralNetLearner(arch, g=g, g_derivative=g_derivative, lambda_=lambda_, maxiter=lbfgs_maxiter, learning_rate=learning_rate, dropout_prob=dropout_prob, lbfgs=lbfgs, print_=print_)\n",
      "    bs = BatchSeq( learner, mini_batch_size=mini_batch_size, iterations=batch_iterations, print_=print_)\n",
      "    if myCV:\n",
      "        score = my_cross_validation(X, Y, bs)\n",
      "    else:\n",
      "        score = - cross_validation.cross_val_score(bs, X, Y, cv=3, scoring='log_loss')\n",
      "    return score"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 53
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Test the number of hidden layers"
     ]
    },
    {
     "cell_type": "heading",
     "level": 6,
     "metadata": {},
     "source": [
      "Hidden = 20"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "tester(myCV=True, \n",
      "       threshold=1000,  \n",
      "       hidden=[20],     # <--- \n",
      "       g=sigmoid, \n",
      "       g_derivative=sigmoid_derivative, \n",
      "       lambda_=1e-5, \n",
      "       descent='lbfgs',\n",
      "       lbfgs_maxiter=3000, \n",
      "       learning_rate=0.1, \n",
      "       dropout_prob=0.0, \n",
      "\n",
      "       mini_batch_size=-1, \n",
      "       batch_iterations=1, \n",
      "       )"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Architecture is:  [93, 20, 9]\n",
        "New fold\n",
        "Lbfgs iterations:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 199\n",
        "Iter: 0  \n",
        "        Time: 26.25 s    Theta diff: 1581.94106423\n",
        "                                             ### Vmesni rezultat ###:  1.4314778432\n",
        "New fold\n",
        "Lbfgs iterations:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 226\n",
        "Iter: 0  \n",
        "        Time: 29.43 s    Theta diff: 1600.42259743\n",
        "                                             ### Vmesni rezultat ###:  1.41634854288\n",
        "New fold\n",
        "Lbfgs iterations:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 95\n",
        "Iter: 0  \n",
        "        Time: 12.3 s    Theta diff: 1821.76359939\n",
        "                                             ### Vmesni rezultat ###:  2.6086232899\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 48,
       "text": [
        "1.818816558659585"
       ]
      }
     ],
     "prompt_number": 48
    },
    {
     "cell_type": "heading",
     "level": 6,
     "metadata": {},
     "source": [
      "Hidden = 20, 20"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "tester(myCV=True, \n",
      "       threshold=1000,  \n",
      "       hidden=[20, 20],     # <--- \n",
      "       g=sigmoid, \n",
      "       g_derivative=sigmoid_derivative, \n",
      "       lambda_=1e-5, \n",
      "       descent='lbfgs',\n",
      "       lbfgs_maxiter=3000, \n",
      "       learning_rate=0.1, \n",
      "       dropout_prob=0.0, \n",
      "\n",
      "       mini_batch_size=-1, \n",
      "       batch_iterations=1, \n",
      "       )"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Architecture is:  [93, 20, 20, 9]\n",
        "New fold\n",
        "Lbfgs iterations:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 250\n",
        "Iter: 0  \n",
        "        Time: 44.75 s    Theta diff: 1657.57107964\n",
        "                                             ### Vmesni rezultat ###:  1.49430000047\n",
        "New fold\n",
        "Lbfgs iterations:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 299\n",
        "Iter: 0  \n",
        "        Time: 53.45 s    Theta diff: 1732.87390614\n",
        "                                             ### Vmesni rezultat ###:  1.4776026904\n",
        "New fold\n",
        "Lbfgs iterations:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 221\n",
        "Iter: 0  \n",
        "        Time: 39.62 s    Theta diff: 1738.64509501\n",
        "                                             ### Vmesni rezultat ###: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 1.54231272067\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 49,
       "text": [
        "1.504738470513346"
       ]
      }
     ],
     "prompt_number": 49
    },
    {
     "cell_type": "heading",
     "level": 6,
     "metadata": {},
     "source": [
      "Hidden = 70"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "tester(myCV=True, \n",
      "       threshold=1000,  \n",
      "       hidden=[70],     # <--- \n",
      "       g=sigmoid, \n",
      "       g_derivative=sigmoid_derivative, \n",
      "       lambda_=1e-5, \n",
      "       descent='lbfgs',\n",
      "       lbfgs_maxiter=3000, \n",
      "       learning_rate=0.1, \n",
      "       dropout_prob=0.0, \n",
      "\n",
      "       mini_batch_size=-1, \n",
      "       batch_iterations=1, \n",
      "       )"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Architecture is:  [93, 70, 9]\n",
        "New fold\n",
        "[[0, 666]]\n",
        "    Batch: 0 666\n",
        "Lbfgs iterations:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 276\n",
        "Iter: 0  \n",
        "        Time: 54.74 s    Theta diff: 3666.37710506\n",
        "                                             ### Vmesni rezultat ###: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 1.17501981147\n",
        "New fold\n",
        "[[0, 667]]\n",
        "    Batch: 0 667\n",
        "Lbfgs iterations:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 273\n",
        "Iter: 0  \n",
        "        Time: 56.08 s    Theta diff: 3543.94614708\n",
        "                                             ### Vmesni rezultat ###: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 1.18314834631\n",
        "New fold\n",
        "[[0, 667]]\n",
        "    Batch: 0 667\n",
        "Lbfgs iterations:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 325\n",
        "Iter: 0  \n",
        "        Time: 63.86 s    Theta diff: 3572.79038288\n",
        "                                             ### Vmesni rezultat ###:  1.42766212945\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 42,
       "text": [
        "1.2619434290789313"
       ]
      }
     ],
     "prompt_number": 42
    },
    {
     "cell_type": "heading",
     "level": 6,
     "metadata": {},
     "source": [
      "Hidden = 70, 40"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "tester(myCV=True, \n",
      "       threshold=1000, \n",
      "       hidden=[70, 40],    # <--- \n",
      "       g=sigmoid, \n",
      "       g_derivative=sigmoid_derivative, \n",
      "       lambda_=1e-5, \n",
      "       descent='lbfgs',\n",
      "       lbfgs_maxiter=3000, \n",
      "       learning_rate=0.1, \n",
      "       dropout_prob=0.0, \n",
      "\n",
      "       mini_batch_size=-1, \n",
      "       batch_iterations=1, \n",
      "       )"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Architecture is:  [93, 70, 40, 9]\n",
        "New fold\n",
        "[[0, 666]]\n",
        "    Batch: 0 666\n",
        "Lbfgs iterations:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 360\n",
        "Iter: 0  \n",
        "        Time: 98.62 s    Theta diff: 4831.5156118\n",
        "                                             ### Vmesni rezultat ###: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 1.34535169891\n",
        "New fold\n",
        "[[0, 667]]\n",
        "    Batch: 0 667\n",
        "Lbfgs iterations:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 324\n",
        "Iter: 0  \n",
        "        Time: 91.8 s    Theta diff: 4884.36338023\n",
        "                                             ### Vmesni rezultat ###: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 1.30569606482\n",
        "New fold\n",
        "[[0, 667]]\n",
        "    Batch: 0 667\n",
        "Lbfgs iterations:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 400\n",
        "Iter: 0  \n",
        "        Time: 113.03 s    Theta diff: 4618.26322244\n",
        "                                             ### Vmesni rezultat ###: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 1.43608962024\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 43,
       "text": [
        "1.3623791279902626"
       ]
      }
     ],
     "prompt_number": 43
    },
    {
     "cell_type": "heading",
     "level": 6,
     "metadata": {},
     "source": [
      "Hidden = 70, 70"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "tester(myCV=True, \n",
      "       threshold=1000, \n",
      "       hidden=[70, 70],    # <--- \n",
      "       g=sigmoid, \n",
      "       g_derivative=sigmoid_derivative, \n",
      "       lambda_=1e-5, \n",
      "       descent='lbfgs',\n",
      "       lbfgs_maxiter=3000, \n",
      "       learning_rate=0.1, \n",
      "       dropout_prob=0.0, \n",
      "\n",
      "       mini_batch_size=-1, \n",
      "       batch_iterations=1, \n",
      "       )"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Architecture is:  [93, 70, 70, 9]\n",
        "New fold\n",
        "Lbfgs iterations:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 391\n",
        "Iter: 0  \n",
        "        Time: 121.32 s    Theta diff: 5246.78147081\n",
        "                                             ### Vmesni rezultat ###: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 1.34483074344\n",
        "New fold\n",
        "Lbfgs iterations:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 395\n",
        "Iter: 0  \n",
        "        Time: 118.3 s    Theta diff: 6012.49301482\n",
        "                                             ### Vmesni rezultat ###: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 1.43838501671\n",
        "New fold\n",
        "Lbfgs iterations:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 127\n",
        "Iter: 0  \n",
        "        Time: 37.59 s    Theta diff: 3267.12502263\n",
        "                                             ### Vmesni rezultat ###: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 3.0267586572\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 47,
       "text": [
        "1.9366581391183015"
       ]
      }
     ],
     "prompt_number": 47
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Stohastic approach lbfgs (Subset of 1000 samples)"
     ]
    },
    {
     "cell_type": "heading",
     "level": 6,
     "metadata": {},
     "source": [
      "Stohastic lbfgs, one iteration"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "tester(myCV=True, \n",
      "       threshold=1000, \n",
      "       hidden=[70], \n",
      "       g=sigmoid, \n",
      "       g_derivative=sigmoid_derivative, \n",
      "       lambda_=1e-5, \n",
      "       descent='lbfgs',\n",
      "       lbfgs_maxiter=3000, \n",
      "       learning_rate=0.1, \n",
      "       dropout_prob=0.0, \n",
      "\n",
      "       mini_batch_size=1,         # <--- \n",
      "       batch_iterations=1,        # <--- \n",
      "       print_=False\n",
      "       )"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Architecture is:  [93, 70, 9]\n",
        "New fold\n",
        "Theta diff"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.195000041456\n",
        "Theta diff"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.195000041456\n",
        "(334, 9)\n",
        "                                             ### Vmesni rezultat ###:  8.40825957397\n",
        "New fold\n",
        "Theta diff"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.195000041456\n",
        "(333, 9)\n",
        "                                             ### Vmesni rezultat ###:  7.82775269986\n",
        "New fold\n",
        "Theta diff"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.195000041456\n",
        "(333, 9)\n",
        "                                             ### Vmesni rezultat ###:  7.90431231697\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 353,
       "text": [
        "8.0467748636015681"
       ]
      }
     ],
     "prompt_number": 353
    },
    {
     "cell_type": "heading",
     "level": 6,
     "metadata": {},
     "source": [
      "Stohastic lbfgs, 10 iterations"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "tester(myCV=True, \n",
      "       threshold=1000, \n",
      "       hidden=[20, 20], \n",
      "       g=sigmoid, \n",
      "       g_derivative=sigmoid_derivative, \n",
      "       lambda_=1e-5, \n",
      "       descent='lbfgs',          # <--- \n",
      "       lbfgs_maxiter=5, \n",
      "       learning_rate=0.1,    \n",
      "       dropout_prob=0.0, \n",
      "\n",
      "       mini_batch_size=1,        # <--- \n",
      "       batch_iterations=10,      # <---\n",
      "       print_=False\n",
      "       )"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Architecture is:  [93, 20, 20, 9]\n",
        "New fold\n",
        "                                             ### Vmesni rezultat ###: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 2.19722457664\n",
        "New fold\n",
        "                                             ### Vmesni rezultat ###: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 2.19722456672\n",
        "New fold\n",
        "                                             ### Vmesni rezultat ###: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 2.19722460247\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 64,
       "text": [
        "2.197224581944166"
       ]
      }
     ],
     "prompt_number": 64
    },
    {
     "cell_type": "heading",
     "level": 6,
     "metadata": {},
     "source": [
      "Three iterations through whole set"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "tester(myCV=True, \n",
      "       threshold=1000, \n",
      "       hidden=[70], \n",
      "       g=sigmoid, \n",
      "       g_derivative=sigmoid_derivative, \n",
      "       lambda_=1e-5, \n",
      "       descent='lbfgs',\n",
      "       lbfgs_maxiter=3000, \n",
      "       learning_rate=0.1, \n",
      "       dropout_prob=0.0, \n",
      "\n",
      "       mini_batch_size=1,           # <--- \n",
      "       batch_iterations=3,          # <--- \n",
      "       print_=False\n",
      "       )"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Architecture is:  [93, 70, 9]\n",
        "New fold\n",
        "                                             ### Vmesni rezultat ###: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 17.300069614\n",
        "New fold\n",
        "                                             ### Vmesni rezultat ###: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 8.52107836144\n",
        "New fold\n",
        "                                             ### Vmesni rezultat ###: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 2.19722469963\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 30,
       "text": [
        "9.3394575583580757"
       ]
      }
     ],
     "prompt_number": 30
    },
    {
     "cell_type": "heading",
     "level": 6,
     "metadata": {},
     "source": [
      "Mini batch 100 samples, 3 iterations through whole set"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "tester(myCV=True, \n",
      "       threshold=1000, \n",
      "       hidden=[70], \n",
      "       g=sigmoid, \n",
      "       g_derivative=sigmoid_derivative, \n",
      "       lambda_=1e-5, \n",
      "       descent='lbfgs',\n",
      "       lbfgs_maxiter=3000, \n",
      "       learning_rate=0.1, \n",
      "       dropout_prob=0.0, \n",
      "\n",
      "       mini_batch_size=100,         # <===\n",
      "       batch_iterations=3,          # <===\n",
      "       print_=False\n",
      "       )"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Architecture is:  [93, 70, 9]\n",
        "New fold\n",
        "                                             ### Vmesni rezultat ###: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 2.23071676124\n",
        "New fold\n",
        "                                             ### Vmesni rezultat ###: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 2.18349450341\n",
        "New fold\n",
        "                                             ### Vmesni rezultat ###: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 2.26119434355\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 26,
       "text": [
        "2.2251352027366482"
       ]
      }
     ],
     "prompt_number": 26
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Check out our gradient descent with updating learning rate"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We are using whole set. It is just for testing how good decreasing learning rate is. "
     ]
    },
    {
     "cell_type": "heading",
     "level": 6,
     "metadata": {},
     "source": [
      "400 iterations"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Gradient descent on whole set\n",
      "tester(myCV=True, \n",
      "       threshold=1000, \n",
      "       hidden=[70], \n",
      "       g=sigmoid, \n",
      "       g_derivative=sigmoid_derivative, \n",
      "       lambda_=1e-5, \n",
      "       descent='stohastic',         \n",
      "       lbfgs_maxiter=3000, \n",
      "       learning_rate=1,     \n",
      "       dropout_prob=0.0, \n",
      "\n",
      "       mini_batch_size=-1,    \n",
      "       batch_iterations=400,  # <---\n",
      "       print_=False\n",
      "       )"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Architecture is:  [93, 70, 9]\n",
        "New fold\n",
        "                                             ### Vmesni rezultat ###: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 1.29768075204\n",
        "New fold\n",
        "                                             ### Vmesni rezultat ###: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 1.40966304723\n",
        "New fold\n",
        "                                             ### Vmesni rezultat ###: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 1.51584462224\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 362,
       "text": [
        "1.4077294738360318"
       ]
      }
     ],
     "prompt_number": 362
    },
    {
     "cell_type": "heading",
     "level": 6,
     "metadata": {},
     "source": [
      "800 iterations"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Gradient descent on whole set\n",
      "tester(myCV=True, \n",
      "       threshold=1000, \n",
      "       hidden=[70], \n",
      "       g=sigmoid, \n",
      "       g_derivative=sigmoid_derivative, \n",
      "       lambda_=1e-5, \n",
      "       descent='stohastic',         \n",
      "       lbfgs_maxiter=3000, \n",
      "       learning_rate=1,     \n",
      "       dropout_prob=0.0, \n",
      "\n",
      "       mini_batch_size=-1,    \n",
      "       batch_iterations=800,  # <---\n",
      "       print_=False\n",
      "       )"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Architecture is:  [93, 70, 9]\n",
        "New fold\n",
        "                                             ### Vmesni rezultat ###: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 1.20163779409\n",
        "New fold\n",
        "                                             ### Vmesni rezultat ###: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 1.29785504836\n",
        "New fold\n",
        "                                             ### Vmesni rezultat ###: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 1.23345288184\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 363,
       "text": [
        "1.2443152414307896"
       ]
      }
     ],
     "prompt_number": 363
    },
    {
     "cell_type": "heading",
     "level": 6,
     "metadata": {},
     "source": [
      "1600 iterations"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "tester(myCV=True, \n",
      "       threshold=1000, \n",
      "       hidden=[70], \n",
      "       g=sigmoid, \n",
      "       g_derivative=sigmoid_derivative, \n",
      "       lambda_=1e-5, \n",
      "       descent='stohastic',         \n",
      "       lbfgs_maxiter=3000, \n",
      "       learning_rate=1,     \n",
      "       dropout_prob=0.0, \n",
      "\n",
      "       mini_batch_size=-1,    \n",
      "       batch_iterations=1600,  # <---\n",
      "       print_=False\n",
      "       )"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Architecture is:  [93, 70, 9]\n",
        "New fold\n",
        "                                             ### Vmesni rezultat ###: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 1.14402420923\n",
        "New fold\n",
        "                                             ### Vmesni rezultat ###: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 1.06158566128\n",
        "New fold\n",
        "                                             ### Vmesni rezultat ###: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 1.18530682853\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 364,
       "text": [
        "1.130305566345021"
       ]
      }
     ],
     "prompt_number": 364
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "STOHASTIC"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "It takes a sample and move in a direction of the gradient for a size of learning rate. \n",
      "Then it goes for the next sample. After we get to the end, we repeat a whole process from the beginning.\n",
      "It stops when the number of iterations reaches batch_iterations."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Checking number of iterations for stohastic descent"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Suggestions are 1 000 000 / number of samples in some articles."
     ]
    },
    {
     "cell_type": "heading",
     "level": 6,
     "metadata": {},
     "source": [
      "100 iterations"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Stohastic\n",
      "\n",
      "tester(myCV=True, \n",
      "       threshold=1000, \n",
      "       hidden=[50, 30], \n",
      "       g=sigmoid, \n",
      "       g_derivative=sigmoid_derivative, \n",
      "       lambda_=1e-5, \n",
      "       descent='stohastic',        \n",
      "       lbfgs_maxiter=3000, \n",
      "       learning_rate=1,      \n",
      "       dropout_prob=0.0, \n",
      "\n",
      "       mini_batch_size=1,        \n",
      "       batch_iterations=100,  # <---\n",
      "       print_=False\n",
      "       )"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Architecture is:  [93, 50, 30, 9]\n",
        "New fold\n",
        "        Time:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 71.69519710540771 s                                     ### Vmesni rezultat ###:  1.27698428421\n",
        "New fold\n",
        "        Time:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 69.77290296554565 s                                     ### Vmesni rezultat ###:  0.958476459511\n",
        "New fold\n",
        "        Time:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 67.4793610572815 s                                     ### Vmesni rezultat ###:  1.03365170777\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 26,
       "text": [
        "1.0897041504979317"
       ]
      }
     ],
     "prompt_number": 26
    },
    {
     "cell_type": "heading",
     "level": 6,
     "metadata": {},
     "source": [
      "200 iterations"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Stohastic\n",
      "\n",
      "tester(myCV=True, \n",
      "       threshold=1000, \n",
      "       hidden=[50, 30], \n",
      "       g=sigmoid, \n",
      "       g_derivative=sigmoid_derivative, \n",
      "       lambda_=1e-5, \n",
      "       descent='stohastic',        \n",
      "       lbfgs_maxiter=3000, \n",
      "       learning_rate=1,      \n",
      "       dropout_prob=0.0, \n",
      "\n",
      "       mini_batch_size=1,        \n",
      "       batch_iterations=200,  # <---\n",
      "       print_=False\n",
      "       )"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Architecture is:  [93, 50, 30, 9]\n",
        "New fold\n",
        "                                             ### Vmesni rezultat ###: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 1.05067121054\n",
        "New fold\n",
        "                                             ### Vmesni rezultat ###: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.878067844731\n",
        "New fold\n",
        "                                             ### Vmesni rezultat ###: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 1.01309188848\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 24,
       "text": [
        "0.98061031458322623"
       ]
      }
     ],
     "prompt_number": 24
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Dropout"
     ]
    },
    {
     "cell_type": "heading",
     "level": 6,
     "metadata": {},
     "source": [
      "Prob 0.25"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Stohastic\n",
      "\n",
      "tester(myCV=True, \n",
      "       threshold=1000, \n",
      "       hidden=[50, 30], \n",
      "       g=sigmoid, \n",
      "       g_derivative=sigmoid_derivative, \n",
      "       lambda_=1e-5, \n",
      "       descent='stohastic',        \n",
      "       lbfgs_maxiter=3000, \n",
      "       learning_rate=1,      \n",
      "       dropout_prob=0.25,     # <---\n",
      "\n",
      "       mini_batch_size=1,        \n",
      "       batch_iterations=200, \n",
      "       print_=False\n",
      "       )"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Architecture is:  [93, 50, 30, 9]\n",
        "New fold\n",
        "        Time:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 130.81793594360352 s                                     ### Vmesni rezultat ###:  1.3901421925\n",
        "New fold\n",
        "        Time:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 129.96436500549316 s                                     ### Vmesni rezultat ###:  1.03348860076\n",
        "New fold\n",
        "        Time:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 130.78591799736023 s                                     ### Vmesni rezultat ###:  1.09519816698\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 27,
       "text": [
        "1.1729429867477401"
       ]
      }
     ],
     "prompt_number": 27
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Stohastic\n",
      "\n",
      "tester(myCV=True, \n",
      "       threshold=1000, \n",
      "       hidden=[50, 30], \n",
      "       g=sigmoid, \n",
      "       g_derivative=sigmoid_derivative, \n",
      "       lambda_=1e-5, \n",
      "       descent='stohastic',        \n",
      "       lbfgs_maxiter=3000, \n",
      "       learning_rate=1,      \n",
      "       dropout_prob=0.25,     # <---\n",
      "\n",
      "       mini_batch_size=1,        \n",
      "       batch_iterations=400, \n",
      "       print_=False\n",
      "       )"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Architecture is:  [93, 50, 30, 9]\n",
        "New fold\n",
        "        Time:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 263.97 s                                     ### Vmesni rezultat ###:  1.06101658536\n",
        "New fold\n",
        "        Time:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 265.18 s                                     ### Vmesni rezultat ###:  0.894771819326\n",
        "New fold\n",
        "        Time:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 264.22 s                                     ### Vmesni rezultat ###:  1.06827952371\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 337,
       "text": [
        "1.0080226427986183"
       ]
      }
     ],
     "prompt_number": 337
    },
    {
     "cell_type": "heading",
     "level": 6,
     "metadata": {},
     "source": [
      "Prob 0.5"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Stohastic\n",
      "\n",
      "tester(myCV=True, \n",
      "       threshold=1000, \n",
      "       hidden=[50, 30], \n",
      "       g=sigmoid, \n",
      "       g_derivative=sigmoid_derivative, \n",
      "       lambda_=1e-5, \n",
      "       descent='stohastic',        \n",
      "       lbfgs_maxiter=3000, \n",
      "       learning_rate=1,      \n",
      "       dropout_prob=0.5,       # <---\n",
      "\n",
      "       mini_batch_size=1,        \n",
      "       batch_iterations=200, \n",
      "       print_=False\n",
      "       )"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Architecture is:  [93, 50, 30, 9]\n",
        "New fold\n",
        "        Time:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 136.59 s                                     ### Vmesni rezultat ###:  1.69229496031\n",
        "New fold\n",
        "        Time:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 140.11 s                                     ### Vmesni rezultat ###:  1.45886206954\n",
        "New fold\n",
        "        Time:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 135.21 s                                     ### Vmesni rezultat ###:  1.45298629729\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 30,
       "text": [
        "1.5347144423789825"
       ]
      }
     ],
     "prompt_number": 30
    },
    {
     "cell_type": "heading",
     "level": 6,
     "metadata": {},
     "source": [
      "Increase number of neurones"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let us take 100, 60 neurones and 50% dropout. Expected value is 50,30 neurones which is something like above."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "tester(myCV=True, \n",
      "       threshold=1000, \n",
      "       hidden=[100, 60],       # <---\n",
      "       g=sigmoid, \n",
      "       g_derivative=sigmoid_derivative, \n",
      "       lambda_=1e-5, \n",
      "       descent='stohastic',        \n",
      "       lbfgs_maxiter=3000, \n",
      "       learning_rate=1,      \n",
      "       dropout_prob=0.5, \n",
      "\n",
      "       mini_batch_size=1,        \n",
      "       batch_iterations=200,  \n",
      "       print_=False\n",
      "       )"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Architecture is:  [93, 100, 60, 9]\n",
        "New fold\n",
        "        Time:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 167.91 s                                     ### Vmesni rezultat ###:  1.46354977168\n",
        "New fold\n",
        "        Time:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 164.95 s                                     ### Vmesni rezultat ###:  1.14836269294\n",
        "New fold\n",
        "        Time:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 164.54 s                                     ### Vmesni rezultat ###:  1.20180857585\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 32,
       "text": [
        "1.2712403468246312"
       ]
      }
     ],
     "prompt_number": 32
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Batch Sequential"
     ]
    },
    {
     "cell_type": "heading",
     "level": 6,
     "metadata": {},
     "source": [
      "100 mini batch"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "tester(myCV=True, \n",
      "       threshold=1000, \n",
      "       hidden=[100, 60], \n",
      "       g=sigmoid, \n",
      "       g_derivative=sigmoid_derivative, \n",
      "       lambda_=1e-5, \n",
      "       descent='stohastic',        \n",
      "       lbfgs_maxiter=3000, \n",
      "       learning_rate=1,      \n",
      "       dropout_prob=0.25, \n",
      "\n",
      "       mini_batch_size=100,    # <---     \n",
      "       batch_iterations=200, \n",
      "       print_=False\n",
      "       )"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Architecture is:  [93, 100, 60, 9]\n",
        "New fold\n",
        "        Time:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 65.79 s                                     ### Vmesni rezultat ###:  1.8066653076\n",
        "New fold\n",
        "        Time:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 64.07 s                                     ### Vmesni rezultat ###:  1.46019669714\n",
        "New fold\n",
        "        Time:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 60.05 s                                     ### Vmesni rezultat ###:  1.49462480059\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 33,
       "text": [
        "1.5871622684409556"
       ]
      }
     ],
     "prompt_number": 33
    },
    {
     "cell_type": "heading",
     "level": 6,
     "metadata": {},
     "source": [
      "400 iterations"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "tester(myCV=True, \n",
      "       threshold=1000, \n",
      "       hidden=[100, 60], \n",
      "       g=sigmoid, \n",
      "       g_derivative=sigmoid_derivative, \n",
      "       lambda_=1e-5, \n",
      "       descent='stohastic',        \n",
      "       lbfgs_maxiter=3000, \n",
      "       learning_rate=1,      \n",
      "       dropout_prob=0.25, \n",
      "\n",
      "       mini_batch_size=100,    # <---     \n",
      "       batch_iterations=400, \n",
      "       print_=False\n",
      "       )"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Architecture is:  [93, 100, 60, 9]\n",
        "New fold\n",
        "        Time:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 129.14 s                                     ### Vmesni rezultat ###:  1.62554362707\n",
        "New fold\n",
        "        Time:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 122.0 s                                     ### Vmesni rezultat ###:  1.21055440977\n",
        "New fold\n",
        "        Time:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 125.83 s                                     ### Vmesni rezultat ###:  1.29597345088\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 34,
       "text": [
        "1.377357162571178"
       ]
      }
     ],
     "prompt_number": 34
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Activations Functions"
     ]
    },
    {
     "cell_type": "heading",
     "level": 6,
     "metadata": {},
     "source": [
      "Tansigmoid (Working bad)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "tester(myCV=True, \n",
      "       threshold=1000, \n",
      "       hidden=[100, 60], \n",
      "       g=tansigmoid,                               # <---  \n",
      "       g_derivative=tansigmoid_derivative,         # <---  \n",
      "       lambda_=1e-5, \n",
      "       descent='lbfgs',        \n",
      "       lbfgs_maxiter=3000, \n",
      "       learning_rate=1,      \n",
      "       dropout_prob=0.25, \n",
      "\n",
      "       mini_batch_size=-1,       \n",
      "       batch_iterations=1, \n",
      "       print_=False\n",
      "       )"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Architecture is:  [93, 100, 60, 9]\n",
        "New fold\n",
        "        Time:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 12.11 s                                     ### Vmesni rezultat ###:  4.5995135058\n",
        "New fold\n",
        "        Time:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 12.33 s                                     ### Vmesni rezultat ###:  3.12662198074\n",
        "New fold\n",
        "        Time:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 11.8 s                                     ### Vmesni rezultat ###:  3.82642410643\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 37,
       "text": [
        "3.8508531976585303"
       ]
      }
     ],
     "prompt_number": 37
    },
    {
     "cell_type": "heading",
     "level": 6,
     "metadata": {},
     "source": [
      "My composed function arctan works surprisingly well"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "More iterations would improve the result."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Stohastic\n",
      "\n",
      "tester(myCV=True, \n",
      "       threshold=1000, \n",
      "       hidden=[50, 30], \n",
      "       g=arctan, \n",
      "       g_derivative=arctan_derivative, \n",
      "       lambda_=1e-5, \n",
      "       descent='stohastic',        \n",
      "       lbfgs_maxiter=3000, \n",
      "       learning_rate=1,      \n",
      "       dropout_prob=0.0, \n",
      "\n",
      "       mini_batch_size=1,        \n",
      "       batch_iterations=200,  # <---  \n",
      "       print_=False\n",
      "       )"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Architecture is:  [93, 50, 30, 9]\n",
        "New fold\n",
        "        Time:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 186.47 s                                     ### Vmesni rezultat ###:  1.09692777497\n",
        "New fold\n",
        "        Time:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 170.26 s                                     ### Vmesni rezultat ###:  1.14288673811\n",
        "New fold\n",
        "        Time:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 177.59 s                                     ### Vmesni rezultat ###:  1.14547504491\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 325,
       "text": [
        "1.1284298526616785"
       ]
      }
     ],
     "prompt_number": 325
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "STACKING (TODO ... fix dimensions)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def my_cross_validation_models(X, y, models, folds=5):\n",
      "   \n",
      "    # y combined with folds\n",
      "    y_real = np.array([])\n",
      "    # prediction, combined with fold for each model\n",
      "    Y_pred = []\n",
      "    for model in models:\n",
      "        Y_pred += [np.array([])]\n",
      "        \n",
      "    k = folds\n",
      "    S = 0\n",
      "    kf = cross_validation.KFold(len(y), n_folds=k, shuffle=True)\n",
      "    for train_index, test_index in kf:\n",
      "        # Split\n",
      "        X_train, X_test = X[train_index], X[test_index]\n",
      "        Y_train, Y_test = y[train_index], y[test_index]\n",
      "\n",
      "        y_real = np.hstack((y_real, Y_test))\n",
      "\n",
      "        # Predict\n",
      "        different_model_predicitons = []\n",
      "        for i in range( len(models) ):\n",
      "            model = models[i]\n",
      "            model.fit(X_train, Y_train)\n",
      "            P = model.predict(X_test)\n",
      "            Y_pred[i] = np.hstack( (Y_pred[i], P) )\n",
      "    # y and Y_pred is build \n",
      "    \n",
      "    return y_real, np.array(np.matrix(Y_pred).T)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 327
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class Stacking():\n",
      "    def __init__(self, models, aggreg_model=None):\n",
      "        self.models = models\n",
      "        self.aggregation = aggreg_model\n",
      "\n",
      "    def fit(self, X, y):\n",
      "        # Get aggregation function\n",
      "        real, y_pred = my_cross_validation_models(X, y, self.models)\n",
      "\n",
      "        # Combine models with reggression\n",
      "        if self.aggregation != None:\n",
      "            self.aggregation.fit(y_pred, real)\n",
      "        \n",
      "        # Nauci modele\n",
      "        for model in self.models:\n",
      "            model.fit(X, y)\n",
      "            \n",
      "    def predict(self, X):\n",
      "        predict_list = []\n",
      "        for model in self.models:\n",
      "            P = model.predict(X)\n",
      "            predict_list += [P]\n",
      "        Ys = np.array(np.matrix(predict_list).T)\n",
      "        if self.aggregation == None:\n",
      "            return avg_predictions(Ys)\n",
      "        return self.aggregation.predict(Ys)\n",
      "        "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 328
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn import linear_model"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 336
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "arch = [X.shape[1], 20, 20,NUM_OF_CLASSES]\n",
      "learner1 = NeuralNetLearner(arch, g=sigmoid, g_derivative=sigmoid_derivative, lambda_=1e-4, maxiter=10000, learning_rate=1, dropout_prob=0.2, lbfgs=True, print_=False)\n",
      "bs1 = BatchSeq( learner1, mini_batch_size=1000, iterations=1, print_=False)\n",
      "\n",
      "arch2 = [X.shape[1], 20, 20,NUM_OF_CLASSES]\n",
      "learner2 = NeuralNetLearner(arch2, g=sigmoid, g_derivative=sigmoid_derivative, lambda_=1e-4, maxiter=10000, learning_rate=1, dropout_prob=0.2, lbfgs=False, print_=False)\n",
      "bs2 = BatchSeq( learner2, mini_batch_size=1, iterations=200, print_=False)\n",
      "\n",
      "models = [bs1, bs2]\n",
      "agreg = linear_model.Ridge()\n",
      "stck = Stacking(models, agreg)\n",
      "stck.fit(X, Y)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Stohastic\n",
      "\n",
      "tester(myCV=True, \n",
      "       threshold=50000, \n",
      "       hidden=[20, 20], \n",
      "       g=sigmoid, \n",
      "       g_derivative=sigmoid_derivative, \n",
      "       lambda_=1e-5, \n",
      "       descent='lbfgs',        \n",
      "       lbfgs_maxiter=300000, \n",
      "       learning_rate=1,      \n",
      "       dropout_prob=0.2, \n",
      "\n",
      "       mini_batch_size=0,        \n",
      "       batch_iterations=1,  # <---  \n",
      "       print_=False\n",
      "       )"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Architecture is:  [93, 20, 20, 9]\n",
        "New fold"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "        Time:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 488.01 s                                     ### Vmesni rezultat ###:  2.00923267484\n",
        "New fold\n"
       ]
      },
      {
       "ename": "KeyboardInterrupt",
       "evalue": "",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
        "\u001b[1;32m<ipython-input-56-d76a6dba90c0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     14\u001b[0m        \u001b[0mmini_batch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m        \u001b[0mbatch_iterations\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;31m# <---\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m        \u001b[0mprint_\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m        )\n",
        "\u001b[1;32m<ipython-input-53-0f991881e760>\u001b[0m in \u001b[0;36mtester\u001b[1;34m(myCV, threshold, hidden, g, g_derivative, lambda_, descent, lbfgs_maxiter, learning_rate, dropout_prob, mini_batch_size, batch_iterations, print_)\u001b[0m\n\u001b[0;32m     26\u001b[0m     \u001b[0mbs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBatchSeq\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0mlearner\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmini_batch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmini_batch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0miterations\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_iterations\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprint_\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mprint_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mmyCV\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m         \u001b[0mscore\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmy_cross_validation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m         \u001b[0mscore\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mcross_validation\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcross_val_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'log_loss'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m<ipython-input-55-9890ba408617>\u001b[0m in \u001b[0;36mmy_cross_validation\u001b[1;34m(X, Y, model, k)\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[0mY_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtrain_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtest_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m         \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m         \u001b[0mpredicted\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[1;31m# Evaluate\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m<ipython-input-43-717ec5c26626>\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, Y)\u001b[0m\n\u001b[0;32m     41\u001b[0m             \u001b[0mstart\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m             \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 43\u001b[1;33m             \u001b[0mthetas\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_thetas\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mthetas\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     44\u001b[0m             \u001b[0mend\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprint_\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m<ipython-input-43-717ec5c26626>\u001b[0m in \u001b[0;36mfit_thetas\u001b[1;34m(self, X, Y, thetas)\u001b[0m\n\u001b[0;32m     23\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mlo\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msplit\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearner\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearning_rate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt0\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m             \u001b[0mthetas\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearner\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlo\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mhi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mY\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlo\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mhi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mthetas\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mthetas\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m             \u001b[0mi\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m             \u001b[1;31m#if ( ((100*i+41) % (X.shape[0] + 10)) == 0 ):\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m<ipython-input-41-21f505285fac>\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, Y, thetas)\u001b[0m\n\u001b[0;32m    266\u001b[0m                                           \u001b[0mx0\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mthetas\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    267\u001b[0m                                            \u001b[0mfprime\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mJgradMatrixDropout\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 268\u001b[1;33m                                            \u001b[0mmaxiter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmaxiter\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    269\u001b[0m                                            \u001b[1;31m#factr=10\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    270\u001b[0m                                            )   \n",
        "\u001b[1;32mC:\\Python34\\lib\\site-packages\\scipy\\optimize\\lbfgsb.py\u001b[0m in \u001b[0;36mfmin_l_bfgs_b\u001b[1;34m(func, x0, fprime, args, approx_grad, bounds, m, factr, pgtol, epsilon, iprint, maxfun, maxiter, disp, callback)\u001b[0m\n\u001b[0;32m    185\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    186\u001b[0m     res = _minimize_lbfgsb(fun, x0, args=args, jac=jac, bounds=bounds,\n\u001b[1;32m--> 187\u001b[1;33m                            **opts)\n\u001b[0m\u001b[0;32m    188\u001b[0m     d = {'grad': res['jac'],\n\u001b[0;32m    189\u001b[0m          \u001b[1;34m'task'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'message'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32mC:\\Python34\\lib\\site-packages\\scipy\\optimize\\lbfgsb.py\u001b[0m in \u001b[0;36m_minimize_lbfgsb\u001b[1;34m(fun, x0, args, jac, bounds, disp, maxcor, ftol, gtol, eps, maxfun, maxiter, iprint, callback, **unknown_options)\u001b[0m\n\u001b[0;32m    313\u001b[0m                 \u001b[1;31m# minimization routine wants f and g at the current x\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    314\u001b[0m                 \u001b[1;31m# Overwrite f and g:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 315\u001b[1;33m                 \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc_and_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    316\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mtask_str\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mb'NEW_X'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    317\u001b[0m             \u001b[1;31m# new iteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32mC:\\Python34\\lib\\site-packages\\scipy\\optimize\\lbfgsb.py\u001b[0m in \u001b[0;36mfunc_and_grad\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m    264\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    265\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mfunc_and_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 266\u001b[1;33m             \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    267\u001b[0m             \u001b[0mg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mjac\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    268\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mg\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32mC:\\Python34\\lib\\site-packages\\scipy\\optimize\\optimize.py\u001b[0m in \u001b[0;36mfunction_wrapper\u001b[1;34m(*wrapper_args)\u001b[0m\n\u001b[0;32m    280\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfunction_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mwrapper_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    281\u001b[0m         \u001b[0mncalls\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 282\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mfunction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwrapper_args\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    283\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    284\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mncalls\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunction_wrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m<ipython-input-41-21f505285fac>\u001b[0m in \u001b[0;36mJDropout\u001b[1;34m(self, thetas)\u001b[0m\n\u001b[0;32m    197\u001b[0m         \u001b[0mm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0mm\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 199\u001b[1;33m             \u001b[0msum_\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mm\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msquare\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeedforwardMatrixDropout\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mthetas\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtestTime\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mY\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    200\u001b[0m         \u001b[0msum_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msum_\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0ml2_reg\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0mthetas\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0msum_\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m<ipython-input-41-21f505285fac>\u001b[0m in \u001b[0;36mfeedforwardMatrixDropout\u001b[1;34m(self, A, thetas, testTime)\u001b[0m\n\u001b[0;32m     81\u001b[0m         \u001b[0mactivations\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mA\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     82\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_levels\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 83\u001b[1;33m             \u001b[0mactivations\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mg\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_ones\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mactivations\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mthetas\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     84\u001b[0m             \u001b[1;31m# Dropout: input layer will be untouched. We need to skip last layer, se we leave 0 for the last layer!\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m             \u001b[0mdropmask\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_levels\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdropout_mask\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mactivations\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtestTime\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
       ]
      }
     ],
     "prompt_number": 56
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Final Prediction"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def predict(hidden=[30, 20], \n",
      "           g=sigmoid, \n",
      "           g_derivative=sigmoid_derivative, \n",
      "           lambda_=1e-5, \n",
      "           lbfgs_maxiter=1, \n",
      "           learning_rate=1, \n",
      "           dropout_prob=0.2, \n",
      "           lbfgs=False,\n",
      "           \n",
      "           mini_batch_size=500, \n",
      "           batch_iterations=500, \n",
      "           ):\n",
      "    \n",
      "    X = X_train\n",
      "    Y = Y_train\n",
      "    \n",
      "    arch = [X.shape[1]] + hidden + [NUM_OF_CLASSES]\n",
      "\n",
      "    learner = NeuralNetLearner(arch, g=g, g_derivative=g_derivative, lambda_=lambda_, maxiter=lbfgs_maxiter, learning_rate=learning_rate, dropout_prob=dropout_prob, lbfgs=lbfgs, print_=False)\n",
      "    bs = BatchSeq( learner, mini_batch_size=mini_batch_size, iterations=batch_iterations)\n",
      "    bs.fit(X, Y)\n",
      "    predicted = bs.predict(X_test)\n",
      "    return predicted"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 45
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def write_to_file(predicted, name):\n",
      "    answer = np.matrix(predicted.astype(str))\n",
      "    ids = np.matrix(range(1, answer.shape[0]+1))\n",
      "    answer = np.hstack((ids.T.astype(str), answer))\n",
      "    answer = np.vstack((np.array([\"id\",\"Class_1\",\"Class_2\",\"Class_3\",\"Class_4\",\"Class_5\",\"Class_6\",\"Class_7\",\"Class_8\",\"Class_9\"]), answer))\n",
      "    answer = answer.tolist()\n",
      "    answer = \"\\n\".join( [\",\".join(line) for line in answer] )\n",
      "\n",
      "    fo = open(\"../results2/\"+name+\".csv\", \"wt\")\n",
      "    fo.write(answer)\n",
      "    fo.close()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 46
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "predicted = predict(hidden=[20, 20], \n",
      "           g=sigmoid, \n",
      "           g_derivative=sigmoid_derivative, \n",
      "           lambda_=1e-5, \n",
      "           lbfgs_maxiter=100000, \n",
      "           learning_rate=1, \n",
      "           dropout_prob=0.2, \n",
      "           lbfgs=True,\n",
      "           \n",
      "           mini_batch_size=0, \n",
      "           batch_iterations=1, \n",
      "           )"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Lbfgs iterations: 30\n",
        "Iter: 0  \n",
        "        Time: 701.18\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "-c:260: FutureWarning: comparison to `None` will result in an elementwise object comparison in the future.\n"
       ]
      }
     ],
     "prompt_number": 49
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "write_to_file(predicted, \"NewAge_a20-20_sWhole_i1_lambda-5_dropout-02\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 51
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}