{
 "metadata": {
  "name": "",
  "signature": "sha256:a6474859382d3c721104b7882c78098a7e17fbfa7acdaecf91d33a9656b4c587"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import scipy.stats\n",
      "import numpy as np\n",
      "import math\n",
      "from collections import defaultdict\n",
      "import random\n",
      "\n",
      "def pearson(x,y):\n",
      "    x,y = np.array(x), np.array(y)\n",
      "    anynan = np.logical_or(np.isnan(x), np.isnan(y))\n",
      "    r = scipy.stats.pearsonr(x[~anynan],y[~anynan])[0]\n",
      "    return 0. if math.isnan(r) else r\n",
      "\n",
      "def fix_dilution(s):\n",
      "    return s.replace('\"', '').strip()\n",
      "\n",
      "def legend(fn):\n",
      "    with open(fn) as f:\n",
      "        for l in f:\n",
      "            vals_legend = l.strip().split('\\t')[6:]\n",
      "            return vals_legend\n",
      " \n",
      "def load_data(fn):\n",
      "    readings = defaultdict(list)\n",
      "    with open(fn) as f:\n",
      "        vals_legend = next(f).strip().split('\\t')[6:]\n",
      "        for l in f:\n",
      "            l = l.strip()\n",
      "            t = l.split('\\t')\n",
      "            cid, dilution, vals = t[0], fix_dilution(t[4]), list(map(float, t[6:]))\n",
      "            readings[cid, dilution].append(vals)\n",
      "    for a,b in readings.items():\n",
      "        readings[a] = np.array(b)\n",
      "    return dict(readings)\n",
      "\n",
      "def mean_indv_notnan(data):\n",
      "    means = []\n",
      "    #average non-nan elements\n",
      "    for vals in data.T:\n",
      "        nonnan = vals[~np.isnan(vals)]\n",
      "        means.append(np.mean(nonnan))\n",
      "    return np.array(means)\n",
      "    \n",
      "def load_data_mean_indv(fn):\n",
      "    readings = load_data(fn)\n",
      "    r2 = {}\n",
      "    for a,b in readings.items():\n",
      "        r2[a] = mean_indv_notnan(np.array(b))\n",
      "    return r2\n",
      "\n",
      "def permuted_chem(readings, perm):\n",
      "    rand = random.Random(perm)\n",
      "    comb = sorted(readings.items())\n",
      "    keys = [ a[0] for a in comb ]\n",
      "    values = [ a[1] for a in comb ]\n",
      "    rand.shuffle(values)\n",
      "    return { a:b for a,b in zip(keys, values) }\n",
      "\n",
      "\n",
      "def read_predictions(s):\n",
      "    rd = {}\n",
      "    leg = set(LEGEND)\n",
      "    for l in s.split(\"\\n\"):\n",
      "        l = l.strip()\n",
      "        if l:\n",
      "            cid, desc, val = t = l.split('\\t')\n",
      "            if desc not in leg:\n",
      "                raise ValueError(\"Descriptor \" + desc + \" not valid.\")\n",
      "            rd[cid, desc] = float(val)\n",
      "    return rd\n",
      "\n",
      "def preds_to_vec(pd):\n",
      "    rd = {}\n",
      "    cids = [a[0] for a in pd.keys()]\n",
      "    for cid in set(cids):\n",
      "        rd[cid] = np.array([ pd[cid, desc] for desc in LEGEND ])\n",
      "    return rd\n",
      " \n",
      "NAN = float(\"NaN\")\n",
      "\n",
      "def realscore(a, answers):\n",
      "    #intensity has a fixed dilution\n",
      "    return list(answers.get((a[0], \"1/1,000\"), [NAN])[:1]) + list(answers[a][1:])\n",
      "\n",
      "def evaluate_r(preds, query, answers):\n",
      "    userscores = np.array([ preds[a[0]] for a in query ])\n",
      "    realscores = np.array([ realscore(a, answers) for a in query ])\n",
      "    rint = pearson(userscores[:,0], realscores[:,0])\n",
      "    rval = pearson(userscores[:,1], realscores[:,1])\n",
      "    rdecall = [ pearson(userscores[:,i], realscores[:,i]) for i in range(2,21) ]\n",
      "    rdec = np.mean(rdecall)\n",
      "    return np.array([rint, rval, rdec])\n",
      "\n",
      "def read_query(fn):\n",
      "    res = []\n",
      "    with open(fn) as f:\n",
      "        for l in f:\n",
      "            l = l.strip()\n",
      "            if l:\n",
      "                res.append(tuple(l.split(\"\\t\")))\n",
      "    return res\n",
      "\n",
      "def normalization_consts(query, answers):\n",
      "    \"\"\" Obtain the gold-standard deviation for our test and\n",
      "    leaderboard data sets. \"\"\"\n",
      "    reals = { cid: realscore((cid, dil), answers) for cid,dil in query }\n",
      "    permres = []\n",
      "    for i in range(10000):\n",
      "        permuted = permuted_chem(answers, i)\n",
      "        permres.append(evaluate_r(reals, query, permuted))\n",
      "    permres = np.array(permres)\n",
      "    return np.mean(permres, axis=0), np.std(permres, axis=0)\n",
      "\n",
      "NORM_STD = [ 0.18, 0.16, 0.06 ] #an average of normalizatin_costs outputs)\n",
      "#means were 0 (as expected for Pearson correlation)\n",
      "\n",
      "def final_score(rs):\n",
      "    zs = rs/NORM_STD\n",
      "    return np.mean(zs)\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "\n",
      "    LEGEND = legend(\"../data/TrainSet-hw2.txt\")\n",
      "    query = read_query(\"../data/predict.txt\")\n",
      "    test = load_data_mean_indv(\"../data/TrainSet-hw2.txt\")\n",
      "    #print (test)\n",
      "    #leaderboard = load_data_mean_indv(\"LeaderboardSet-hw2.txt\")\n",
      "\n",
      "\n",
      "    query_test = [ a for a in query if a in test ]\n",
      "    #query_leaderboard = [ a for a in query if a in leaderboard ]\n",
      "\n",
      "    #print(\"NORM_TEST\", normalization_consts(query_test, test))\n",
      "    #print(\"NORM_LEAD\", normalization_consts(query_leaderboard, leaderboard))\n",
      "    #fdsfd\n",
      "\n",
      "    preds = read_predictions(open(\"../data/mean.txt\").read())\n",
      "    preds = preds_to_vec(preds)\n",
      "    rs = evaluate_r(preds, query_test, test)\n",
      "    print(\"FINAL\", final_score(rs))\n",
      "    #rs = evaluate_r(preds, query_leaderboard, leaderboard)\n",
      "    #print(\"LEADERBORD\", final_score(rs))\n",
      "\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "IndexError",
       "evalue": "too many indices for array",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
        "\u001b[1;32m<ipython-input-2-9dcdc928af66>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    136\u001b[0m     \u001b[0mpreds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mread_predictions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"../data/mean.txt\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    137\u001b[0m     \u001b[0mpreds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpreds_to_vec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 138\u001b[1;33m     \u001b[0mrs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mevaluate_r\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mquery_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    139\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"FINAL\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfinal_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    140\u001b[0m     \u001b[1;31m#rs = evaluate_r(preds, query_leaderboard, leaderboard)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m<ipython-input-2-9dcdc928af66>\u001b[0m in \u001b[0;36mevaluate_r\u001b[1;34m(preds, query, answers)\u001b[0m\n\u001b[0;32m     85\u001b[0m     \u001b[0muserscores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m \u001b[0mpreds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mquery\u001b[0m \u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m     \u001b[0mrealscores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m \u001b[0mrealscore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0manswers\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mquery\u001b[0m \u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 87\u001b[1;33m     \u001b[0mrint\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpearson\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muserscores\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrealscores\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     88\u001b[0m     \u001b[0mrval\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpearson\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muserscores\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrealscores\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m     \u001b[0mrdecall\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m \u001b[0mpearson\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muserscores\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrealscores\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m21\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;31mIndexError\u001b[0m: too many indices for array"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}