{
 "metadata": {
  "name": "",
  "signature": "sha256:d2ed45b9a394d9532f9b70fa149b09a0579b3dfda8477269be285641522f25f7"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "import math"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X = np.loadtxt(open(\"../data/train.csv\",\"rb\"),delimiter=\",\",skiprows=1, usecols=range(1,94))\n",
      "classes = np.loadtxt(open(\"../data/train.csv\",\"rb\"),dtype=str,delimiter=\",\",skiprows=1, usecols=[94])\n",
      "# get class\n",
      "intClasses = []\n",
      "for c in classes:\n",
      "    intClasses += [int(c[-2])]\n",
      "y_ = np.array(intClasses)\n",
      "Xy = np.hstack((X, np.matrix(intClasses).T))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class LogisticRegression():\n",
      "    def __init__(self, x_train=None, y_train=None, x_test=None, y_test=None,alpha=.1):\n",
      "        # Set L2 regularization strength\\n\",\n",
      "        self.alpha = alpha\n",
      "\n",
      "        # Set the data\n",
      "        self.x_train = x_train\n",
      "        self.y_train = y_train\n",
      "        self.x_test = x_test\n",
      "        self.y_test = y_test\n",
      "\n",
      "    def logistic_function(self, z):\n",
      "        return 1 / (1 + math.e**(-z))\n",
      "\n",
      "    def logistic_probability(self, y, x_, theta_):\n",
      "        \"\"\"y is 0 or 1, x_ and theta_ are vectors of the same length\"\"\"\n",
      "        result = logistic_function(np.dot(theta_, x_))\n",
      "        return result**y * (1 - result) ** (1 - y)\n",
      "\n",
      "    def l2(self, vector_):\n",
      "        \"\"\"l2 regularization\"\"\"\n",
      "        return sum( [element**2 for element in vector_] )\n",
      "\n",
      "    def cost_function(self, y_, X, theta_):\n",
      "        \"\"\"Error function\"\"\"\n",
      "        m = len(y_)\n",
      "        s = 0\n",
      "        for i in range(m):\n",
      "            x_ = X[i]\n",
      "            g = self.logistic_function( np.dot( theta_, x_ ) )\n",
      "            ###############################################################################################\n",
      "            print(\"ali\",y_[i], (1 - y_[i]) )\n",
      "            if y_[i] == 1:\n",
      "                s += math.log(g)\n",
      "            elif y_[i] == 0:\n",
      "                s += math.log(1 - g)\n",
      "            else:\n",
      "                print(\"Error pri cost function\")\n",
      "        #s += y_[i] * math.log(g) + (1 - y_[i]) * math.log(1 - g)\n",
      "        J = (1/m) * s + (1/2) * self.l2(theta_) \n",
      "        # Warning: Using minus. We can do max, but we need min.\n",
      "        return -J\n",
      "\n",
      "    def grad_cost_function(self, y_, X, theta_):\n",
      "        \"\"\"Gradient of error function\"\"\"\n",
      "        m = len(y_)\n",
      "        # s is a vector\\n\",\n",
      "        s_ = np.zeros( len(theta_) ) \n",
      "        for i in range(m):\n",
      "            x_ = X[i]\n",
      "            g = self.logistic_function( np.dot( theta_, x_ ) )\n",
      "            # Instead of going for each theta like:\n",
      "            # s += y_[i] - g * x[j]\n",
      "            # we can do everython in one line, that we remove [j]\n",
      "            s_ += y_[i] - g * x_\n",
      "        # Add regularization\n",
      "        s_ += theta_\n",
      "        return s_\n",
      "\n",
      "    def numerical_grad(self, f, params, epsilon):\n",
      "        \"\"\"Method of finite differences, sanity check to see if our Jgrad is implemented correctly\"\"\"\n",
      "        num_grad = np.zeros_like(params)\n",
      "        perturb = np.zeros_like(params)\n",
      "        for i in range(params.size):\n",
      "            perturb[i] = epsilon\n",
      "            j1 = f(params + perturb)\n",
      "            j2 = f(params - perturb)\n",
      "            num_grad[i] = (j1 - j2) / (2. * epsilon)\n",
      "            perturb[i] = 0\n",
      "        return num_grad\n",
      "\n",
      "    def check_grad(self, X, y_):\n",
      "        theta_ = np.ones( X.shape[1] )\n",
      "        ag = self.grad_cost_function(y_, X, theta_)\n",
      "        ng = 1\n",
      "        ng = self.numerical_grad(lambda p_: self.cost_function(y_, X, p_), theta_, 1e-1)\n",
      "\n",
      "        print(np.sum((ag - ng)**2))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "LR = LogisticRegression()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "y1_ = np.ones_like(y_)\n",
      "LR.check_grad(X, y1_)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}