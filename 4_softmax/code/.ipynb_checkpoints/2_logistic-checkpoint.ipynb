{
 "metadata": {
  "name": "",
  "signature": "sha256:52d2f56e6a4878f9d933923448e7868785253b788e369b3195ffb5ef263219dc"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Usefull links\n",
      "- http://blog.smellthedata.com/2009/06/python-logistic-regression-with-l2.html\n",
      "- http://scikit-learn.org/stable/modules/multiclass.html\n",
      "- http://scikit-learn.org/stable/developers/#get-params-and-set-params\n",
      "- http://aimotion.blogspot.com/2011/11/machine-learning-with-python-logistic.html"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "import random\n",
      "import math\n",
      "import scipy as sp\n",
      "from scipy.optimize.optimize import fmin_bfgs\n",
      "from sklearn.base import BaseEstimator, ClassifierMixin\n",
      "import random\n",
      "import warnings\n",
      "import random\n",
      "from sklearn import preprocessing\n",
      "from sklearn import linear_model\n",
      "from sklearn import multiclass\n",
      "from sklearn import ensemble\n",
      "from sklearn import metrics"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 160
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Load data and normalize it"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X = np.loadtxt(open(\"../data/train.csv\",\"rb\"),delimiter=\",\",skiprows=1, usecols=range(1,94))\n",
      "classes = np.loadtxt(open(\"../data/train.csv\",\"rb\"),dtype=str,delimiter=\",\",skiprows=1, usecols=[94])\n",
      "# get class\n",
      "intClasses = []\n",
      "for c in classes:\n",
      "    intClasses += [int(c[-2])]\n",
      "y = np.array(intClasses)\n",
      "# Normalize data\n",
      "norm = preprocessing.Normalizer()\n",
      "X = norm.fit_transform(X)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 75
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Logistic Regression"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def logistic_function(z):\n",
      "    return 1 / (1 + np.exp(-z))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 77
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class LogisticRegression(BaseEstimator, ClassifierMixin):\n",
      "    def __init__(self):\n",
      "        pass\n",
      "\n",
      "    def l2_regularization(self, vector_):\n",
      "        return sum( vector_**2 )\n",
      "    \n",
      "    def likelihood(self, theta_):\n",
      "        e = -50   # For correction of -inf\n",
      "        X = self.X_train\n",
      "        m = len(self.y_train)\n",
      "        s = 0\n",
      "        for i in range(m):\n",
      "            p = logistic_function( np.dot( theta_, X[i, :] ) )\n",
      "            cur = np.log(np.maximum(np.minimum(p, 1 - 1e-15), 1e-15)) if self.y_train[i] == 1 else np.log(np.maximum(np.minimum(1 - p, 1 - 1e-15), 1e-15)) \n",
      "            s += cur\n",
      "        \n",
      "        J = (1/m) * s# - self.alpha/(2*m)#  * self.l2_regularization(theta_) \n",
      "        return J\n",
      "        \n",
      "    def grad_likelihood(self, theta_):\n",
      "        y_ = self.y_train\n",
      "        X = self.X_train\n",
      "        m = len(self.y_train)\n",
      "        \n",
      "        # s is a vector\n",
      "        s_ = np.zeros_like(theta_) \n",
      "        for j in range(len(theta_)):\n",
      "            for i in range(m):\n",
      "                p = logistic_function( np.dot( theta_, X[i, :]  ) )\n",
      "                s_[j] += ( y_[i] - p ) * X[i, j]\n",
      "        # Add regularization\n",
      "        s_ = 1/m * s_#- self.alpha/m * theta_\n",
      "        return s_\n",
      "\n",
      "    def cost_function(self, theta_):\n",
      "        \"\"\"Negate likelihood to minimize it\"\"\"\n",
      "        return (-1) * self.likelihood(theta_)\n",
      "        #return sum( theta_ )\n",
      "        \n",
      "    def grad_cost_function(self, theta_):\n",
      "        return (-1) * self.grad_likelihood(theta_)\n",
      "        #return np.ones_like(theta_)\n",
      "    \n",
      "    def get_params(self, deep=True):\n",
      "        # suppose this estimator has parameters \"alpha\" and \"recursive\"\n",
      "        return {}\n",
      "\n",
      "    def set_params(self, **parameters):\n",
      "        for parameter, value in parameters.items():\n",
      "            self.setattr(parameter, value)\n",
      "        return self\n",
      "    \n",
      "    def fit(self, X_train=None, y_train=None):\n",
      "        self.X_train = X_train\n",
      "        self.y_train = y_train\n",
      "        self.alpha = 0.1\n",
      "        \n",
      "        theta_ = np.ones(self.X_train.shape[1])  # Starting point\n",
      "        self.theta = fmin_bfgs(self.cost_function, theta_, fprime=self.grad_cost_function)  # , disp=False\n",
      "        return self.theta\n",
      "    \n",
      "        \n",
      "    def predict_proba(self, X):\n",
      "        \"\"\"Predict 1 or 0 for a given vector x_\"\"\"\n",
      "        result = [logistic_function(np.dot(self.theta, x_)) for x_ in X]\n",
      "        return result\n",
      "    \n",
      "    def decision_function(self, X):\n",
      "        \"\"\"Predict 1 or 0 for a given vector x_\"\"\"\n",
      "        result = [logistic_function(np.dot(self.theta, x_)) for x_ in X]\n",
      "        return result\n",
      "    #result**y * (1 - result) ** (1 - y)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 355
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Testing cost, grad, num. grad"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def numerical_grad(f, params, epsilon):\n",
      "    \"\"\"Method of finite differences, sanity check to see if our Jgrad is implemented correctly\"\"\"\n",
      "    num_grad = np.zeros_like(params)\n",
      "    perturb = np.zeros_like(params)\n",
      "    for i in range(params.size):\n",
      "        perturb[i] = epsilon\n",
      "        j1 = f(params + perturb)\n",
      "        j2 = f(params - perturb)\n",
      "        num_grad[i] = (j1 - j2) / (2. * epsilon)\n",
      "        perturb[i] = 0\n",
      "    return num_grad\n",
      "\n",
      "def check_grad(LR, theta_):\n",
      "    f = lambda theta: LR.cost_function(theta)\n",
      "    ag = LR.grad_cost_function(theta_)\n",
      "    ng = numerical_grad(f, theta_, 1e-5)\n",
      "    return np.sum((ag - ng)**2)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 352
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "indices = range(2000)\n",
      "\n",
      "X_test = X[indices]\n",
      "y_test = y[indices]\n",
      "LR = LogisticRegression()\n",
      "LR.fit(X_test, y_test)\n",
      "\n",
      "# Test cost function\n",
      "theta_test = np.ones(X_test.shape[1])\n",
      "c = LR.cost_function(theta_test)\n",
      "g = LR.grad_cost_function(theta_test).shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 353
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "gr = check_grad(LR, theta_test)\n",
      "print(\"Cost:\", c)\n",
      "print(\"Grad:\", g)\n",
      "print(\"Grad diff:\", gr)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Cost: 0.620764309994\n",
        "Grad: (93,)\n",
        "Grad diff: 0.0572959667038\n"
       ]
      }
     ],
     "prompt_number": 354
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "One vs all"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "treshold = 10000\n",
      "\n",
      "step = 200\n",
      "indices = range(0, X.shape[0], step)\n",
      "\n",
      "X_test = X[indices]\n",
      "y_test = y[indices]\n",
      "\n",
      "LR2 = LogisticRegression()\n",
      "classifier = sklearn.multiclass.OneVsRestClassifier(LR2)\n",
      "classifier.fit(X_test, y_test)\n",
      "classifier.predict(X[treshold:treshold + 30])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "One vs one"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "treshold = 3000\n",
      "LR3 = LogisticRegression()\n",
      "classifier = multiclass.OneVsOneClassifier(LR3).fit(X[:treshold], y[:treshold])\n",
      "classifier.fit(X_test, y_test).predict(X[2800:2830])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Warning: Desired error not necessarily achieved due to precision loss.\n",
        "         Current function value: 0.001550\n",
        "         Iterations: 0\n",
        "         Function evaluations: 26\n",
        "         Gradient evaluations: 14\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 39,
       "text": [
        "array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
        "       1, 1, 1, 1, 1, 1, 1])"
       ]
      }
     ],
     "prompt_number": 39
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Cross validation"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def evaluate(act, pred):\n",
      "    \"\"\"Log loss\"\"\"\n",
      "    epsilon = 1e-15\n",
      "    pred = sp.maximum(epsilon, pred)\n",
      "    pred = sp.minimum(1-epsilon, pred)\n",
      "    ll = sum(act*sp.log(pred) + sp.subtract(1,act)*sp.log(sp.subtract(1,pred)))\n",
      "    ll = ll * -1.0/len(act)\n",
      "    return -ll"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 287
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def evaluate2(act, pred):\n",
      "    return metrics.log_loss(act, pred, eps=1e-15, normalize=True)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 288
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def my_cross_validation(X, y_, model):\n",
      "    k = 3\n",
      "    S = []\n",
      "    kf = sklearn.cross_validation.KFold(len(y_), n_folds=k, shuffle=True)\n",
      "    for train_index, test_index in kf:\n",
      "        # Split\n",
      "        X_train, X_test = X[train_index], X[test_index]\n",
      "        y_train, y_test = y_[train_index], y_[test_index]\n",
      "        # Predict\n",
      "        model.fit(X_train, y_train)\n",
      "        P = model.predict(X_test) \n",
      "        # Evaluate\n",
      "        S += [evaluate(y_test, P)]\n",
      "    S = np.array(S)\n",
      "    return(np.mean(S))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 289
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "LogisticRegression sklearn"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Use integrated Logistic Regression and OneVsOneClassifier\n",
      "step = 10\n",
      "indices = range(0, X.shape[0], step)\n",
      "\n",
      "X_test = X[indices]\n",
      "y_test = y[indices]\n",
      "# Integrated\n",
      "\n",
      "logreg = linear_model.LogisticRegression()\n",
      "classifier = multiclass.OneVsOneClassifier(logreg)\n",
      "one = my_cross_validation(X_test, y_test, classifier)\n",
      "\n",
      "logreg = linear_model.LogisticRegression()\n",
      "classifier = multiclass.OneVsRestClassifier(logreg)\n",
      "rest = my_cross_validation(X_test, y_test, classifier)\n",
      "\n",
      "print(\"OneVsOne\", one)\n",
      "print(\"OneVsAll\", rest)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "OneVsOne 132.618447109\n",
        "OneVsAll 132.617970368\n"
       ]
      }
     ],
     "prompt_number": 290
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Random Forest sklearn"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Use integrated Logistic Regression and OneVsOneClassifier\n",
      "step = 10\n",
      "indices = range(0, X.shape[0], step)\n",
      "\n",
      "indices = random.shuffle(indices)\n",
      "\n",
      "X_test = X[indices]\n",
      "y_test = y[indices]\n",
      "# Integrated\n",
      "\n",
      "classifier = ensemble.RandomForestClassifier()\n",
      "one = my_cross_validation(X_test, y_test, classifier)\n",
      "\n",
      "classifier = ensemble.RandomForestClassifier()\n",
      "rest = my_cross_validation(X_test, y_test, classifier)\n",
      "\n",
      "print(\"OneVsOne\", one)\n",
      "print(\"OneVsAll\", rest)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "OneVsOne 132.498059624\n",
        "OneVsAll 132.494321845\n"
       ]
      }
     ],
     "prompt_number": 291
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "My model"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "warnings.filterwarnings(\"ignore\")\n",
      "\n",
      "threshold = X.shape[0]\n",
      "threshold = 1000\n",
      "X_try = X[:threshold]\n",
      "y_try = np.array([random.randint(0, 5) for i in range(X_try.shape[0])])\n",
      "\n",
      "logreg = LogisticRegression()\n",
      "classifier = multiclass.OneVsOneClassifier(logreg)\n",
      "one = my_cross_validation(X_try, y_try, classifier)\n",
      "\n",
      "logreg = LogisticRegression()\n",
      "classifier = multiclass.OneVsRestClassifier(logreg)\n",
      "rest = my_cross_validation(X_try, y_try, classifier)\n",
      "\n",
      "print(\"OneVsOne\", one)\n",
      "print(\"OneVsAll\", rest)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Warning: Desired error not necessarily achieved due to precision loss.\n",
        "         Current function value: 0.021831\n",
        "         Iterations: 0\n",
        "         Function evaluations: 26\n",
        "         Gradient evaluations: 14\n",
        "Warning: Desired error not necessarily achieved due to precision loss."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "         Current function value: 0.022356\n",
        "         Iterations: 0\n",
        "         Function evaluations: 26\n",
        "         Gradient evaluations: 14\n",
        "Warning: Desired error not necessarily achieved due to precision loss."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "         Current function value: 0.023020\n",
        "         Iterations: 0\n",
        "         Function evaluations: 26\n",
        "         Gradient evaluations: 14\n",
        "Warning: Desired error not necessarily achieved due to precision loss."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "         Current function value: 0.022143\n",
        "         Iterations: 0\n",
        "         Function evaluations: 26\n",
        "         Gradient evaluations: 14\n",
        "Warning: Desired error not necessarily achieved due to precision loss."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "         Current function value: 0.021041\n",
        "         Iterations: 0\n",
        "         Function evaluations: 26\n",
        "         Gradient evaluations: 14\n",
        "Warning: Desired error not necessarily achieved due to precision loss."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "         Current function value: 0.020485\n",
        "         Iterations: 0\n",
        "         Function evaluations: 26\n",
        "         Gradient evaluations: 14\n",
        "Warning: Desired error not necessarily achieved due to precision loss."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "         Current function value: 0.021041\n",
        "         Iterations: 0\n",
        "         Function evaluations: 26\n",
        "         Gradient evaluations: 14\n",
        "Warning: Desired error not necessarily achieved due to precision loss."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "         Current function value: 0.020306\n",
        "         Iterations: 0\n",
        "         Function evaluations: 26\n",
        "         Gradient evaluations: 14\n",
        "Warning: Desired error not necessarily achieved due to precision loss."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "         Current function value: 0.019375\n",
        "         Iterations: 0\n",
        "         Function evaluations: 26\n",
        "         Gradient evaluations: 14\n",
        "Warning: Desired error not necessarily achieved due to precision loss."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "         Current function value: 0.021528\n",
        "         Iterations: 0\n",
        "         Function evaluations: 26\n",
        "         Gradient evaluations: 14\n",
        "Warning: Desired error not necessarily achieved due to precision loss."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "         Current function value: 0.020759\n",
        "         Iterations: 0\n",
        "         Function evaluations: 26\n",
        "         Gradient evaluations: 14\n",
        "Warning: Desired error not necessarily achieved due to precision loss."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "         Current function value: 0.019787\n",
        "         Iterations: 0\n",
        "         Function evaluations: 26\n",
        "         Gradient evaluations: 14\n",
        "Warning: Desired error not necessarily achieved due to precision loss."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "         Current function value: 0.021330\n",
        "         Iterations: 0\n",
        "         Function evaluations: 26\n",
        "         Gradient evaluations: 14\n",
        "Warning: Desired error not necessarily achieved due to precision loss."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "         Current function value: 0.020306\n",
        "         Iterations: 0\n",
        "         Function evaluations: 26\n",
        "         Gradient evaluations: 14\n",
        "Warning: Desired error not necessarily achieved due to precision loss."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "         Current function value: 0.019620\n",
        "         Iterations: 0\n",
        "         Function evaluations: 26\n",
        "         Gradient evaluations: 14\n",
        "Warning: Desired error not necessarily achieved due to precision loss."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "         Current function value: 0.022143\n",
        "         Iterations: 0\n",
        "         Function evaluations: 26\n",
        "         Gradient evaluations: 14\n",
        "Warning: Desired error not necessarily achieved due to precision loss."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "         Current function value: 0.022464\n",
        "         Iterations: 0\n",
        "         Function evaluations: 26\n",
        "         Gradient evaluations: 14\n"
       ]
      },
      {
       "ename": "KeyboardInterrupt",
       "evalue": "",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
        "\u001b[1;32m<ipython-input-42-723463950816>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mlogreg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mclassifier\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmulticlass\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOneVsOneClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogreg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[0mone\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmy_cross_validation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_try\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_try\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[0mlogreg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m<ipython-input-39-b7f14bc1fd68>\u001b[0m in \u001b[0;36mmy_cross_validation\u001b[1;34m(X, y_, model)\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my_\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtrain_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtest_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[1;31m# Predict\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m         \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m         \u001b[0mP\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[1;31m# Evaluate\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32mC:\\Python34\\lib\\site-packages\\sklearn\\multiclass.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    409\u001b[0m         \"\"\"\n\u001b[0;32m    410\u001b[0m         self.estimators_, self.classes_ = fit_ovo(self.estimator, X, y,\n\u001b[1;32m--> 411\u001b[1;33m                                                   self.n_jobs)\n\u001b[0m\u001b[0;32m    412\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    413\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32mC:\\Python34\\lib\\site-packages\\sklearn\\multiclass.py\u001b[0m in \u001b[0;36mfit_ovo\u001b[1;34m(estimator, X, y, n_jobs)\u001b[0m\n\u001b[0;32m    320\u001b[0m         delayed(_fit_ovo_binary)(\n\u001b[0;32m    321\u001b[0m             estimator, X, y, classes[i], classes[j])\n\u001b[1;32m--> 322\u001b[1;33m         for i in range(n_classes) for j in range(i + 1, n_classes))\n\u001b[0m\u001b[0;32m    323\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    324\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mestimators\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclasses\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32mC:\\Python34\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m    651\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    652\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mfunction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 653\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    654\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    655\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpre_dispatch\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"all\"\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mn_jobs\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32mC:\\Python34\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36mdispatch\u001b[1;34m(self, func, args, kwargs)\u001b[0m\n\u001b[0;32m    398\u001b[0m         \"\"\"\n\u001b[0;32m    399\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pool\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 400\u001b[1;33m             \u001b[0mjob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImmediateApply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    401\u001b[0m             \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    402\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0m_verbosity_filter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32mC:\\Python34\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, func, args, kwargs)\u001b[0m\n\u001b[0;32m    136\u001b[0m         \u001b[1;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    137\u001b[0m         \u001b[1;31m# arguments in memory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 138\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    139\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    140\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32mC:\\Python34\\lib\\site-packages\\sklearn\\multiclass.py\u001b[0m in \u001b[0;36m_fit_ovo_binary\u001b[1;34m(estimator, X, y, i, j)\u001b[0m\n\u001b[0;32m    310\u001b[0m     \u001b[0my_binary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0my\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    311\u001b[0m     \u001b[0mind\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 312\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_fit_binary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mind\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcond\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_binary\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclasses\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    313\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    314\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32mC:\\Python34\\lib\\site-packages\\sklearn\\multiclass.py\u001b[0m in \u001b[0;36m_fit_binary\u001b[1;34m(estimator, X, y, classes)\u001b[0m\n\u001b[0;32m     59\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m         \u001b[0mestimator\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclone\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m         \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     62\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mestimator\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m<ipython-input-22-2cc758e536ac>\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X_train, y_train)\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     61\u001b[0m         \u001b[0mtheta_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mones\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# Starting point\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 62\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtheta\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfmin_bfgs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcost_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtheta_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfprime\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrad_cost_function\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# , disp=False\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     63\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtheta\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32mC:\\Python34\\lib\\site-packages\\scipy\\optimize\\optimize.py\u001b[0m in \u001b[0;36mfmin_bfgs\u001b[1;34m(f, x0, fprime, args, gtol, norm, epsilon, maxiter, full_output, disp, retall, callback)\u001b[0m\n\u001b[0;32m    780\u001b[0m             'return_all': retall}\n\u001b[0;32m    781\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 782\u001b[1;33m     \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_minimize_bfgs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfprime\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcallback\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mopts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    783\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    784\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfull_output\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32mC:\\Python34\\lib\\site-packages\\scipy\\optimize\\optimize.py\u001b[0m in \u001b[0;36m_minimize_bfgs\u001b[1;34m(fun, x0, args, jac, callback, gtol, norm, eps, maxiter, disp, return_all, **unknown_options)\u001b[0m\n\u001b[0;32m    853\u001b[0m             \u001b[0malpha_k\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mold_fval\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mold_old_fval\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgfkp1\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    854\u001b[0m                      _line_search_wolfe12(f, myfprime, xk, pk, gfk,\n\u001b[1;32m--> 855\u001b[1;33m                                           old_fval, old_old_fval)\n\u001b[0m\u001b[0;32m    856\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0m_LineSearchError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    857\u001b[0m             \u001b[1;31m# Line search failed to find a better solution.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32mC:\\Python34\\lib\\site-packages\\scipy\\optimize\\optimize.py\u001b[0m in \u001b[0;36m_line_search_wolfe12\u001b[1;34m(f, fprime, xk, pk, gfk, old_fval, old_old_fval, **kwargs)\u001b[0m\n\u001b[0;32m    688\u001b[0m     ret = line_search_wolfe1(f, fprime, xk, pk, gfk,\n\u001b[0;32m    689\u001b[0m                              \u001b[0mold_fval\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mold_old_fval\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 690\u001b[1;33m                              **kwargs)\n\u001b[0m\u001b[0;32m    691\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    692\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mret\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32mC:\\Python34\\lib\\site-packages\\scipy\\optimize\\linesearch.py\u001b[0m in \u001b[0;36mline_search_wolfe1\u001b[1;34m(f, fprime, xk, pk, gfk, old_fval, old_old_fval, args, c1, c2, amax, amin, xtol)\u001b[0m\n\u001b[0;32m     94\u001b[0m     stp, fval, old_fval = scalar_search_wolfe1(\n\u001b[0;32m     95\u001b[0m             \u001b[0mphi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mderphi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mold_fval\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mold_old_fval\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mderphi0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 96\u001b[1;33m             c1=c1, c2=c2, amax=amax, amin=amin, xtol=xtol)\n\u001b[0m\u001b[0;32m     97\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     98\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mstp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfval\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mold_fval\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgval\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32mC:\\Python34\\lib\\site-packages\\scipy\\optimize\\linesearch.py\u001b[0m in \u001b[0;36mscalar_search_wolfe1\u001b[1;34m(phi, derphi, phi0, old_phi0, derphi0, c1, c2, amax, amin, xtol)\u001b[0m\n\u001b[0;32m    166\u001b[0m             \u001b[0malpha1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    167\u001b[0m             \u001b[0mphi1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mphi\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 168\u001b[1;33m             \u001b[0mderphi1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mderphi\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    169\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    170\u001b[0m             \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32mC:\\Python34\\lib\\site-packages\\scipy\\optimize\\linesearch.py\u001b[0m in \u001b[0;36mderphi\u001b[1;34m(s)\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     84\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mderphi\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 85\u001b[1;33m         \u001b[0mgval\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfprime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxk\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mpk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mnewargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     86\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m             \u001b[0mgc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32mC:\\Python34\\lib\\site-packages\\scipy\\optimize\\optimize.py\u001b[0m in \u001b[0;36mfunction_wrapper\u001b[1;34m(*wrapper_args)\u001b[0m\n\u001b[0;32m    280\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfunction_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mwrapper_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    281\u001b[0m         \u001b[0mncalls\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 282\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mfunction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwrapper_args\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    283\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    284\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mncalls\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunction_wrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m<ipython-input-22-2cc758e536ac>\u001b[0m in \u001b[0;36mgrad_cost_function\u001b[1;34m(self, theta_)\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mgrad_cost_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtheta_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 45\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrad_likelihood\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtheta_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     46\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdeep\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m<ipython-input-22-2cc758e536ac>\u001b[0m in \u001b[0;36mgrad_likelihood\u001b[1;34m(self, theta_)\u001b[0m\n\u001b[0;32m     32\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtheta_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mm\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 34\u001b[1;33m                 \u001b[0mg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlogistic_function\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0mtheta_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m  \u001b[1;33m)\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     35\u001b[0m                 \u001b[0ms_\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;33m(\u001b[0m \u001b[0mg\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0my_\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m         \u001b[1;31m# Add regularization\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
       ]
      }
     ],
     "prompt_number": 42
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "p = np.array([0, 0.5, 1])\n",
      "corr_p = np.maximum(np.minimum(p, 1 - 1e-15), 1e-15)\n",
      "print(np.log(corr_p))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[ -3.45387764e+01  -6.93147181e-01  -9.99200722e-16]\n"
       ]
      }
     ],
     "prompt_number": 304
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}