{
 "metadata": {
  "name": "",
  "signature": "sha256:a559ae6e664f2bdd22587603a9dabd2b8bf186e5054ada7feb9ba2766042550c"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Useful links\n",
      "- http://blog.datumbox.com/machine-learning-tutorial-the-multinomial-logistic-regression-softmax-regression/\n",
      "- https://chrisjmccormick.wordpress.com/2014/06/13/deep-learning-tutorial-softmax-regression/"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import random\n",
      "import numpy as np\n",
      "from sklearn import preprocessing\n",
      "from sklearn import cross_validation\n",
      "from scipy.optimize.optimize import fmin_bfgs\n",
      "\n",
      "\n",
      "N_CLASSES = 9"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 350
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "norm = preprocessing.Normalizer()\n",
      "X_train = np.loadtxt(open(\"../data/train.csv\",\"rb\"),delimiter=\",\",skiprows=1, usecols=range(1,94))\n",
      "X_train = norm.fit_transform(X_train)\n",
      "\n",
      "y_train = np.loadtxt(open(\"../data/train.csv\",\"rb\"),dtype=str,delimiter=\",\",skiprows=1, usecols=[94])\n",
      "y_train = np.array([int(c[-2]) for c in y_train])  # Parse classes from Class_1 into 1\n",
      "\n",
      "X_test = np.loadtxt(open(\"../data/test.csv\",\"rb\"),delimiter=\",\",skiprows=1, usecols=range(1,94))\n",
      "X_test = norm.transform(X_test)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 220
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def prob(X, Theta):\n",
      "        vctr = np.e**np.dot(Theta, X.T)\n",
      "        norm = sum( vctr )\n",
      "        vctr = vctr / norm\n",
      "        return vctr"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 271
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fix_me = lambda p: np.maximum(np.minimum(p, 1 - 1e-15), 1e-15)\n",
      "\n",
      "def J(X, y_, Theta, alpha):\n",
      "    m = len(y_)  \n",
      "    y_ = y_ - 1\n",
      "    Theta = np.reshape(Theta, (N_CLASSES, X.shape[1]))  \n",
      "    J = sum([np.log( fix_me( prob(X[i], Theta)[y_[i]] )) for i in range(len(y_))])\n",
      "    # Substract regularization, because we are maximizing this expression\n",
      "    J -= alpha/(2) * np.sum(Theta**2)\n",
      "    return -1/m*J # revert to minimizing\n",
      "\n",
      "        \n",
      "def Jgrad(X, y_, Theta, alpha):\n",
      "    m = len(y_)  \n",
      "    y_ = y_ - 1\n",
      "    Theta = np.reshape(Theta, (N_CLASSES, X.shape[1])) \n",
      "    Jgrad = []\n",
      "    for j in range(Theta.shape[0]):\n",
      "        #grad = sum([print((y_[i]==j) - prob(X[i], Theta)[j] ) for i in range(m)])\n",
      "        grad = sum([X[i] * ( (y_[i]==j) - prob(X[i], Theta)[j] ) for i in range(m)])\n",
      "        # Substrach regularization\n",
      "        grad -= alpha * Theta[j]\n",
      "        Jgrad.append(np.array(grad))\n",
      "    Jgrad = np.array(Jgrad)\n",
      "    return np.ravel(np.array(-1/m*Jgrad))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 352
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def numerical_grad(f, params, epsilon):\n",
      "    \"\"\"Method of finite differences, sanity check to see if our Jgrad is implemented correctly\"\"\"\n",
      "    num_grad = np.zeros_like(params)\n",
      "    perturb = np.zeros_like(params)\n",
      "    for i in range(params.size):\n",
      "        perturb[i] = epsilon\n",
      "        j1 = f(params + perturb)\n",
      "        j2 = f(params - perturb)\n",
      "        num_grad[i] = (j1 - j2) / (2. * epsilon)\n",
      "        perturb[i] = 0\n",
      "    return num_grad\n",
      "\n",
      "def check_grad(X, y_, Theta):\n",
      "    alpha = 2\n",
      "    f = lambda theta: J(X, y_, theta, alpha)\n",
      "    ag = Jgrad(X, y_, Theta, alpha)\n",
      "    ng = numerical_grad(f, Theta, 1e-5)\n",
      "    return np.sum((ag - ng)**2)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 339
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "check_grad(X_train[:100], y_train[:100], Theta)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[-0.00225688  0.01360982  0.01292238  0.00752326  0.00484267  0.01045147\n",
        " -0.01067766 -0.05472727  0.01272724 -0.00878294]\n",
        "[-0.00225688  0.01360982  0.01292238  0.00752326  0.00484267  0.01045147\n",
        " -0.01067766 -0.05472727  0.01272724 -0.00878294]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 338,
       "text": [
        "3.0045670543234442e-18"
       ]
      }
     ],
     "prompt_number": 338
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class SoftmaxRegression():\n",
      "    def __init__(self, h=prob, alpha=0.01):\n",
      "        self.alpha = alpha\n",
      "        self.h = h\n",
      "    \n",
      "    def get_params(self, deep=True):\n",
      "        # suppose this estimator has parameters \"alpha\" and \"recursive\"\n",
      "        return {}\n",
      "\n",
      "    def set_params(self, **parameters):\n",
      "        for parameter, value in parameters.items():\n",
      "            self.setattr(parameter, value)\n",
      "        return self\n",
      "    \n",
      "    def fit(self, X, y_):\n",
      "        X = np.column_stack((np.ones(len(y_)), X))  # za prosti clen\n",
      "        Theta0 = np.ones(N_CLASSES * X.shape[1])    # Starting point\n",
      "        self.Theta = fmin_bfgs(lambda th_: J(X, y_, th_, self.alpha),\n",
      "                                    Theta0,\n",
      "                                    fprime=lambda th_: Jgrad(X, y_, th_, self.alpha),\n",
      "                                    ) \n",
      "    \n",
      "        \n",
      "    def predict_proba(self, X):\n",
      "        if len(X.shape) == 2:  # za vec na enkrat\n",
      "            X = np.column_stack((np.ones(X.shape[0]), X))\n",
      "            pred = self.h(X, self.Theta)\n",
      "            pred = np.array(list(zip(1-pred, pred)))   # tole smo videli iz errorja ... potrebujemo pac n*2 matrix\n",
      "            return pred\n",
      "        else:  # za en vektor\n",
      "            X = np.hstack(([1], X))\n",
      "            pred = self.h(X, self.Theta)\n",
      "            return np.array([1-pred, pred])\n",
      "        \n",
      "    def predict(self, X):\n",
      "        probs = self.predict_proba(X)\n",
      "        if len(probs.shape) == 2:\n",
      "            return np.array(list(map(np.argmax, probs)))  # najboljso predikcijo izberi\n",
      "        return np.array(np.argmax(probs))\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 344
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Cross Validation"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def logloss(pred, act):\n",
      "    epsilon = 1e-15\n",
      "    pred = sp.maximum(epsilon, pred)\n",
      "    pred = sp.minimum(1-epsilon, pred)\n",
      "    x = np.array([p[a-1] for p, a in zip(pred, act)])\n",
      "    return -sum(np.log(x))/len(x)\n",
      "\n",
      "def evaluate(act, prob):\n",
      "    return logloss(prob, act)\n",
      "\n",
      "def my_cross_validation(X, y_, model):\n",
      "    k = 3\n",
      "    S = []\n",
      "    kf = cross_validation.KFold(len(y_), n_folds=k, shuffle=True)\n",
      "    for train_index, test_index in kf:\n",
      "        # Split\n",
      "        X_train, X_test = X[train_index], X[test_index]\n",
      "        y_train, y_test = y_[train_index], y_[test_index]\n",
      "        # Predict\n",
      "        model.fit(X_train, y_train)\n",
      "        P = model.predict_proba(X_test) \n",
      "        # Evaluate\n",
      "        S += [evaluate(y_test, P)]\n",
      "    S = np.array(S)\n",
      "    return(np.mean(S))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 340
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "step = 100\n",
      "indices = list(range(0, X_train.shape[0], step))\n",
      "random.shuffle(indices)\n",
      "\n",
      "X = X_train[indices]\n",
      "y_ = y_train[indices]\n",
      "softmax = SoftmaxRegression()\n",
      "s = my_cross_validation(X, y_, softmax)\n",
      "s"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}