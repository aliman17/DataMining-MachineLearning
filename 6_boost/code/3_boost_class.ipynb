{
 "metadata": {
  "name": "",
  "signature": "sha256:b78ca8df767f96ef423fafc70a994a26d10d4e5f600f512fc1e2dad873602d6e"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn import tree\n",
      "from sklearn.ensemble import GradientBoostingClassifier\n",
      "from sklearn import cross_validation\n",
      "from sklearn import preprocessing\n",
      "from sklearn import metrics\n",
      "import numpy as np\n",
      "import queue\n",
      "import copy\n",
      "import time\n",
      "import sys"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "NUM_OF_CLASSES = 0\n",
      "def load_data1():\n",
      "    \"\"\"Load keggle\"\"\"\n",
      "    global NUM_OF_CLASSES\n",
      "    X_train = np.loadtxt(open(\"../data/train.csv\",\"rb\"),delimiter=\",\",skiprows=1, usecols=range(1,94))\n",
      "    X_test = np.loadtxt(open(\"../data/test.csv\",\"rb\"),delimiter=\",\",skiprows=1, usecols=range(1,94))\n",
      "    \n",
      "    y_train = np.loadtxt(open(\"../data/train.csv\",\"rb\"),dtype=str,delimiter=\",\",skiprows=1, usecols=[94])\n",
      "    y_train = np.array([int(c[-2])-1 for c in y_train])  # Parse classes from Class_1 into 1\n",
      "    return X_train, y_train, X_test\n",
      "\n",
      "def load_data2():\n",
      "    \"\"\"Load iris\"\"\"\n",
      "    iris = datasets.load_iris()\n",
      "    X_train = iris.data\n",
      "    y_train = iris.target.T\n",
      "    return X_train, y_train, None"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "data = 1\n",
      "if data == 1:\n",
      "    X_train, Y_train, X_test = load_data1()\n",
      "else:\n",
      "    X_train, Y_train, X_test = load_data2()\n",
      "NUM_OF_CLASSES = len( np.unique(Y_train))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Preprocesse Data"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def shuffle(X, Y):\n",
      "    ind = np.array(list(range(X.shape[0])))\n",
      "    np.random.shuffle(ind)\n",
      "    X = np.array( [ X[i] for i in ind] )\n",
      "    Y = np.array( [ Y[i] for i in ind] )\n",
      "    return X, Y \n",
      "X_train, Y_train = shuffle(X_train, Y_train)\n",
      "std = preprocessing.StandardScaler()\n",
      "X_train = std.fit_transform(X_train)\n",
      "X_test = std.transform(X_test)\n",
      "X_train = np.matrix(X_train)\n",
      "X_test = np.matrix(X_test)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def binaryClassRepresentation(Y):\n",
      "    \"\"\" Class 1 write as 1,0,0,0,0,0,0,0,0, class 2 write as 0,1,0,0,0,0,0,0,0, ... \"\"\"\n",
      "    I = np.identity(NUM_OF_CLASSES)\n",
      "    Y = np.array( [ I[y] for y in Y])\n",
      "    return Y\n",
      "Y_train = binaryClassRepresentation(Y_train)\n",
      "Y_train = np.matrix(Y_train)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "CrossValidation"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def evaluate(pred, real):\n",
      "    return metrics.log_loss(real, pred)\n",
      "\n",
      "\n",
      "def my_cross_validation(X, Y, model, k=3):\n",
      "    S = []\n",
      "    kf = cross_validation.KFold(len(Y), n_folds=k, shuffle=False)\n",
      "    for train_index, test_index in kf:\n",
      "        print(\"New fold\")\n",
      "        start = time.time()  \n",
      "        # Split\n",
      "        X_train, X_test = X[train_index], X[test_index]\n",
      "        Y_train, Y_test = Y[train_index], Y[test_index]\n",
      "        \n",
      "        model.fit(X_train, Y_train)\n",
      "        predicted = model.predict(X_test)\n",
      "        # Evaluate\n",
      "        score = evaluate(predicted, Y_test)\n",
      "         \n",
      "        end = time.time()\n",
      "        \n",
      "        print(\"        Time:\", round(end - start, 2), \"s                                     ### Vmesni rezultat ###: \",  score)\n",
      "        sys.stdout.flush()\n",
      "        S.append( score )\n",
      "    S = np.array(S)\n",
      "    return(np.mean(S))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Initial models"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class InitModel():\n",
      "    def fit(self, X, Y):\n",
      "        self.Y = Y\n",
      "    def predict(self, X):\n",
      "        return np.mean(self.Y, axis=0)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "init = InitModel()\n",
      "init.fit(1, np.matrix([[1,1,1], [2,2,2]]))\n",
      "\n",
      "init.predict(1)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 11
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Learner"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class GradBoostLearner():\n",
      "    \"\"\"Gradient Boosting for Regression.\"\"\"\n",
      "\n",
      "    def __init__(self, learner, n_estimators=10, epsilon=1e-5, l_rate = 0.1, loss=\"log_loss\"):\n",
      "        self.n_estimators = n_estimators\n",
      "        self.learner = learner  # base learner\n",
      "        self.name = \"gb_est\" + self.n_estimators + \"_\" + loss\n",
      "        self.epsilon = epsilon\n",
      "        self.l_rate = l_rate\n",
      "        losses = {\"huber\": self.grad_huber_loss, \n",
      "                  \"squared\": self.grad_squared_loss, \n",
      "                  \"abs\": self.grad_abs_loss,\n",
      "                  \"log_loss\": self.log_loss}\n",
      "        self.loss = losses[loss]\n",
      "        self.model_series = None\n",
      "\n",
      "    \n",
      "    def log_loss(self, real, pred):\n",
      "        \"\"\"Negative gradient for log loss\"\"\"\n",
      "        return metrics.log_loss(real, pred)\n",
      "    \n",
      "    def grad_squared_loss(self, y, f):\n",
      "        \"\"\"Negative gradiant for squared loss.\"\"\"\n",
      "        return y - f\n",
      "    \n",
      "    def grad_abs_loss(self, y, f):\n",
      "        \"\"\"Negative gradient for absolute loss.\"\"\"\n",
      "        return np.sign(y - f)\n",
      "    \n",
      "    def grad_huber_loss(self, y, f, delta=0.5):\n",
      "        \"\"\"Negative gradient for Huber loss.\"\"\"\n",
      "        r0 = y - f\n",
      "        r1 = delta * np.sign(r0)\n",
      "        return np.vstack((r0, r1)).T[np.arange(y.shape[0]), (np.abs(r0)>delta).astype(int)]\n",
      "    \n",
      "   \n",
      "    \n",
      "    def fit_all_classes(self, X, Y):\n",
      "        # Description: Function performs gradient boosting for ALL classes\n",
      "        #\n",
      "        # Input:       X     matrix of samples\n",
      "        #              Y     MATRIX of a classes identification\n",
      "        #\n",
      "        # Output:      List of fitted models that are used for classification.\n",
      "        \n",
      "        \n",
      "        # Prepare list of queues which are intended for NUM_OF_CLASSES series of models\n",
      "        model_series = [0] * NUM_OF_CLASSES\n",
      "        for i in range(NUM_OF_CLASSES):\n",
      "            model_seris[i] =  queue.Queue()\n",
      "            \n",
      "            \n",
      "        # Initial models\n",
      "        curr_models = [ InitModel() for i in range(NUM_OF_CLASSES) ]          \n",
      "        \n",
      "        dY = Y.astype(float)\n",
      "        dYT = dY.T\n",
      "        \n",
      "        for i in range(n_iter):\n",
      "            # Add models to series\n",
      "            model_series.put(curr_models)  \n",
      "            \n",
      "            # Fit models \n",
      "            for j in range(NUM_OF_CLASSES):\n",
      "                curr_models[j].fit(X, dY)\n",
      "                \n",
      "            # Predict models\n",
      "            # Output: 2D\n",
      "            Predicted = np.array([curr_models[j].predict(X) for j in range(NUM_OF_CLASSES)]).T\n",
      "            \n",
      "            # Softmax\n",
      "            Predicted = np.exp( Predicted )\n",
      "            SumPredicted = np.sum( Predicted, axis = 1 )   # sum rows, because these are gonna be denumerators\n",
      "            Predicted = Predicted / SumPredicted \n",
      "            \n",
      "            # Calculate negative gradients \n",
      "            dYT = dYT - self.l_rate * Predicted\n",
      "            dY = dYT.T\n",
      "            \n",
      "            curr_models = [ copy.copy(self.learner) for i in range(NUM_OF_CLASSES) ]  # WARNING# WARNING\n",
      "            \n",
      "            #alpha[i] = 1  # learning rate; smaller alpha needs more classifiers\n",
      "                          # but predicts better\n",
      "\n",
      "        self.model_series = model_series \n",
      "            \n",
      "    \n",
      "    def fit(self, X, Y):\n",
      "        self.fit_all_classes(X, Y)\n",
      "    \n",
      "    \n",
      "    def predict_class(self, X, models):\n",
      "        predict = 0\n",
      "        for model in models:\n",
      "            predict += self.l_rate * model.predict(X)\n",
      "        return predict\n",
      "    \n",
      "    def predict(self, X):\n",
      "        \"\"\"Given a data instance or table of data instances returns predicted class.\"\"\"\n",
      "        #for models in models_per_class:\n",
      "        return np.array([ self.predict_class(X, models) for models in self.models_per_each_class]).T\n",
      "    \n",
      "class GradBoostModel():\n",
      "    \"\"\"Classifier for gradient boosting.\"\"\"\n",
      "    def __init__(self, models_per_each_class, l_rate = 0.1):\n",
      "        self.models_per_each_class = models_per_each_class\n",
      "        self.l_rate = l_rate\n",
      "        \n",
      "    def predict_class(self, X, models):\n",
      "        predict = 0\n",
      "        for model in models:\n",
      "            predict += self.l_rate * model.predict(X)\n",
      "        return predict\n",
      "    \n",
      "    def predict(self, X):\n",
      "        \"\"\"Given a data instance or table of data instances returns predicted class.\"\"\"\n",
      "        #for models in models_per_class:\n",
      "        return np.array([ self.predict_class(X, models) for models in self.models_per_each_class]).T"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 94
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}