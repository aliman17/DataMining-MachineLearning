# http://docs.orange.biolab.si/3/modules/data.storage.html#data-access
import Orange.data
import numpy as np
import sklearn
from sklearn.metrics import mean_squared_error
import scipy
from random import shuffle

# # Not really used, but here for testing purposes/sanity check
#
# def gradient_descent(X, y, alpha=0.01, epochs=1000):
#     "Returns theta for gradient descent with no regularization applied"
#     theta = np.zeros((X.shape[1])).T
#     for i in range(epochs):
#         theta = theta - alpha * (X.dot(theta) - y).dot(X)
#     return theta
#
# def gradient_descent_reg(X, y, alpha=0.1, lambda_=0.1, epochs=1000):
#     "Returns theta for gradient descent using L2 regularization"
#     m = X.shape[0]
#     theta = np.zeros((X.shape[1])).T
#     for i in range(epochs):
#         theta = theta * (1 - (alpha * lambda_ / m)) - alpha / m * (X.dot(theta) - y).dot(X)
#     return theta

# http://www.holehouse.org/mlclass/07_Regularization.html
def J(theta, X, y, lambda_):
    "The cost function"
    m = X.shape[0]
    # return 0.5 * sum((X.dot(theta) - y) ** 2)
    return 1 / (2 * m) * sum((X.dot(theta) - y) ** 2) + lambda_ * sum(theta[1:] ** 2)

def Jgrad(theta, X, y, lambda_):
    "Gradient of the cost function"
    m = X.shape[0]
    # return (X.dot(theta) - y).dot(X)
    return 1 / m * (X.dot(theta) - y).dot(X) + lambda_ / m * theta

def numerical_grad(f, params, epsilon):
    "Method of finite differences, sanity check to see if our Jgrad is implemented correctly"
    # ag = Jgrad(theta, selected.X, selected.Y)
    # ng = numerical_grad(lambda params: J(params, selected.X, selected.Y), theta, 1e-7)
    # ag, ng # should be about the same
    num_grad = np.zeros_like(params)
    perturb = np.zeros_like(params)
    for i in range(params.size):
        perturb[i] = epsilon
        j1 = f(params + perturb)
        j2 = f(params - perturb)
        num_grad[i] = (j1 - j2) / (2. * epsilon)
        perturb[i] = 0
    return num_grad

def test_grad():
    lambda_ = 0
    X = np.column_stack((np.ones(train_selected.X.shape[0]), train_selected.X))
    theta = np.zeros((X.shape[1]))
    ag = Jgrad(theta, X, train_selected.Y, lambda_)
    theta = np.zeros((X.shape[1]))
    ng = numerical_grad(lambda params: J(params, X, train_selected.Y, lambda_), theta, 1e-2)
    return RMSE(ag, ng)

def select_attributes(n, data):
    "Returns n best attributes from data with greatest correlation with the class"
    atts = list(range(len(data.domain.attributes)))
    atts.sort(key=lambda att: abs(scipy.stats.pearsonr(data[:, att], data[:, data.domain.class_var])[0][0]), reverse=True)
    return atts[:n]

def LinearRegression(X, y, lambda_=0.1):
    """Uses linear regression (J and Jgrad we defined earlier) to try and learn
    to predict y based on X.  Returns a procedure which takes new sample x as
    input and returns the prediction."""
    X = np.column_stack((np.ones(X.shape[0]), X))
    theta, _, _ = scipy.optimize.fmin_l_bfgs_b(lambda t: J(t, X, y, lambda_),
            np.zeros((X.shape[1])),
            lambda t: Jgrad(t, X, y, lambda_))
    def predict(new_x):
        x_with_ones = np.column_stack((np.ones(new_x.shape[0]), new_x))
        return x_with_ones.dot(theta)
    return predict

def ElasticNet(X, y, lambda_=0.1, alpha=0.1, epochs=1000):
    """Uses elastic net (J and Jgrad we defined earlier) to try and learn
    to predict y based on X.  Returns a procedure which takes new sample x as
    input and returns the prediction."""
    def S(z, gamma):
        return np.sign(z) * np.max((np.abs(z) - gamma), 0)
    X = sklearn.preprocessing.scale(X)
    X = np.column_stack((np.ones(X.shape[0]), X))
    N = X.shape[0]
    theta = np.zeros((X.shape[1])).T
    # theta = np.ones((X.shape[1])).T
    for _ in range(epochs):
        for j in range(X.shape[1]):
            y_line = X.dot(theta)
            y_line_j = y_line - X[:, j] * theta[j]
            theta[j] = S((y - y_line_j).dot(X[:, j]) / N, lambda_ * alpha) / (1 + lambda_ * (1 - alpha))
    def predict(new_x):
        new_x = sklearn.preprocessing.scale(new_x)
        x_with_ones = np.column_stack((np.ones(new_x.shape[0]), new_x))
        return x_with_ones.dot(theta)
    return predict

# predictor = ElasticNet(train_data.X[:, sorted_atts[:300]], train_data.Y, lambda_=0.5, alpha=0.9, epochs=19)
# h = predictor(train_data.X[:, sorted_atts[:300]])
# h, train_data.Y, RMSE(h, train_data.Y)
#
# predictor = LinearRegression(train_data.X, train_data.Y)
# h = predictor(train_data.X)
# h, train_data.Y, RMSE(h, train_data.Y)
#
# kfoldcv(14, train_data.X[:, sorted_atts[:300]], train_data.Y, ElasticNet, lambda_=0.7, alpha=0.9, epochs=15)

def rand_partition_dataset(k, n):
    "Returns indices that split the list of length n on k partitions"
    indices = list(range(n))
    shuffle(indices)
    partitions = []
    partition_size = int(len(indices)/k);
    for i in range(0, len(indices), partition_size):
        partitions.append(indices[i:i+partition_size])
    return partitions

def RMSE(h, y): return mean_squared_error(list(map(clamp, h)), y)**0.5

def kfoldcv(k, X, Y, learner, **learner_args):
    "Perform k-fold cross validation on data using learner. Returns RMSE."
    partitions = rand_partition_dataset(k, Y.shape[0])
    rmses = []
    for test in partitions:
        train = [inner for outer in partitions for inner in outer if outer != test]
        predictor = learner(X[train], Y[train], **learner_args)
        h = predictor(X[test])
        rmses.append(RMSE(h, Y[test]))
    return np.mean(rmses)

# kfoldcv(10, optimized_train_data.X, optimized_train_data.Y, LinearRegression, lambda_=0.2)

def optimize_parameters(train_data, sorted_atts):
    "prints the results of running k-fold cv on training data with various number of top attributes/lambda values"
    glob_min_l = []
    for n_atts in range(40, 300, 10):
    # for n_atts in range(1, len(sorted_atts), 5):
        X = train_data.X[:, sorted_atts[:n_atts]]
        # train_selected = Orange.data.Table(train_data.X[:, sorted_atts[:n_atts]], train_data.Y)
        print(n_atts)
        for l in np.arange(0.0, 1.0, 0.01):
            # for a in np.arange(0.6, 1.0, 0.1):
            # kf = kfoldcv(14, X, train_data.Y, ElasticNet, lambda_=l, alpha=a, epochs=10)
            kf = kfoldcv(14, X, train_data.Y, LinearRegression, lambda_=l)
            glob_min_l.append((n_atts, l, kf))
            print((n_atts, l, kf))
    glob_min_l.sort(key=lambda r: r[2])
    return glob_min_l

# results = optimize_parameters(train_data, sorted_atts)

train_data = Orange.data.Table("train.tab")
test_data = Orange.data.Table("test.tab")

sorted_atts = select_attributes(train_data.X.shape[1], train_data)

optimized_train_data = Orange.data.Table(train_data.X, train_data.Y)
predictor = ElasticNet(optimized_train_data.X, optimized_train_data.Y, lambda_=0.8, alpha=0.9, epochs=20)
h = predictor(test_data.X)

def clamp(n, min_val=0, max_val=100):
    "Clamps n between min_val and max_val"
    return min(max_val, max(min_val, n))

def compose(f1, f2):
    "Simple composition of f1 and f2"
    return lambda arg: f1(f2(arg))

outfile = open('predictions.txt', 'w')
outfile.write("\n".join(list(map(compose(str, clamp), h))))